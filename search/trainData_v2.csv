category,doc,,,,
AI,"Artificial intelligence ""AI"" redirects here. For other uses, see AI (disambiguation) and Artificial intelligence (disambiguation). Part on a series on vte Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] Colloquially, the term ""artificial intelligence"" is often used to describe machines (or computers) that mimic ""cognitive"" functions that humans associate with the human mind, such as ""learning"" and ""problem solving"".[2] As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.[3] A quip in Tesler's Theorem says ""AI is whatever hasn't been done yet.""[4] For instance, optical character recognition is frequently excluded from things considered to be AI,[5] having become a routine technology.[6] Modern machine capabilities generally classified as AI include successfully understanding human speech,[7] competing at the highest level in strategic game systems (such as chess and Go),[8] autonomously operating cars, intelligent routing in content delivery networks, and military simulations.[9] Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism,[10][11] followed by disappointment and the loss of funding (known as an ""AI winter""),[12][13] followed by new approaches, success and renewed funding.[11][14] For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other.[15] These sub-fields are based on technical considerations, such as particular goals (e.g. ""robotics"" or ""machine learning""),[16] the use of particular tools (""logic"" or artificial neural networks), or deep philosophical differences.[17][18][19] Sub-fields have also been based on social factors (particular institutions or the work of particular researchers).[15] The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects.[16] General intelligence is among the field's long-term goals.[20] Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields. The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".[21] This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity.[22] Some people also consider AI to be a danger to humanity if it progresses unabated.[23][24] Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.[25] In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research Computer science defines AI research as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] A more elaborate definition characterizes AI as ""a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.""[62] A typical AI analyzes its environment and takes actions that maximize its chance of success.[1] An AI's intended utility function (or goal) can be simple (""1 if the AI wins a game of Go, 0 otherwise"") or complex (""Perform actions mathematically similar to ones that succeeded in the past""). Goals can be explicitly defined or induced. If the AI is programmed for ""reinforcement learning"", goals can be implicitly induced by rewarding some types of behavior or punishing others.[a] Alternatively, an evolutionary system can induce goals by using a ""fitness function"" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[63] Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[64] Such systems can still be benchmarked if the non-goal system is framed as a system whose ""goal"" is to successfully accomplish its narrow classification task.[65] AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute.[b] A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following (optimal for first player) recipe for play at tic-tac-toe:[66] If someone has a ""threat"" (that is, two in a row), take the remaining square. Otherwise, if a move ""forks"" to create two threats at once, play that move. Otherwise, take the center square if it is free. Otherwise, if your opponent has played in a corner, take the opposite corner. Otherwise, take an empty corner if one exists. Otherwise, take any empty square. Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or ""rules of thumb"", that have worked well in the past), or can themselves write other algorithms. Some of the ""learners"" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, (given infinite data, time, and memory) learn to approximate any function, including which combination of mathematical functions would best describe the world.[citation needed] These learners could therefore derive all possible knowledge, by considering every possible hypothesis and matching them against the data. In practice, it is seldom possible to consider every possibility, because of the phenomenon of ""combinatorial explosion"", where the time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering a broad range of possibilities unlikely to be beneficial.[67][68] For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding a pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered.[69] The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): ""If an otherwise healthy adult has a fever, then they may have influenza"". A second, more general, approach is Bayesian inference: ""If the current patient has a fever, adjust the probability they have influenza in such-and-such way"". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: ""After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza"". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial ""neurons"" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to ""reinforce"" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem.[70][71] Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as ""since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well"". They can be nuanced, such as ""X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist"". Learners also work on the basis of ""Occam's razor"": The simplest theory that explains the data is the likeliest. Therefore, according to Occam's razor principle, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. The blue line could be an example of overfitting a linear function due to random noise. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is.[72] Besides classic overfitting, learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[73] A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an ""adversarial"" image that the system misclassifies.[c][74][75][76] A self-driving car system may use a neural network to determine which parts of the picture seem to match previous training images of pedestrians, and then model those areas as slow-moving but somewhat unpredictable rectangular prisms that must be avoided.[77][78] Compared with humans, existing AI lacks several features of human ""commonsense reasoning""; most notably, humans have powerful mechanisms for reasoning about ""naïve physics"" such as space, time, and physical interactions. This enables even young children to easily make inferences like ""If I roll this pen off a table, it will fall on the floor"". Humans also have a powerful mechanism of ""folk psychology"" that helps them to interpret natural-language sentences such as ""The city councilmen refused the demonstrators a permit because they advocated violence"" (A generic AI has difficulty discerning whether the ones alleged to be advocating violence are the councilmen or the demonstrators[79][80][81]). This lack of ""common knowledge"" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents Planning A hierarchical control system is a form of control system in which a set of devices and governing software is arranged in a hierarchy. Main article: Automated planning and scheduling Intelligent agents must be able to set goals and achieve them.[108] They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or ""value"") of available choices.[109] In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[110] However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions but also evaluate its predictions and adapt based on its assessment.[111] Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[112] Learning Main article: Machine learning Machine learning (ML), a fundamental concept of AI research since the field's inception,[113] is the study of computer algorithms that improve automatically through experience.[114][115] Unsupervised learning is the ability to find patterns in a stream of input, without requiring a human to label the inputs first. Supervised learning includes both classification and numerical regression, which requires a human to label the input data first. Classification is used to determine what category something belongs in, and occurs after a program sees a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change.[115] Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"". Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[116] In reinforcement learning[117] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. Natural language processing A parse tree represents the syntactic structure of a sentence according to some formal grammar. Main article: Natural language processing Natural language processing[118] (NLP) allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering[119] and machine translation.[120] Many current approaches use word co-occurrence frequencies to construct syntactic representations of text. ""Keyword spotting"" strategies for search are popular and scalable but dumb; a search query for ""dog"" might only match documents with the literal word ""dog"" and miss a document with the word ""poodle"". ""Lexical affinity"" strategies use the occurrence of words such as ""accident"" to assess the sentiment of a document. Modern statistical NLP approaches can combine all these strategies as well as others, and often achieve acceptable accuracy at the page or paragraph level. Beyond semantic NLP, the ultimate goal of ""narrative"" NLP is to embody a full understanding of commonsense reasoning.[121] By 2019, transformer-based deep learning architectures could generate coherent text.[122] Perception Main articles: Machine perception, Computer vision, and Speech recognition Feature detection (pictured: edge detection) helps AI compose informative abstract structures out of raw data. Machine perception[123] is the ability to use input from sensors (such as cameras (visible spectrum or infrared), microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[124] facial recognition, and object recognition.[125] Computer vision is the ability to analyze visual input. Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its ""object model"" to assess that fifty-meter pedestrians do not exist.[126] Motion and manipulation Main article: Robotics AI is heavily used in robotics.[127] Advanced robotic arms and other industrial robots, widely used in modern factories, can learn from experience how to move efficiently despite the presence of friction and gear slippage.[128] A modern mobile robot, when given a small, static, and visible environment, can easily determine its location and map its environment; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge. Motion planning is the process of breaking down a movement task into ""primitives"" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object.[129][130][131] Moravec's paradox generalizes that low-level sensorimotor skills that humans take for granted are, counterintuitively, difficult to program into a robot; the paradox is named after Hans Moravec, who stated in 1988 that ""it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility"".[132][133] This is attributed to the fact that, unlike checkers, physical dexterity has been a direct target of natural selection for millions of years.[134] Social intelligence Main article: Affective computing Kismet, a robot with rudimentary social skills[135] Moravec's paradox can be extended to many forms of social intelligence.[136][137] Distributed multi-agent coordination of autonomous vehicles remains a difficult problem.[138] Affective computing is an interdisciplinary umbrella that comprises systems which recognize, interpret, process, or simulate human affects.[139][140][141] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal affect analysis (see multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.[142] In the long run, social skills and an understanding of human emotion and game theory would be valuable to a social agent. The ability to predict the actions of others by understanding their motives and emotional states would allow an agent to make better decisions. Some computer systems mimic human emotion and expressions to appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.[143] Similarly, some virtual assistants are programmed to speak conversationally or even to banter humorously; this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[144] General intelligence Main articles: Artificial general intelligence and AI-complete Historically, projects such as the Cyc knowledge base (1984–) and the massive Japanese Fifth Generation Computer Systems initiative (1982–1992) attempted to cover the breadth of human cognition. These early projects failed to escape the limitations of non-quantitative symbolic logic models and, in retrospect, greatly underestimated the difficulty of cross-domain AI. Nowadays, most current AI researchers work instead on tractable ""narrow AI"" applications (such as medical diagnosis or automobile navigation).[145] Many researchers predict that such ""narrow AI"" work in different individual domains will eventually be incorporated into a machine with artificial general intelligence (AGI), combining most of the narrow skills mentioned in this article and at some point even exceeding human ability in most or all these areas.[20][146] Many advances have general, cross-domain significance. One high-profile example is that DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own, and later developed a variant of the system which succeeds at sequential learning.[147][148][149] Besides transfer learning,[150] hypothetical AGI breakthroughs could include the development of reflective architectures that can engage in decision-theoretic metareasoning, and figuring out how to ""slurp up"" a comprehensive knowledge base from the entire unstructured Web.[7] Some argue that some kind of (currently-undiscovered) conceptually straightforward, but mathematically difficult, ""Master Algorithm"" could lead to AGI.[151] Finally, a few ""emergent"" approaches look to simulating human intelligence extremely closely, and believe that anthropomorphic features like an artificial brain or simulated child development may someday reach a critical point where general intelligence emerges.[152][153] Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered ""AI-complete"", because all of these problems need to be solved simultaneously in order to reach human-level machine performance. ",,,,
AI,"Robotics is an interdisciplinary research area at the interface of computer science and engineering.[1] Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe. Robotics draws on the achievement of information engineering, computer engineering, mechanical engineering, electronic engineering and others. Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations and for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid.[2] Robotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with computer engineering, computer science (especially artificial intelligence), electronics, mechatronics, mechanical, nanotechnology and bioengineering Robotic aspects Mechanical construction Electrical aspect A level of programming There are many types of robots; they are used in many different environments and for many different uses. Although being very diverse in application and form, they all share three basic similarities when it comes to their construction: Robots all have some kind of mechanical construction, a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud, might use caterpillar tracks. The mechanical aspect is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function. Robots have electrical components which power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status) and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations) All robots contain some level of computer programming code. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly constructed its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence and hybrid. A robot with remote control programing has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with a remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. Hybrid is a form of programming that incorporates both AI and RC functions in them. Applications As more and more robots are designed for specific tasks this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed as ""assembly robots"". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a ""welding robot"" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as ""heavy-duty robots"".[23] Atlas Robot a humanoid robot designed to perform a variety of complex tasks, especially in situations unsafe for humans. It is currently developed by Boston Dynamics.[24] Current and potential applications include: Military robots. Industrial robots. Robots are increasingly used in manufacturing (since the 1960s). According to the Robotic Industries Association US data, in 2016 automotive industry was the main customer of industrial robots with 52% of total sales.[25] In the auto industry, they can amount for more than half of the ""labor"". There are even ""lights off"" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[26] Cobots (collaborative robots).[27] Construction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[28] Agricultural robots (AgRobots).[29] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[30] 1996-1998 research also proved that robots can perform a herding task.[31] Medical robots of various types (such as da Vinci Surgical System and Hospi). Kitchen automation. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts) and Sally (salads).[32] Home examples are Rotimatic (flatbreads baking)[33] and Boris (dishwasher loading).[34] Robot combat for sport – hobby or sport event where two or more robots fight in an arena to disable each other. This has developed from a hobby in the 1990s to several TV series worldwide. Cleanup of contaminated areas, such as toxic waste or nuclear facilities.[35] Domestic robots. Nanorobots. Swarm robotics.[36] Autonomous drones. Sports field line marking. Components Power source Further information: Power supply and Energy storage The InSight lander with solar panels deployed in a cleanroom At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries that are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need a fuel, require heat dissipation and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[37] Potential power sources could be: pneumatic (compressed gases) Solar power (using the sun's energy and converting it into electrical power) hydraulics (liquids) flywheel energy storage organic garbage (through anaerobic digestion) nuclear Actuation Main article: Actuator A robotic leg powered by air muscles Actuators are the ""muscles"" of a robot, the parts which convert stored energy into movement.[38] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air. Electric motors Main article: Electric motor The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational. Linear actuators Main article: Linear actuator Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator that is turned by hand, such as a rack and pinion on a car. Series elastic actuators A flexure is designed as part of the motor actuator, to improve safety and provide robust force control, energy efficiency, shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. The resultant lower reflected inertia can improve safety when a robot is interacting with humans or during collisions. It has been used in various robots, particularly advanced manufacturing robots [39] and walking humanoid robots.[40][41] Air muscles Main article: Pneumatic artificial muscles Pneumatic artificial muscles, also known as air muscles, are special tubes that expand(typically up to 40%) when air is forced inside them. They are used in some robot applications.[42][43][44] Muscle wire Main article: Shape memory alloy Muscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material which contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[45][46] Electroactive polymers Main article: Electroactive polymers EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[47] and to enable new robots to float,[48] fly, swim or walk.[49] Piezo motors Main article: Piezoelectric motor Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[50] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[51] These motors are already available commercially, and being used on some robots.[52][53] Elastic nanotubes Further information: Carbon nanotube Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact ""muscle"" might allow future robots to outrun and outjump humans.[54] Sensing Main articles: Robotic sensing and Robotic sensors Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information of the task it is performing. Touch Main article: Tactile sensor Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[55][56] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting robotic grip on held objects. Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one—allowing patients to write with it, type on a keyboard, play piano and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feeling in its fingertips.[57] ",,,,
AI,"Pattern recognition Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. However, these activities can be viewed as two facets of the same field of application, and together they have undergone substantial development over the past few decades. A modern definition of pattern recognition is: The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.[1] This article focuses on machine learning approaches to pattern recognition. Pattern recognition systems are in many cases trained from labeled ""training"" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). Machine learning is strongly related to pattern recognition and originates from artificial intelligence. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input;[2] sequence labeling, which assigns a class to each member of a sequence of values [3](for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.[4] Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of ""simple"", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.[5] A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled is the training data. Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. In some fields, the terminology is different: For example, in community ecology, the term ""classification"" is used to refer to what is commonly known as ""clustering"". The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of ""male"" or ""female"", or a blood type of ""A"", ""B"", ""AB"" or ""O""), ordinal (consisting of one of a set of ordered items, e.g., ""large"", ""medium"" or ""small""), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10). Probabilistic classifiers Main article: Probabilistic classifier Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a ""best"" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms: They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.) Correspondingly, they can abstain when the confidence of choosing any particular output is too low. Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation. Number of important feature variables Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given.[6] The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of {displaystyle n}n features the powerset consisting of all {displaystyle 2^{n}-1}2^{n}-1 subsets of features need to be explored. The Branch-and-Bound algorithm[7] does reduce this complexity but is intractable for medium to large values of the number of available features {displaystyle n}n. For a large-scale comparison of feature-selection algorithms see .[8] Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features. ",,,,
AI,"Machine Learning is the field of study that gives computers the capability to learn without being explicitly programmed. ML is one of the most exciting technologies that one would have ever come across. As it is evident from the name, it gives the computer that makes it more similar to humans: The ability to learn. Machine learning is actively being used today, perhaps in many more places than one would expect. Getting Started with Machine Learning An Introduction to Machine Learning What is Machine Learning ? Introduction to Data in Machine Learning Demystifying Machine Learning ML – Applications Best Python libraries for Machine Learning Artificial Intelligence | An Introduction Machine Learning and Artificial Intelligence Difference between Machine learning and Artificial Intelligence Agents in Artificial Intelligence 10 Basic Machine Learning Interview Questions Data and It’s Processing: Introduction to Data in Machine Learning Understanding Data Processing Python | Create Test DataSets using Sklearn Python | Generate test datasets for Machine learning Python | Data Preprocessing in Python Data Cleansing Feature Scaling – Part 1 Feature Scaling – Part 2 Python | Label Encoding of datasets Python | One Hot Encoding of datasets Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python Supervised learning : Getting started with Classification Basic Concept of Classification Types of Regression Techniques Classification vs Regression ML | Types of Learning – Supervised Learning Multiclass classification using scikit-learn Gradient Descent : Gradient Descent algorithm and its variants Stochastic Gradient Descent (SGD) Mini-Batch Gradient Descent with Python Optimization techniques for Gradient Descent Introduction to Momentum-based Gradient Optimizer Linear Regression : Introduction to Linear Regression Gradient Descent in Linear Regression Mathematical explanation for Linear Regression working Normal Equation in Linear Regression Linear Regression (Python Implementation) Simple Linear-Regression using R Univariate Linear Regression in Python Multiple Linear Regression using Python Multiple Linear Regression using R Locally weighted Linear Regression Python | Linear Regression using sklearn Linear Regression Using Tensorflow A Practical approach to Simple Linear Regression using R Linear Regression using PyTorch Pyspark | Linear regression using Apache MLlib ML | Boston Housing Kaggle Challenge with Linear Regression Python | Implementation of Polynomial Regression Softmax Regression using TensorFlow Logistic Regression : Understanding Logistic Regression Why Logistic Regression in Classification ? Logistic Regression using Python Cost function in Logistic Regression Logistic Regression using Tensorflow Naive Bayes Classifiers Support Vector: Support Vector Machines(SVMs) in Python SVM Hyperparameter Tuning using GridSearchCV Support Vector Machines(SVMs) in R Using SVM to perform classification on a non-linear dataset Decision Tree: Decision Tree Decision Tree Regression using sklearn Decision Tree Introduction with example Decision tree implementation using Python Decision Tree in Software Engineering Random Forest: Random Forest Regression in Python Ensemble Classifier Voting Classifier using Sklearn Bagging classifier Unsupervised learning : ML | Types of Learning – Unsupervised Learning Supervised and Unsupervised learning Clustering in Machine Learning Different Types of Clustering Algorithm K means Clustering – Introduction Elbow Method for optimal value of k in KMeans ML | K-means++ Algorithm Analysis of test data using K-Means Clustering in Python Mini Batch K-means clustering algorithm Mean-Shift Clustering DBSCAN – Density based clustering Implementing DBSCAN algorithm using Sklearn Fuzzy Clustering Spectral Clustering OPTICS Clustering OPTICS Clustering Implementing using Sklearn Hierarchical clustering (Agglomerative and Divisive clustering) Implementing Agglomerative Clustering using Sklearn Gaussian Mixture Model Reinforcement Learning: Reinforcement learning Reinforcement Learning Algorithm : Python Implementation using Q-learning Introduction to Thompson Sampling Genetic Algorithm for Reinforcement Learning SARSA Reinforcement Learning Q-Learning in Python Dimensionality Reduction : Introduction to Dimensionality Reduction Introduction to Kernel PCA Principal Component Analysis(PCA) Principal Component Analysis with Python Independent Component Analysis Feature Mapping Extra Tree Classifier for Feature Selection Chi-Square Test for Feature Selection – Mathematical Explanation ML | T-distributed Stochastic Neighbor Embedding (t-SNE) Algorithm Python | How and where to apply Feature Scaling? Parameters for Feature Selection Underfitting and Overfitting in Machine Learning Natural Language Processing : Introduction to Natural Language Processing Text Preprocessing in Python | Set – 1 Text Preprocessing in Python | Set 2 Removing stop words with NLTK in Python Tokenize text using NLTK in python How tokenizing text, sentence, words works Introduction to Stemming Stemming words with NLTK Lemmatization with NLTK Lemmatization with TextBlob How to get synonyms/antonyms from NLTK WordNet in Python? Neural Networks : Introduction to Artificial Neutral Networks | Set 1 Introduction to Artificial Neural Network | Set 2 Introduction to ANN (Artificial Neural Networks) | Set 3 (Hybrid Systems) Introduction to ANN | Set 4 (Network Architectures) Activation functions Implementing Artificial Neural Network training process in Python A single neuron neural network in Python Convolutional Neural Networks Introduction to Convolution Neural Network Introduction to Pooling Layer Introduction to Padding Types of padding in convolution layer Applying Convolutional Neural Network on mnist dataset Recurrent Neural Networks Introduction to Recurrent Neural Network Recurrent Neural Networks Explanation seq2seq model Introduction to Long Short Term Memory Long Short Term Memory Networks Explanation Gated Recurrent Unit Networks(GAN) Text Generation using Gated Recurrent Unit Networks GANs – Generative Adversarial Network Introduction to Generative Adversarial Network Generative Adversarial Networks (GANs) Use Cases of Generative Adversarial Networks Building a Generative Adversarial Network using Keras Modal Collapse in GANs Introduction to Deep Q-Learning Implementing Deep Q-Learning using Tensorflow ML – Applications : Rainfall prediction using Linear regression Identifying handwritten digits using Logistic Regression in PyTorch Kaggle Breast Cancer Wisconsin Diagnosis using Logistic Regression Python | Implementation of Movie Recommender System Support Vector Machine to recognize facial features in C++ Decision Trees – Fake (Counterfeit) Coin Puzzle (12 Coin Puzzle) Credit Card Fraud Detection NLP analysis of Restaurant reviews Applying Multinomial Naive Bayes to NLP Problems Image compression using K-means clustering Deep learning | Image Caption Generation using the Avengers EndGames Characters How Does Google Use Machine Learning? How Does NASA Use Machine Learning? 5 Mind-Blowing Ways Facebook Uses Machine Learning Targeted Advertising using Machine Learning How Machine Learning Is Used by Famous Companies? Misc : Pattern Recognition | Introduction Calculate Efficiency Of Binary Classifier Logistic Regression v/s Decision Tree Classification R vs Python in Datascience Explanation of Fundamental Functions involved in A3C algorithm Differential Privacy and Deep Learning Artificial intelligence vs Machine Learning vs Deep Learning Introduction to Multi-Task Learning(MTL) for Deep Learning Top 10 Algorithms every Machine Learning Engineer should know Azure Virtual Machine for Machine Learning 30 minutes to machine learning What is AutoML in Machine Learning? Confusion Matrix in Machine Learning ",,,,
AI,"Machine learning Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1][2] It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to do so.[3] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[5][6] In its application across business problems, machine learning is also referred to as predictive analytics. Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.[7][8] The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.[7][8] Machine learning approaches Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the ""signal"" or ""feedback"" available to the learning system: Supervised learning: The computer is presented with example inputs and their desired outputs, given by a ""teacher"", and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.[4] Other approaches have been developed which don't fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example topic modeling, dimensionality reduction or meta learning.[9] As of 2020, deep learning has become the dominant approach for much ongoing work in the field of machine learning.[7] History and relationships to other fields See also: Timeline of machine learning The term machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence.[10][11] A representative book of the machine learning research during the 1960s was the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[12] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[13] In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[14] Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.""[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".[16] Artificial intelligence As a scientific endeavor, machine learning grew out of the quest for artificial intelligence. In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[17] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[18]:488 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[18]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[19] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[18]:708–710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[18]:25 Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[19] As of 2019, many sources continue to assert that machine learning remains a subfield of AI. Yet some practitioners, for example Dr Daniel Hulme, who teaches AI and runs a company operating in the field, argues that machine learning and AI are separate.[8][20][7] Data mining Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Optimization Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[21] Statistics Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[22] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[23] He also suggested the term data science as a placeholder to call the overall field.[23] Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[24] wherein ""algorithmic model"" means more or less the machine learning algorithms like Random forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[25] Theory Main articles: Computational learning theory and Statistical learning theory A core objective of a learner is to generalize from its experience.[4][26] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error. For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[27] In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. Approaches Types of learning algorithms The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve. Supervised learning Main article: Supervised learning A support vector machine is a supervised learning model that divides the data into regions separated by a linear boundary. Here, the linear boundary divides the black circles from the white. Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[28] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[29] An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[15] Types of supervised learning algorithms include Active learning, classification and regression.[30] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning Main article: Unsupervised learning See also: Cluster analysis Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function.[31] Though unsupervised learning encompasses other domains involving summarizing and explaining data features. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. Semi-supervised learning Main article: Semi-supervised learning Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[32] Reinforcement learning Main article: Reinforcement learning Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov Decision Process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[33] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Self learning Self-learning as machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning named Crossbar Adaptive Array (CAA).[34] It is a learning with no external rewards and no external teacher advices. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[35] The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: In situation s perform action a; Receive consequence situation s’; Compute emotion of being in consequence situation v(s’); Update crossbar memory  w’(a,s) = w(a,s) + v(s’). It is a system with only one input, situation s, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal seeking behavior, in an environment that contains both desirable and undesirable situations.[36] Feature learning Main article: Feature learning Several learning algorithms aim at discovering better representations of the inputs provided during training.[37] Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[38] and various forms of clustering.[39][40][41] Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[42] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[43] Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning Main article: Sparse dictionary learning Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[44] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[45] Anomaly detection Main article: Anomaly detection In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[46] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[47] In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[48] Three broad categories of anomaly detection techniques exist.[49] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model. Robot learning In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies and imitation. Association rules Main article: Association rule learning See also: Inductive logic programming Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of ""interestingness"".[50] Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves ""rules"" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[51] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli?ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[52] For example, the rule {displaystyle {mathrm {onions,potatoes} }Rightarrow {mathrm {burger} }}{{mathrm  {onions,potatoes}}}Rightarrow {{mathrm  {burger}}} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[53] Inductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[54][55][56] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[57] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. Models Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems. Artificial neural networks Main article: Artificial neural network See also: Deep learning An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called ""artificial neurons"", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a ""signal"", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called ""edges"". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[58] Decision trees Main article: Decision tree learning Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making. Support vector machines Main article: Support vector machines Support vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.[59] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Illustration of linear regression",,,,
AI,"et. Regression analysis Main article: Regression analysis Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization (mathematics) methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[60]), Logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher dimensional space. Bayesian networks Main article: Bayesian network A simple Bayesian network. Rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet. A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. Genetic algorithms Main article: Genetic algorithm A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[61][62] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[63] Training models Usually, machine learning models require a lot of data in order for them to perform well. Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set. Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Federated learning Main article: Federated learning Federated learning is an adapted form of Distributed Artificial Intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[64] ",,,,
AI,"Machine learning evolved from left to right as shown in the above diagram. Initially, researchers started out with Supervised Learning. This is the case of housing price prediction discussed earlier. This was followed by unsupervised learning, where the machine is made to learn on its own without any supervision. Scientists discovered further that it may be a good idea to reward the machine when it does the job the expected way and there came the Reinforcement Learning. Very soon, the data that is available these days has become so humongous that the conventional techniques developed so far failed to analyze the big data and provide us the predictions. Thus, came the deep learning where the human brain is simulated in the Artificial Neural Networks (ANN) created in our binary computers. The machine now learns on its own using the high computing power and huge memory resources that are available today. It is now observed that Deep Learning has solved many of the previously unsolvable problems. The technique is now further advanced by giving incentives to Deep Learning networks as awards and there finally comes Deep Reinforcement Learning. Let us now study each of these categories in more detail. Supervised Learning Supervised learning is analogous to training a child to walk. You will hold the child’s hand, show him how to take his foot forward, walk yourself for a demonstration and so on, until the child learns to walk on his own. Regression Similarly, in the case of supervised learning, you give concrete known examples to the computer. You say that for given feature value x1 the output is y1, for x2 it is y2, for x3 it is y3, and so on. Based on this data, you let the computer figure out an empirical relationship between x and y. Once the machine is trained in this way with a sufficient number of data points, now you would ask the machine to predict Y for a given X. Assuming that you know the real value of Y for this given X, you will be able to deduce whether the machine’s prediction is correct. Thus, you will test whether the machine has learned by using the known test data. Once you are satisfied that the machine is able to do the predictions with a desired level of accuracy (say 80 to 90%) you can stop further training the machine. Now, you can safely use the machine to do the predictions on unknown data points, or ask the machine to predict Y for a given X for which you do not know the real value of Y. This training comes under the regression that we talked about earlier. Classification You may also use machine learning techniques for classification problems. In classification problems, you classify objects of similar nature into a single group. For example, in a set of 100 students say, you may like to group them into three groups based on their heights - short, medium and long. Measuring the height of each student, you will place them in a proper group. Now, when a new student comes in, you will put him in an appropriate group by measuring his height. By following the principles in regression training, you will train the machine to classify a student based on his feature – the height. When the machine learns how the groups are formed, it will be able to classify any unknown new student correctly. Once again, you would use the test data to verify that the machine has learned your technique of classification before putting the developed model in production. Supervised Learning is where the AI really began its journey. This technique was applied successfully in several cases. You have used this model while doing the hand-written recognition on your machine. Several algorithms have been developed for supervised learning. You will learn about them in the following chapters. Unsupervised Learning In unsupervised learning, we do not specify a target variable to the machine, rather we ask machine “What can you tell me about X?”. More specifically, we may ask questions such as given a huge data set X, “What are the five best groups we can make out of X?” or “What features occur together most frequently in X?”. To arrive at the answers to such questions, you can understand that the number of data points that the machine would require to deduce a strategy would be very large. In case of supervised learning, the machine can be trained with even about few thousands of data points. However, in case of unsupervised learning, the number of data points that is reasonably accepted for learning starts in a few millions. These days, the data is generally abundantly available. The data ideally requires curating. However, the amount of data that is continuously flowing in a social area network, in most cases data curation is an impossible task. The following figure shows the boundary between the yellow and red dots as determined by unsupervised machine learning. You can see it clearly that the machine would be able to determine the class of each of the black dots with a fairly good accuracy. Reinforcement Learning Consider training a pet dog, we train our pet to bring a ball to us. We throw the ball at a certain distance and ask the dog to fetch it back to us. Every time the dog does this right, we reward the dog. Slowly, the dog learns that doing the job rightly gives him a reward and then the dog starts doing the job right way every time in future. Exactly, this concept is applied in “Reinforcement” type of learning. The technique was initially developed for machines to play games. The machine is given an algorithm to analyze all possible moves at each stage of the game. The machine may select one of the moves at random. If the move is right, the machine is rewarded, otherwise it may be penalized. Slowly, the machine will start differentiating between right and wrong moves and after several iterations would learn to solve the game puzzle with a better accuracy. The accuracy of winning the game would improve as the machine plays more and more games. The entire process may be depicted in the following diagram ? Game Puzzle This technique of machine learning differs from the supervised learning in that you need not supply the labelled input/output pairs. The focus is on finding the balance between exploring the new solutions versus exploiting the learned solutions. Deep Learning The deep learning is a model based on Artificial Neural Networks (ANN), more specifically Convolutional Neural Networks (CNN)s. There are several architectures used in deep learning such as deep neural networks, deep belief networks, recurrent neural networks, and convolutional neural networks. These networks have been successfully applied in solving the problems of computer vision, speech recognition, natural language processing, bioinformatics, drug design, medical image analysis, and games. There are several other fields in which deep learning is proactively applied. The deep learning requires huge processing power and humongous data, which is generally easily available these days. We will talk about deep learning more in detail in the coming chapters. Deep Reinforcement Learning The Deep Reinforcement Learning (DRL) combines the techniques of both deep and reinforcement learning. The reinforcement learning algorithms like Q-learning are now combined with deep learning to create a powerful DRL model. The technique has been with a great success in the fields of robotics, video games, finance and healthcare. Many previously unsolvable problems are now solved by creating DRL models. There is lots of research going on in this area and this is very actively pursued by the industries. So far, you have got a brief introduction to various machine learning models, now let us explore slightly deeper into various algorithms that are available under these models. ",,,,
AI,"Data mining Data mining is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate. The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps. The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[10] The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations. Free open-source data mining software and applications The following applications are available under free/open-source licenses. Public access to application source code is also available. Carrot2: Text and search results clustering framework. Chemicalize.org: A chemical structure miner and web search engine. ELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language. GATE: a natural language processing and language engineering tool. KNIME: The Konstanz Information Miner, a user-friendly and comprehensive data analytics framework. Massive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language. MEPX - cross-platform tool for regression and classification problems based on a Genetic Programming variant. ML-Flex: A software package that enables users to integrate with third-party machine-learning packages written in any programming language, execute classification analyses in parallel across multiple computing nodes, and produce HTML reports of classification results. mlpack: a collection of ready-to-use machine learning algorithms written in the C++ language. NLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language. OpenNN: Open neural networks library. Orange: A component-based data mining and machine learning software suite written in the Python language. R: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project. scikit-learn is an open-source machine learning library for the Python programming language Torch: An open-source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms. UIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video – originally developed by IBM. Weka: A suite of machine learning software applications written in the Java programming language. Proprietary data-mining software and applications The following applications are available under proprietary licenses. Angoss KnowledgeSTUDIO: data mining tool LIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach. Megaputer Intelligence: data and text mining software is called PolyAnalyst. Microsoft Analysis Services: data mining software provided by Microsoft. NetOwl: suite of multilingual text and entity analytics products that enable data mining. Oracle Data Mining: data mining software by Oracle Corporation. PSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE. Qlucore Omics Explorer: data mining software. RapidMiner: An environment for machine learning and data mining experiments. SAS Enterprise Miner: data mining software provided by the SAS Institute. SPSS Modeler: data mining software provided by IBM. STATISTICA Data Miner: data mining software provided by StatSoft. Tanagra: Visualisation-oriented data mining software, also for teaching. Vertica: data mining software provided by Hewlett-Packard. ",,,,
AI,"Natural language processing Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation. Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[12][13] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses. Statistical methods Since the so-called ""statistical revolution""[14][15] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of ""features"" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of handwritten rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks. Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required. Neural networks Further information: Artificial neural network A major drawback of statistical methods is that they require elaborate feature engineering. Since the early 2010s,[16] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Common NLP Tasks The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Text and speech processing Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text. Speech recognition Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed ""AI-complete"" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. Speech segmentation Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it. Text-to-speech Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.[17] Word segmentation (Tokenization) Separate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining. Morphological analysis Lemmatization The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., ""open, opens, opened, opening"") as separate words. In languages such as Turkish or Meitei,[18] a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, ""book"" can be a noun (""the book on the table"") or verb (""to book a flight""); ""set"" can be a noun, verb or adjective; and ""out"" can be any of at least five different parts of speech. Some languages have more such ambiguity than others.[dubious – discuss] Languages with little inflectional morphology, such as English, are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey the intended meaning. Stemming The process of reducing inflected (or sometimes derived) words to their root form. (e.g., ""close"" will be the root for ""closed"", ""closing"", ""close"", ""closer"" etc.). Syntactic analysis Grammar induction[19] Generate a formal grammar that describes a language's syntax. Sentence breaking (also known as ""sentence boundary disambiguation"") Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations). Parsing Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar). Lexical semantics (of individual words in context) Lexical semantics What is the computational meaning of individual words in context? Distributional semantics How can we learn semantic representations from data? Named entity recognition (NER) Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. Sentiment analysis (see also multimodal sentiment analysis) Extract subjective information usually from a set of documents, often using online reviews to determine ""polarity"" about specific objects. It is especially useful for identifying trends of public opinion in social media, for marketing. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word sense disambiguation Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet. Relational semantics (semantics of individual sentences) Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom). Semantic Parsing Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural Language Understanding below). Semantic Role Labelling (see also implicit semantic role labelling below) Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles). Discourse (semantics beyond individual sentences) Coreference resolution Given a sentence or larger chunk of text, determine which words (""mentions"") refer to the same objects (""entities""). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called ""bridging relationships"" involving referring expressions. For example, in a sentence such as ""He entered John's house through the front door"", ""the front door"" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). Discourse analysis This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.). Implicit Semantic Role Labelling Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic Role Labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages. Recognizing Textual entailment Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.[20] Topic segmentation and recognition Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment. Higher-level NLP applications Automatic summarization (text summarization) Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper. Book generation Not an NLP task proper but an extension of Natural Language Generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policemen's beard is half-constructed).[21] The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).[22] Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Dialogue management Computer systems intended to converse with a human. Machine translation Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed ""AI-complete"", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. Natural language generation (NLG): Convert information from computer databases or semantic intents into readable human language. Natural language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.[23] Question answering Given a human-language question, determine its answer. Typical questions have a specific right answer (such as ""What is the capital of Canada?""), but sometimes open-ended questions are also considered (such as ""What is the meaning of life?""). Recent works have looked at even more complex questions.[24] ",,,,
GV,"Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing. Computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues. Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities. Connected studies include: Applied mathematics Computational geometry Computational topology Computer vision Image processing Information visualization Scientific visualization Applications of computer graphics include: Print design Digital art Special effects Video games Visual effects A broad classification of major subfields in computer graphics might be: Geometry: ways to represent and process surfaces Animation: ways to represent and manipulate motion Rendering: algorithms to reproduce light transport Imaging: image acquisition or image editing Geometry Successive approximations of a surface computed using quadric error metrics The subfield of geometry studies the representation of three-dimensional objects in a discrete digital setting. Because the appearance of an object depends largely on its exterior, boundary representations are most commonly used. Two dimensional surfaces are a good representation for most objects, though they may be non-manifold. Since surfaces are not finite, discrete digital approximations are used. Polygonal meshes (and to a lesser extent subdivision surfaces) are by far the most common representation, although point-based representations have become more popular recently (see for instance the Symposium on Point-Based Graphics).[8] These representations are Lagrangian, meaning the spatial locations of the samples are independent. Recently, Eulerian surface descriptions (i.e., where spatial samples are fixed) such as level sets have been developed into a useful representation for deforming surfaces which undergo many topological changes (with fluids being the most notable example).[9] Geometry Subfields Implicit surface modeling – an older subfield which examines the use of algebraic surfaces, constructive solid geometry, etc., for surface representation. Digital geometry processing – surface reconstruction, simplification, fairing, mesh repair, parameterization, remeshing, mesh generation, surface compression, and surface editing all fall under this heading.[10][11][12] Discrete differential geometry – a nascent field which defines geometric quantities for the discrete surfaces used in computer graphics.[13] Point-based graphics – a recent field which focuses on points as the fundamental representation of surfaces. Subdivision surfaces Out-of-core mesh processing – another recent field which focuses on mesh datasets that do not fit in main memory. Animation The subfield of animation studies descriptions for surfaces (and other phenomena) that move or deform over time. Historically, most work in this field has focused on parametric and data-driven models, but recently physical simulation has become more popular as computers have become more powerful computationally. Subfields Performance capture Character animation Physical simulation (e.g. cloth modeling, animation of fluid dynamics, etc.) Rendering Indirect diffuse scattering simulated using path tracing and irradiance caching. Rendering generates images from a model. Rendering may simulate light transport to create realistic images or it may create images that have a particular artistic style in non-photorealistic rendering. The two basic operations in realistic rendering are transport (how much light passes from one place to another) and scattering (how surfaces interact with light). See Rendering (computer graphics) for more information. Transport Transport describes how illumination in a scene gets from one place to another. Visibility is a major component of light transport. Scattering Models of scattering and shading are used to describe the appearance of a surface. In graphics these problems are often studied within the context of rendering since they can substantially affect the design of rendering algorithms. Shading can be broken down into two orthogonal issues, which are often studied independently: scattering – how light interacts with the surface at a given point shading – how material properties vary across the surface The former problem refers to scattering, i.e., the relationship between incoming and outgoing illumination at a given point. Descriptions of scattering are usually given in terms of a bidirectional scattering distribution function or BSDF. The latter issue addresses how different types of scattering are distributed across the surface (i.e., which scattering function applies where). Descriptions of this kind are typically expressed with a program called a shader. (Note that there is some confusion since the word ""shader"" is sometimes used for programs that describe local geometric variation.) Other subfields Non-photorealistic rendering Physically based rendering – concerned with generating images according to the laws of geometric optics Real-time rendering – focuses on rendering for interactive applications, typically using specialized hardware like GPUs Relighting – recent area concerned with quickly re-rendering scenes ",,,,
GV,"Computer graphics is the branch of computer science[1][2] that deals with generating images with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). Some topics in computer graphics include user interface design, sprite graphics, rendering, ray tracing, geometry processing, computer animation, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization, image processing, computational photography, scientific visualization, computational geometry and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, physics, and perception. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design in general. The term computer graphics has been used in a broad sense to describe ""almost everything on computers that is not text or sound"".[3] Typically, the term computer graphics refers to several different things: the representation and manipulation of image data by a computer the various technologies used to create and manipulate images methods for digitally synthesizing and manipulating visual content, see study of computer graphics Today, computer graphics is widespread. Such imagery is found in and on television, newspapers, weather reports, and in a variety of medical investigations and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media ""such graphs are used to illustrate papers, reports, theses"", and other presentation material.[4] Many tools have been developed to visualize data. Computer generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like information visualization, and scientific visualization more concerned with ""the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component"".[5] Concepts and principles Images are typically created by devices such as cameras, mirrors, lenses, telescopes, microscopes, etc. Digital images include both vector images and raster images, but raster images are more commonly used. Pixel In the enlarged portion of the image individual pixels are rendered as squares and can be easily seen. In digital imaging, a pixel (or picture element[29]) is a single point in a raster image. Pixels are placed on a regular 2-dimensional grid, and are often represented using dots or squares. Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel has typically three components such as red, green, and blue. Graphics are visual presentations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style. Primitives Primitives are basic units which a graphics system may combine to create more complex images or models. Examples would be sprites and character maps in 2D video games, geometric primitives in CAD, or polygons or triangles in 3D rendering. Primitives may be supported in hardware for efficient rendering, or the building blocks provided by a graphics application. Rendering Rendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texture, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term ""rendering"" may be by analogy with an ""artist's rendering"" of a scene. Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a device able to assist the CPU in calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing file to produce final video output. 3D projection 3D projection is a method of mapping three dimensional points to a two dimensional plane. As most current methods for displaying graphical data are based on planar two dimensional media, the use of this type of projection is widespread. This method is used in most real-time 3D applications and typically uses rasterization to produce the final image. Ray tracing Ray tracing is a technique from the family of image order algorithms for generating an image by tracing the path of light through pixels in an image plane. The technique is capable of producing a high degree of photorealism; usually higher than that of typical scanline rendering methods, but at a greater computational cost. Shading Example of shading. Shading refers to depicting depth in 3D models or illustrations by varying levels of darkness. It is a process used in drawing for depicting levels of darkness on paper by applying media more densely or with a darker shade for darker areas, and less densely or with a lighter shade for lighter areas. There are various techniques of shading including cross hatching where perpendicular lines of varying closeness are drawn in a grid pattern to shade an area. The closer the lines are together, the darker the area appears. Likewise, the farther apart the lines are, the lighter the area appears. The term has been recently generalized to mean that shaders are applied. Texture mapping Texture mapping is a method for adding detail, surface texture, or colour to a computer-generated graphic or 3D model. Its application to 3D graphics was pioneered by Dr Edwin Catmull in 1974. A texture map is applied (mapped) to the surface of a shape, or polygon. This process is akin to applying patterned paper to a plain white box. Multitexturing is the use of more than one texture at a time on a polygon.[30] Procedural textures (created from adjusting parameters of an underlying algorithm that produces an output texture), and bitmap textures (created in an image editing application or imported from a digital camera) are, generally speaking, common methods of implementing texture definition on 3D models in computer graphics software, while intended placement of textures onto a model's surface often requires a technique known as UV mapping (arbitrary, manual layout of texture coordinates) for polygon surfaces, while non-uniform rational B-spline (NURB) surfaces have their own intrinsic parameterization used as texture coordinates. Texture mapping as a discipline also encompasses techniques for creating normal maps and bump maps that correspond to a texture to simulate height and specular maps to help simulate shine and light reflections, as well as environment mapping to simulate mirror-like reflectivity, also called gloss. Anti-aliasing Rendering resolution-independent entities (such as 3D models) for viewing on a raster (pixel-based) device such as a liquid-crystal display or CRT television inevitably causes aliasing artifacts mostly along geometric edges and the boundaries of texture details; these artifacts are informally called ""jaggies"". Anti-aliasing methods rectify such problems, resulting in imagery more pleasing to the viewer, but can be somewhat computationally expensive. Various anti-aliasing algorithms (such as supersampling) are able to be employed, then customized for the most efficient rendering performance versus quality of the resultant imagery; a graphics artist should consider this trade-off if anti-aliasing methods are to be used. A pre-anti-aliased bitmap texture being displayed on a screen (or screen location) at a resolution different than the resolution of the texture itself (such as a textured model in the distance from the virtual camera) will exhibit aliasing artifacts, while any procedurally defined texture will always show aliasing artifacts as they are resolution-independent; techniques such as mipmapping and texture filtering help to solve texture-related aliasing problems. Volume rendering Volume rendered CT scan of a forearm with different colour schemes for muscle, fat, bone, and blood. Volume rendering is a technique used to display a 2D projection of a 3D discretely sampled data set. A typical 3D data set is a group of 2D slice images acquired by a CT or MRI scanner. Usually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image pixels in a regular pattern. This is an example of a regular volumetric grid, with each volume element, or voxel represented by a single value that is obtained by sampling the immediate area surrounding the voxel. 3D modeling Main article: 3D modeling 3D modeling is the process of developing a mathematical, wireframe representation of any three-dimensional object, called a ""3D model"", via specialized software. Models may be created automatically or manually; the manual modeling process of preparing geometric data for 3D computer graphics is similar to plastic arts such as sculpting. 3D models may be created using multiple approaches: use of NURBs to generate accurate and smooth surface patches, polygonal mesh modeling (manipulation of faceted geometry), or polygonal mesh subdivision (advanced tessellation of polygons, resulting in smooth surfaces similar to NURB models). A 3D model can be displayed as a two-dimensional image through a process called 3D rendering, used in a computer simulation of physical phenomena, or animated directly for other purposes. The model can also be physically created using 3D Printing devices. ",,,,
GV,"Computer vision Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.[1][2][3] Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8] The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.[6] ",,,,
GV,"Computer animation is the process used for digitally generating animated images. The more general term computer-generated imagery (CGI) encompasses both static scenes and dynamic images, while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics to generate a two-dimensional picture, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well. Computer animation is essentially a digital successor to stop motion techniques, but using 3D models, and traditional animation techniques using frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other, more physically based processes, like constructing miniatures for effects shots, or hiring extras for crowd scenes, because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it but advanced slightly in time (usually at a rate of 24, 25, or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures. For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.[1] For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-user's computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations. To trick the eye and the brain into thinking they are seeing a smoothly moving object, the pictures should be drawn at around 12 frames per second or faster.[2] (A frame is one complete image.) With rates above 75-120 frames per second, no improvement in realism or smoothness is perceivable due to the way the eye and the brain both process images. At rates below 12 frames per second, most people can detect jerkiness associated with the drawing of new images that detracts from the illusion of realistic movement.[3] Conventional hand-drawn cartoon animation often uses 15 frames per second in order to save on the number of drawings needed, but this is usually accepted because of the stylized nature of cartoons. To produce more realistic imagery, computer animation demands higher frame rates. Films seen in theaters in the United States run at 24 frames per second, which is sufficient to create the illusion of continuous movement. For high resolution, adapters are used. In most 3D computer animation systems, an animator creates a simplified representation of a character's anatomy, which is analogous to a skeleton or stick figure.[20] They are arranged into a default position known as a bind pose, or T-Pose. The position of each segment of the skeletal model is defined by animation variables, or Avars for short. In human and animal characters, many parts of the skeletal model correspond to the actual bones, but skeletal animation is also used to animate other things, with facial features (though other methods for facial animation exist).[21] The character ""Woody"" in Toy Story, for example, uses 700 Avars (100 in the face alone). The computer doesn't usually render the skeletal model directly (it is invisible), but it does use the skeletal model to compute the exact position and orientation of that certain character, which is eventually rendered into an image. Thus by changing the values of Avars over time, the animator creates motion by making the character move from frame to frame. There are several methods for generating the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly.[22] Rather than set Avars for every frame, they usually set Avars at strategic points (frames) in time and let the computer interpolate or tween between them in a process called keyframing. Keyframing puts control in the hands of the animator and has roots in hand-drawn traditional animation.[23] In contrast, a newer method called motion capture makes use of live action footage.[24] When computer animation is driven by motion capture, a real performer acts out the scene as if they were the character to be animated.[25] His/her motion is recorded to a computer using video cameras and markers and that performance is then applied to the animated character.[26] Each method has its advantages and as of 2007, games and films are using either or both of these methods in productions. Keyframe animation can produce motions that would be difficult or impossible to act out, while motion capture can reproduce the subtleties of a particular actor.[27] For example, in the 2006 film Pirates of the Caribbean: Dead Man's Chest, Bill Nighy provided the performance for the character Davy Jones. Even though Nighy doesn't appear in the movie himself, the movie benefited from his performance by recording the nuances of his body language, posture, facial expressions, etc. Thus motion capture is appropriate in situations where believable, realistic behavior and action is required, but the types of characters required exceed what can be done throughout the conventional costuming. Modeling 3D computer animation combines 3D models of objects and programmed or hand ""keyframed"" movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model is intended to be a solid color, it must be painted with ""textures"" for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as rigging, the virtual marionette is given various controllers and handles for controlling movement.[28] Animation data can be created using motion capture, or keyframing by a human animator, or a combination of the two.[29] 3D models rigged for animation may contain thousands of control points — for example, ""Woody"" from Toy Story uses 700 specialized animation controllers. Rhythm and Hues Studios labored for two years to create Aslan in the movie The Chronicles of Narnia: The Lion, the Witch and the Wardrobe, which had about 1,851 controllers (742 in the face alone). In the 2004 film The Day After Tomorrow, designers had to design forces of extreme weather with the help of video references and accurate meteorological facts. For the 2005 remake of King Kong, actor Andy Serkis was used to help designers pinpoint the gorilla's prime location in the shots and used his expressions to model ""human"" characteristics onto the creature. Serkis had earlier provided the voice and performance for Gollum in J. R. R. Tolkien's The Lord of the Rings trilogy. Equipment A ray-traced 3-D model of a jack inside a cube, and the jack alone below. Computer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can require much time on an ordinary home computer.[30] Professional animators of movies, television and video games could make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used.[31] Graphics workstation computers use two to four processors, and they are a lot more powerful than an actual home computer and are specialized for rendering. Many workstations (known as a ""render farm"") are networked together to effectively act as a giant computer,[32] resulting in a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000-16,000 with the more expensive stations being able to render much faster due to the more technologically-advanced hardware that they contain. Professionals also use digital movie cameras, motion/performance capture, bluescreens, film editing software, props, and other tools used for movie animation. Programs like Blender allow for people who can't afford expensive animation and rendering software to be able to work in a similar manner to those who use the commercial grade equipment.[33] Facial animation Main article: Computer facial animation The realistic modeling of human facial features is both one of the most challenging and sought after elements in computer-generated imagery. Computer facial animation is a highly complex field where models typically include a very large number of animation variables.[34] Historically speaking, the first SIGGRAPH tutorials on State of the art in Facial Animation in 1989 and 1990 proved to be a turning point in the field by bringing together and consolidating multiple research elements and sparked interest among a number of researchers.[35] The Facial Action Coding System (with 46 ""action units"", ""lip bite"" or ""squint""), which had been developed in 1976, became a popular basis for many systems.[36] As early as 2001, MPEG-4 included 68 Face Animation Parameters (FAPs) for lips, jaws, etc., and the field has made significant progress since then and the use of facial microexpression has increased.[36][37] In some cases, an affective space, the PAD emotional state model, can be used to assign specific emotions to the faces of avatars.[38] In this approach, the PAD model is used as a high level emotional space and the lower level space is the MPEG-4 Facial Animation Parameters (FAP). A mid-level Partial Expression Parameters (PEP) space is then used to in a two-level structure – the PAD-PEP mapping and the PEP-FAP translation model.[39] Realism File:Joy & Heron - Animated CGI Spot by Passion Pictures.webm Joy & Heron - A typical example of realistic animation Realism in computer animation can mean making each frame look photorealistic, in the sense that the scene is rendered to resemble a photograph or make the characters' animation believable and lifelike.[40] Computer animation can also be realistic with or without the photorealistic rendering.[41] One of the greatest challenges in computer animation has been creating human characters that look and move with the highest degree of realism. Part of the difficulty in making pleasing, realistic human characters is the uncanny valley, the concept where the human audience (up to a point) tends to have an increasingly negative, emotional response as a human replica looks and acts more and more human. Films that have attempted photorealistic human characters, such as The Polar Express,[42][43][44] Beowulf,[45] and A Christmas Carol[46][47] have been criticized as ""creepy"" and ""disconcerting"". The goal of computer animation is not always to emulate live action as closely as possible, so many animated films instead feature characters who are anthropomorphic animals, legendary creatures and characters, superheroes, or otherwise have non-realistic, cartoon-like proportions.[48] Computer animation can also be tailored to mimic or substitute for other kinds of animation, like traditional stop-motion animation (as shown in Flushed Away or The Lego Movie). Some of the long-standing basic principles of animation, like squash & stretch, call for movement that is not strictly realistic, and such principles still see widespread application in computer animation.[49] Films File:Spring - Blender Open Movie.webm ""Spring"", a 3D animated short film made using Blender CGI short films have been produced as independent animation since 1976.[50] An early example of an animated feature film to incorporate CGI animation was the 1983 Japanese anime film Golgo 13: The Professional.[51] The popularity of computer animation (especially in the field of special effects) skyrocketed during the modern era of U.S. animation.[52] The first completely computer-animated movie was Toy Story (1995), but VeggieTales is the first American fully 3D computer animated series sold directly (made in 1993); its success inspired other animation series, such as ReBoot in 1994. While films like Avatar and The Jungle Book use CGI for the majority of the movie runtime, they still incorporate human actors into the mix.[53] ",,,,
GV,"Digital image processing is the use of a digital computer to process digital images through an algorithm.[1][2] As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased. Image sensors Main article: Image sensor The basis for modern image sensors is metal-oxide-semiconductor (MOS) technology,[5] which originates from the invention of the MOSFET (MOS field-effect transistor) by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.[6] This led to the development of digital semiconductor image sensors, including the charge-coupled device (CCD) and later the CMOS sensor.[5] The charge-coupled device was invented by Willard S. Boyle and George E. Smith at Bell Labs in 1969.[7] While researching MOS technology, they realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straighforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next.[5] The CCD is a semiconductor circuit that was later used in the first digital video cameras for television broadcasting.[8] The NMOS active-pixel sensor (APS) was invented by Olympus in Japan during the mid-1980s. This was enabled by advances in MOS semiconductor device fabrication, with MOSFET scaling reaching smaller micron and then sub-micron levels.[9][10] The NMOS APS was fabricated by Tsutomu Nakamura's team at Olympus in 1985.[11] The CMOS active-pixel sensor (CMOS sensor) was later developed by Eric Fossum's team at the NASA Jet Propulsion Laboratory in 1993.[12] By 2007, sales of CMOS sensors had surpassed CCD sensors.[13] Image compression Main article: Image compression An important development in digital image compression technology was the discrete cosine transform (DCT), a lossy compression technique first proposed by Nasir Ahmed in 1972.[14] DCT compression became the basis for JPEG, which was introduced by the Joint Photographic Experts Group in 1992.[15] JPEG compresses images down to much smaller file sizes, and has become the most widely used image file format on the Internet.[16] Its highly efficient DCT compression algorithm was largely responsible for the wide proliferation of digital images and digital photos,[17] with several billion JPEG images produced every day as of 2015.[18] Digital signal processor (DSP) Main article: Digital signal processor Electronic signal processing was revolutionized by the wide adoption of MOS technology in the 1970s.[19] MOS integrated circuit technology was the basis for the first single-chip microprocessors and microcontrollers in the early 1970s,[20] and then the first single-chip digital signal processor (DSP) chips in the late 1970s.[21][22] DSP chips have since been widely used in digital image processing.[21] The discrete cosine transform (DCT) image compression algorithm has been widely implemented in DSP chips, with many companies developing DSP chips based on DCT technology. DCTs are widely used for encoding, decoding, video coding, audio coding, multiplexing, control signals, signaling, analog-to-digital conversion, formatting luminance and color differences, and color formats such as YUV444 and YUV411. DCTs are also used for encoding operations such as motion estimation, motion compensation, inter-frame prediction, quantization, perceptual weighting, entropy encoding, variable encoding, and motion vectors, and decoding operations such as the inverse operation between different color formats (YIQ, YUV and RGB) for display purposes. DCTs are also commonly used for high-definition television (HDTV) encoder/decoder chips.[23] Medical imaging Further information: Medical imaging In 1972, the engineer from British company EMI Housfield invented the X-ray computed tomography device for head diagnosis, which is what we usually called CT (Computer Tomography). The CT nucleus method is based on the projection of the human head section and is processed by computer to reconstruct the cross-sectional image, which is called image reconstruction. In 1975, EMI successfully developed a CT device for the whole body, which obtained a clear tomographic image of various parts of the human body. In 1979, this diagnostic technique won the Nobel Prize.[4] Digital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.[24] Filtering Digital filters are used to blur and sharpen digital images. Filtering can be performed by: convolution with specifically designed kernels (filter array) in the spatial domain[25] masking specific frequency regions in the frequency (Fourier) domain Further information: Digital imaging and Applications of computer vision Digital camera images Digital cameras generally include specialized digital image processing hardware – either dedicated chips or added circuitry on other chips – to convert the raw data from their image sensor into a color-corrected image in a standard image file format. Film Westworld (1973) was the first feature film to use the digital image processing to pixellate photography to simulate an android's point of view.[28] ",,,,
GV,"Computational geometry From Wikipedia, the free encyclopedia Jump to navigationJump to search For the journal, see Computational Geometry (journal). Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation. The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization. Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction). The main branches of computational geometry are: Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term ""computational geometry"" in this sense by 1975.[1] Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term ""computational geometry"" in this meaning has been in use since 1971 Combinatorial computational geometry The primary goal of research in combinatorial computational geometry is to develop efficient algorithms and data structures for solving problems stated in terms of basic geometrical objects: points, line segments, polygons, polyhedra, etc. Some of these problems seem so simple that they were not regarded as problems at all until the advent of computers. Consider, for example, the Closest pair problem: Given n points in the plane, find the two with the smallest distance from each other. One could compute the distances between all the pairs of points, of which there are n(n-1)/2, then pick the pair with the smallest distance. This brute-force algorithm takes O(n2) time; i.e. its execution time is proportional to the square of the number of points. A classic result in computational geometry was the formulation of an algorithm that takes O(n log n). Randomized algorithms that take O(n) expected time,[3] as well as a deterministic algorithm that takes O(n log log n) time,[4] have also been discovered. Problem classes The core problems in computational geometry may be classified in different ways, according to various criteria. The following general classes may be distinguished. Static problems In the problems of this category, some input is given and the corresponding output needs to be constructed or found. Some fundamental problems of this type are: Convex hull: Given a set of points, find the smallest convex polyhedron/polygon containing all the points. Line segment intersection: Find the intersections between a given set of line segments. Delaunay triangulation Voronoi diagram: Given a set of points, partition the space according to which points are closest to the given points. Linear programming Closest pair of points: Given a set of points, find the two with the smallest distance from each other. Largest empty circle: Given a set of points, find a largest circle with its center inside of their convex hull and enclosing none of them. Euclidean shortest path: Connect two points in a Euclidean space (with polyhedral obstacles) by a shortest path. Polygon triangulation: Given a polygon, partition its interior into triangles Mesh generation Boolean operations on polygons The computational complexity for this class of problems is estimated by the time and space (computer memory) required to solve a given problem instance. ",,,,
GV,"A video game is an electronic game that involves interaction with a user interface or input device, such as a joystick, controller, keyboard, or motion sensing devices, to generate visual feedback on a two- or three-dimensional video display device such as a TV set, monitor, touchscreen, or virtual reality headset. Video games are augmented with audio feedback from speakers or headphones, and optionally with other types of feedback systems including haptic technology. Video games are defined based on their platform, which include arcade games, console games, and PC games. More recently, the industry has expanded onto mobile gaming through smartphones and tablet computers, and remote cloud gaming. Video games are classified into a wide range of genres based on their type of gameplay and purpose. The first video games were simple extensions of electronic games using video-like output from large room-size computers in the 1950s and 1960s, while the first video games available to consumers appears in 1972 through way of the Magnavox Odyssey home console, and the 1971 release of the arcade game Computer Space, followed the next year by Pong. Today, video game development requires numerous skills to bring a game to market, including developers, publishers, distributors, retailers, console and other third-party manufacturers, and other roles. Since the 2010s, the commercial importance of the video game industry has been increasing. The emerging Asian markets and mobile games on smartphones in particular are driving the growth of the industry. As of 2018, video games generated sales of US$134.9 billion annually worldwide,[1] and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV. Components of a video game Platform Various gaming consoles at the Computer Games Museum in Berlin Video games require a ""platform"", a specific combination of electronic components or computer hardware and associated software, to operate.[22] The term ""system"" is also commonly used. Games are typically designed to be played on one or a limited number of platforms, and exclusivity to a platform is used as a competitive edge in the video game market.[23] The list below is not exhaustive and excludes other electronic devices capable of playing video games such as PDAs and graphing calculators. Computer game Most computer games are PC games, referring to those that involve a player interacting with a personal computer (PC) connected to a video monitor.[24] Personal computers are not dedicated game platforms, so there may be differences running the same game on different hardware. Also, the openness allows some features to developers like reduced software cost,[25] increased flexibility, increased innovation, emulation, creation of modifications (""mods""), open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others. A gaming computer is a PC or laptop intended specifically for gaming typically using high-performance, high-cost components. In additional to personal computer gaming, there also exist games that work on mainframe computers and other similarly shared systems, with users logging in remotely to use the computer. Home console A ""console game"" is played on a specialized electronic device (""home video game console"") that connects to a common television set or composite video monitor, unlike PCs, which can run all sorts of computer programs, a console is a dedicated video game platform manufactured by a specific company. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include Xbox, PlayStation, and Nintendo. Handheld console A ""handheld"" gaming device is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardware. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games. A police-themed arcade game in which players use a light gun Arcade game An arcade game generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them. Browser game A browser game takes advantages of standardizations of technologies for the functionality of web browsers across multiple devices providing a cross-platform environment. These games may be identified based on the website that they appear, such as with ""Miniclip"" games. Others are named based on the programming platform used to develop them, such as Java and Flash games. Mobile game With the introduction of smartphones and tablet computers standardized on the iOS and Android operating systems, mobile gaming has become a significant platform. These games may utilize unique features of mobile devices that are not necessary present on other platforms, such as global positing information and camera devices to support augmented reality gameplay. Players using the PlayStation VR headsets in 2017 Virtual reality Virtual reality (VR) games generally require players to use a special head-mounted unit that provides stereoscopic screens and motion tracking to immerse a player within virtual environment that responds to their head movements. Some VR systems include control units for the player's hands as to provide a direct way to interact with the virtual world. VR systems generally require a separate computer, console, or other processing device that couples with the head-mounted unit. Emulation An emulator enables games from a console or otherwise different system to be run in a type of virtual machine on a modern system, simulating the hardware of the original and allows old games to be played. While emulators themselves have been found to be legal in United States case law, the act of obtaining the game software that one does not already own may violate copyrights. However, there are some official releases of emulated software from game manufacturers, such as Nintendo with its Virtual Console or Nintendo Switch Online offerings. Game media Early arcade games, home consoles, and handheld games were dedicated hardware units with the game's logic built into the electronic componentry of the hardware. Since then, most video game platforms have means to use multiple games distributed on different types of media or formats. Physical formats include ROM cartridges, magnetic storage including magnetic tape data storage and floppy discs, optical media formats including CD-ROM and DVDs, and flash memory cards. Furthermore digital distribution over the Internet or other communication methods as well as cloud gaming alleviate the need for any physical media. In some cases, the media serves as the direct read-only memory for the game, or it may be the form of installation media that is used to write the main assets to the player's platform's local storage for faster loading periods and later updates. Games can be extended with new content and software patches through either expansion packs which are typically available as physical media, or as downloadable content nominally available via digital distribution. These can be offered freely or can be used to monetize a game following its initial release. Several games offer players the ability to create user-generated content to share with others to play. Other games, mostly those on personal computers, can be extended with user-created modifications or mods that alter or add onto the game; these often are unofficial and were developed by players from reverse engineering of the game, but other games provide official support for modding the game.[26] Controller Main article: Game controller A North American Super NES game controller from the early 1990s Video game can use several types of input devices to translate human actions to a game. Most common are the use of game controllers like gamepads and joysticks for most consoles. Handheld consoles will have built in buttons and directional pads, similarly arcade games will have controls built into the console unit itself. Many games on personal computers can take advantage of keyboard and mouse controls. Other game controllers are commonly used for specific games like racing wheels, light guns or dance pads. Digital cameras can also be used as game controllers capturing movements of the body of the player. As technology continues to advance, more can be added onto the controller to give the player a more immersive experience when playing different games. There are some controllers that have presets so that the buttons are mapped a certain way to make playing certain games easier. Along with the presets, a player can sometimes custom map the buttons to better accommodate their play style. On keyboard and mouse, different actions in the game are already preset to keys on the keyboard. Most games allow the player to change that so that the actions are mapped to different keys that are more to their liking. The companies that design the controllers are trying to make the controller visually appealing and also feel comfortable in the hands of the consumer. An example of a technology that was incorporated into the controller was the touchscreen. It allows the player to be able to interact with the game differently than before. The person could move around in menus easier and they are also able to interact with different objects in the game. They can pick up some objects, equip others, or even just move the objects out of the player's path. Another example is motion sensor where a person's movement is able to be captured and put into a game. Some motion sensor games are based on where the controller is. The reason for that is because there is a signal that is sent from the controller to the console or computer so that the actions being done can create certain movements in the game. Other type of motion sensor games are webcam style where the player moves around in front of it, and the actions are repeated by a game character. Display and output Main article: Video game graphics By definition, all video games are intended to output graphics to an external video display, such as cathode-ray tube televisions, newer liquid-crystal display (LCD) televisions and built-in screens, projectors or computer monitors, depending on the type of platform the game is played on. Features such as color depth, refresh rate, frame rate, and screen resolution are a combination of the limitations of the game platform and display device and the program efficiency of the game itself. The game's output can range from fixed displays using LED or LCD elements, text-based games, two-dimensional and three-dimensional graphics, and augmented reality displays. The game's graphics are often accompanied by sound produced by internal speakers on the game platform or external speakers attached to the platform, as directed by the game's programming. This often will include sound effects tied to the player's actions to provide audio feedback, as well as background music for the game. Some platforms support additional feedback mechanics to the player that a game can take advantage of. This is most commonly haptic technology built into the game controller, such as causing the controller to shake in the player's hands to simulate a shaking earthquake occurring in game. ",,,,
SE,"Definitions Notable definitions of software engineering include: ""the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software""—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary[18] ""The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software""—IEEE Standard Glossary of Software Engineering Terminology[19] ""an engineering discipline that is concerned with all aspects of software production""—Ian Sommerville[20] ""the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines""—Fritz Bauer[21] ""a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs""—Merriam-Webster[22] The term has also been used less formally: as the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis;[23] as the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science;[24] as the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices.[25] Fields Main article: Outline of software engineering Software requirements Main article: Software requirements Requirements engineering is about the elicitation, analysis, specification, and validation of requirements for software. Software design Main article: Software design Software design is about the process of defining the architecture, components, interfaces, and other characteristics of a system or component. This is also called Software architecture. Software development Main article: Software development Software development, the main activity of software construction:[1][26] is the combination of programming (aka coding), verification, software testing, and debugging. A Software development process:[1][26] is the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses Software configuration management[1][26] which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning. Software testing Main article: Software testing Software testing:[1][26] is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test, with different approaches such as unit testing and integration testing. It is one aspect of software quality. Software maintenance Main article: Software maintenance Software maintenance:[1][26] refers to the activities required to provide cost-effective support after shipping the software product. Education Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2004, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.[27] Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014.[28] A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States. In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering. Profession Main articles: Software engineer and Software engineering professionalism Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario,[29] and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. The United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized.[30] NCEES will end the exam after April 2019 due to lack of participation.[31] Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license. The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014.[32] The IEEE also promulgates a ""Software Engineering Code of Ethics"".[33] Employment The U. S. Bureau of Labor Statistics counted 1,365,500 software developers holding jobs in the U.S. in 2018.[34] Employment of computer and information technology occupations is projected to grow 13 percent from 2016 to 2026, faster than the average for all occupations. These occupations are projected to add about 557,100 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security.[35] Yet, the BLS also says some employment in these occupations are slowing, especially for women[36], and computer programmers is projected to decline 7 percent from 2016 to 2026 since computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower.[37] Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.[38] Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Specializations include: in industry (analysts, architects, developers, testers, technical support, middleware analysts, managers) and in academia (educators, researchers). Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008[39]. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.[40] Certification The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture.[41] IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies.[42] These certification programs are tailored to the institutions that would employ people who use these technologies. Broader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP).[43] In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA).[44] The ACM had a professional certification program in the early 1980s,[citation needed] which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.[45] In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP).[46] In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng.[47] The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license. Impact of globalization The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.[48] Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.[49][50] Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations.[51] When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns. While global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations).[52] Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas. Controversy Criticism Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls[by whom?] for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field. Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters. One of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a ""theoretical environment."" Edsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of ""software engineering"" up until his death in 2002, arguing that those terms were poor analogies for what he called the ""radical novelty"" of computer science: A number of these phenomena have been bundled under the name ""Software Engineering"". As economics is known as ""The Miserable Science"", software engineering should be known as ""The Doomed Discipline"", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter ""How to program if you cannot.""[53] ",,,,
SE,"Job options Jobs directly related to your degree include: Applications developer Database administrator Game developer Multimedia programmer Web developer Web designer Software engineer Software tester Jobs where your degree would be useful include: Application analyst IT consultant IT technical support officer IT sales professional Sound designer Systems analyst Remember that many employers accept applications from graduates with any degree subject, so don't restrict your thinking to the jobs listed here. What is a C++ Developer? They can do a lot of things and may wear several hats. Get the low down down low on this hot programming career path. You may even find some salary data here. Check out the C++ Developer field here. What Are Redhat Systems Administrators Doing? These is not your Grandma's Red Hat Society. These are the cream of the crop when it comes to systems administration. Check out what it takes to become a Redhat Systems Administrator. Read about it here... One of the hottest clips in software engineering is in the game app development field. I recently spoke to a software engineer friend from the Bay Area who was telling me that all of his buddies are getting recruited into the game app development field with established as well as start up companies. He said, ""game app development is hot. Things are moving and shaking in a big way."" Here's a quick overview and some ideas on how to prepare yourself to enter the game app development field: Becoming a Game App Developer. Women in Computer Science A field dominated by men? We think not. Check out some of the latest data and find out which colleges and universities are focusing their efforts on attracting more women to the computer science field: Women in Computer Science Software Development, A Place for Women? Women in Glasses staring at computer monitors? Is there an image thing going on that deters women from entering this field? Are there organizations encouraging women to enter the field? I wanted to know what a woman can do, young or mid-career, to get into the software development field... Women in Software Development Women in Software Engineering Underrepresented and underpayed. Things are a changing though.... Women in Software Engineering Software Maintenance: The art of Debugging and Fixing A critical component in software engineering, maintenance is often delegated to entry level software engineers. Don't let the title fool you, this is an area where you can gain valuable in depth knowledge of programming that can pay huge dividends later on. Who knows, you may develop a passion for this area...Software Maintenance Software Testing Engineers...Getting the fix Rarely does it ever happen that s programming project gets done and performs without a hitch. Software testing is an important and on-going part of the software development life cycle. Learn more about this career and the educational paths that professionals have taken to arrive in a software testing engineer position....Software Testing Engineer Cloud Computing Opportunities Ever wondered what cloud computing is all about? Are there job opportunities in this field? Check out ""Transitioning to the Cloud"" to learn more about this quickly growing field. What in the World Does a Software Designer Do? Designs Software. Designing is actually only one piece of the software development life cycle and software designers need to know it all. This field allows for an opportunity to use your creativity coupled with software design knowledge to create amazing things. Check it out...Software Designer Cyber Security Management Hundreds of companies around the world are hacked each year. The White House fell victim to an email phishing scheme. Cyber Security is a field critical to the protection of consumers, corporations and government agencies. Learn more about this field and why it might be for you. Read the article... Information Security Engineers Keeping IT Secure Legally hacking into a system to see if it is secure and getting paid for it? Sign me up. This is one role of an Information Security manager. Check out what it takes to get a start in this field and learn a bit more about what they do. Read on.... Animation Programmers and Engineers: Creative and Passionate A passion for creating and a desire to apply hard science and precise engineering to developing video games. That sounds pretty good. Explore this field if it sounds up your alley. You may want to check it out even if it doesn't. Read the article What Are SAP Developers? SAP is only the fourth largest software company in the world. Rivaling Microsoft. They have their own programming language and lots of opportunity for talented developers and tech-savvy business folks. Check out how you could become a SAP Developer... What is a Perl Developer or Engineer? As of October 2012, it is only the 9th most popular programming language on the TIOBE list. If you aren't familiar with the list, this is kind of a big deal. Learn more about this path and what it takes to become a Perl developer or engineer. Read on here... What is a Java Developer? What are they doing for these companies that make this profession so sought after? What training do they have and what certifications are they getting? Find this information and more here... What is Embedded Software Engineering? Building invisible computers? That's how one embedded software engineer put it. Learn about this exciting area of software engineering and what can help you prepare for a career in this area. Read more now... What is a Computer Network Engineer? It's only a rapidly growing field in tech. Typical academic preparation often involves a Bachelor's degree in Computer Science. Read on to learn more about this path...Check it out. What is a PHP Developer? PHP is one of the hottest programming languages out there. Employers are scraping for developers with knowledge of PHP. Learn about this language and some of the traits a PHP developer commonly possess. Read more about the PHP Developer path here. Facebook App Developers I've been hearing these guys and gals are the most sought after specialists on the market right now. So what are they doing? Who is hiring them? What does it take to become one of them? Facebook App Development: A Growing Industry Smart Opportunity as A Smartphone APP Developer Android Apps, iPhone Apps, Blackberry Apps oh my! Yup, forecasts show the smartphone app industry booming to the tune of $15 Billion annually by 2013. Why not get a piece of this action? Smartphone App Development: A Now Opportunity Software Development Career Path Find out what it takes to become a software developer. Learn about some of the specializations and areas of software development as well as educational paths. Job outlook and salary data looks promising according to the BLS. Check this field and all its options out...Software Developer What is Computer Engineering Technology I wanted to find out. This is a different educational path than there is in computer engineering. Learn more about this field...Computer Engineering Technology Software Configuration Management The Software Configuration Management field is something most people learn about once they are already in a software engineering or similar degree program. Fortunately for you, you are going to have a head start. Check it out...Software Configuration Management Software Engineering Management..Everyone wants to be the boss Going into management is a logical step for an experienced and educated software engineer. In most cases there are higher salaries in managerial roles. With higher salaries comes increased responsibilities. Learn about the path to management in software engineering...Software Engineering Management Software Quality Engineering... This is the quality assurance area in software engineering and development. Necessary attributes to proper functioning include maintaining speed of a system or program, user friendliness and security among other important facets. This area requires good communication skills as often times a software quality engineer will speak directly to clients about issues. Learn more...Software Quality Engineer A Niche Field..Software Requirements Engineering Only one of the more critical phases of the software development life cycle, software requirements engineers work with clients to conceptualize and gather requirements prior to the design phase begins. Learn more...Software Requirements Engineering What is a Software Architect and How Do You Become One? Like a traditional architect, a software architect is one of the many people involved in the construction of a project. The software architect plays the biggest role in the beginning of a project, bringing a vision or idea to life. This is not often an entry-level position, so read up on how to prepare for a role in this field. Software Architect, Check it out ==> What Do Systems Integration Engineers do? Help Computers talk to each other. really. Learn about the educational path options and professional What Does a Software Engineer Do? Computer software engineers apply engineering principles and systematic methods to develop programs and operating data for computers. If you have ever asked yourself, ""What does a software engineer do?"" note that daily tasks vary widely. Professionals confer with system programmers, analysts, and other engineers to extract pertinent information for designing systems, projecting capabilities, and determining performance interfaces. Computer software engineers also analyze user needs, provide consultation services to discuss design elements, and coordinate software installation. Designing software systems requires professionals to consider mathematical models and scientific analysis to project outcomes. The demand for high-performing computer software engineers continues to grow. According to projections from the Bureau of Labor Statistics (BLS), job growth should increase by 21% through 2028, much faster than the national average. Computer software engineers can find employment in almost any industry. In fact, over half a million new computer and information technology jobs are projected to be added to the workforce. Furthermore, as software engineering is synonymous with innovation, new programming tools lead to inexplicable dilemmas that demand professionals with cutting-edge knowledge and skills to solve them. KEY HARD SKILLS Hard skills refers to practical, teachable competencies that an employee must develop to qualify for a particular position. Examples of hard skills for software engineers include learning to code with programming languages such as Java, SQL, and Python. Java: This programming language produces software on multiple platforms without the need for recompilation. The code runs on nearly all operating systems including Mac OS or Windows. Java uses syntax from C and C++ programming. Browser-operated programs facilitate GUI and object interaction from users. JavaScript: This scripting programming language allows users to perform complex tasks and is incorporated in most webpages. This language allows users to update content, animate images, operate multimedia, and store variables. JavaScript represents one of the web's three major technologies. SQL: Also known as Structured Query Language, SQL queries, updates, modifies, deletes, and inserts data. To achieve this, SQL uses a set number of commands. This computer language is standard for the manipulation of data and relational database management. Professionals use SQL to manage structured data where relationships between variables and entities exist. C++: Regarded as an object-oriented, general purpose programming language, C++ uses both low and high-level language. Given that virtually all computers contain C++, computer software engineers must understand this language. C++ encompases most C programs without switching the source code line. C++ primarily manipulates text, numbers, and other computer-capable tasks. C#: Initially developed for Microsoft, this highly expressive program language is more simple in comparison to other languages, yet it includes components of C++ and Java. Generic types and methods provide additional safety and increased performance. C# also allows professionals to define iteration behavior, while supporting encapsulation, polymorphism, and inheritance. Python: This high-level programing language contains dynamic semantics, structures, typing, and binding that connect existing components; however, the Python syntax is easy to learn with no compilation stage involved, reducing program maintenance and enhancing productivity. Python also supports module and package use, which allows engineers to use the language for varying projects. Programming languages comprise a software engineer's bread and butter, with nearly as many options to explore as there are job possibilities. Examples include Ruby, an object-oriented language that works in blocks; Rust, which integrates with other languages for application development; PHP, a web development script that integrates with HTML; and Swift, which can program apps for all Apple products. Learn more about programming languages here. KEY SOFT SKILLS While hard skills like knowledge of programming languages are essential, software engineers must also consider which soft skills they may need to qualify for the position they seek. Soft skills include individual preferences and personality traits that demonstrate how an employee performs their duties and fits into a team. Communication: Whether reporting progress to a supervisor, explaining a product to a client, or coordinating with team members to work on the same product, software engineers must be adept at communicating via email, phone, and in-person meetings. Multitasking: Software development can require engineers to split attention across different modules of the same project, or switch easily between projects when working on a deadline or meeting team needs. Organization: To handle multiple projects through their various stages and keep track of details, software engineers must demonstrate a certain level of organization. Busy supervisors oversee entire teams and need to access information efficiently at a client's request. Attention to Detail: Concentration plays a critical role for software engineers. They must troubleshoot coding issues and bugs as they arise, and keep track of a host of complex details surrounding multiple ongoing projects. DAILY TASKS Depending on the particular position, the daily responsibilities of a software engineer can vary. Software engineers may confer with clients or executives to begin the development of a project, designing programs to meet those expectations. They assemble charts and diagrams for visual representation of the software, writing code themselves in addition to supervising a team of programmers. They also run tests and fix issues that may occur with the programs they have designed. ",,,,
SE,"Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic. Tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is the opposite process. A hacker is any skilled computer expert that uses their technical knowledge to overcome a problem, but it can also mean a security hacker in common language. Quality requirements Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important: Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors). Robustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services and network connections, user error, and unexpected power outages. Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface. Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code. Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customisations, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term. Efficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks. This is often discussed under the shadow of a chosen programming language. The language certainly effects the performance, but even slower languages such as python can execute programs, from a human perspective, instantly. The speed, resource usage, and performance may be important for some programs which require it, but many programs that are not bottlenecked by speed of hardware, in general, do not require optimization. Readability of source code In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability. Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study[11] found that a few simple readability transformations made code shorter and drastically reduced the time to understand it. Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[12] Some of these factors include: Different indent styles (whitespace) Comments Decomposition Naming conventions for objects (such as variables, classes, procedures, etc.) The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills. Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability. Algorithmic complexity The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances. Chess algorithms as an example ""Programming a Computer for Playing Chess"" was a 1950 paper that evaluated a ""minimax"" algorithm that is part of the history of algorithmic complexity; a course on IBM's Deep Blue (chess computer) is part of the computer science curriculum at Stanford University.[13] Methodologies The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process. Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA. A similar technique used for database design is Entity-Relationship Modeling (ER Modeling). Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages. Measuring language usage Main article: Measuring programming language popularity It is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[14] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL). Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[15] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation). Debugging The first known actual bug causing a problem in a running program. ""Bug"" was already a common term for a software defect when this bug was found. Main article: Debugging Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem. After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it crash when passing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be done manually, using a divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Debugging is often done with IDEs like Eclipse, Visual Studio, Xcode, Kdevelop, NetBeans and Code::Blocks. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment. Programming languages Main articles: Programming language and List of programming languages Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from ""low-level"" to ""high-level""; ""low-level"" languages are typically more machine-oriented and faster to execute, whereas ""high-level"" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in ""high-level"" languages than in ""low-level"" ones. Allen Downey, in his book How To Think Like A Computer Scientist, writes: The details look different in different languages, but a few basic instructions appear in just about every language: Input: Gather data from the keyboard, a file, or some other device. Output: Display data on the screen or send data to a file or other device. Arithmetic: Perform basic arithmetical operations like addition and multiplication. Conditional Execution: Check for certain conditions and execute the appropriate sequence of statements. Repetition: Perform some action repeatedly, usually with some variation. Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language. Programmers Main article: Programmer See also: Software developer and Software engineer Computer programmers are those who write computer software. Their jobs usually involve: Coding Debugging Documentation Integration Maintenance Requirements analysis Software architecture Software testing Specification ",,,,
SE,"List of programming languages From Wikipedia, the free encyclopedia Jump to navigationJump to search Programming language lists Alphabetical Categorical Chronological Generational vte The aim of this list of programming languages is to include all notable programming languages in existence, both those in current use and historical ones, in alphabetical order. Dialects of BASIC, esoteric programming languages, and markup languages are not included. Contents: 0–9ABCDEFGHIJKLMNOPQRSTUVWXYZ See also A A.NET A-0 System A+ ABAP ABC ABC ALGOL ACC Accent Ace DASL (Distributed Application Specification Language) Action! ActionScript Actor Ada Adenine Agda Agilent VEE Agora AIMMS Aldor Alef ALF ALGOL 58 ALGOL 60 ALGOL 68 ALGOL W Alice Alma-0 AmbientTalk Amiga E AMOS AMPL AngelScript Apache Pig latin Apex APL App Inventor for Android's visual block language AppleScript APT Arc ARexx Argus Assembly language AutoHotkey AutoIt AutoLISP / Visual LISP Averest AWK Axum B B Babbage Ballerina Bash BASIC Batch file (Windows/MS-DOS) bc BCPL BeanShell Bertrand BETA BLISS Blockly BlooP Boo Boomerang Bosque C C C-- (C minus minus) C++ (C plus plus) – ISO/IEC 14882 C* C# (C sharp) – ISO/IEC 23270 C/AL Caché ObjectScript C Shell (csh) Caml Cayenne CDuce Cecil Cesil Céu Ceylon CFEngine Cg Ch Chapel Charm CHILL CHIP-8 chomski ChucK Cilk Citrine CL (IBM) Claire Clarion Clean Clipper CLIPS CLIST Clojure CLU CMS-2 COBOL – ISO/IEC 1989 CobolScript – COBOL Scripting language Cobra CoffeeScript ColdFusion COMAL Combined Programming Language (CPL) COMIT Common Intermediate Language (CIL) Common Lisp (also known as CL) COMPASS Component Pascal Constraint Handling Rules (CHR) COMTRAN Cool Coq Coral 66 CorVision COWSEL CPL Cryptol Crystal Csound Cuneiform Curl Curry Cybil Cyclone Cypher Query Language Cython CEEMAC D D Dart Darwin DataFlex Datalog DATATRIEVE dBase dc DCL DinkC DIBOL Dog Draco DRAKON Dylan DYNAMO DAX (Data Analysis Expressions) E E Ease Easy PL/I EASYTRIEVE PLUS eC ECMAScript Edinburgh IMP EGL Eiffel ELAN Elixir Elm Emacs Lisp Emerald Epigram EPL (Easy Programming Language) EPL (Eltron Programming Language) Erlang es Escher ESPOL Esterel Etoys Euclid Euler Euphoria EusLisp Robot Programming Language CMS EXEC (EXEC) EXEC 2 Executable UML Ezhil F F F# F* Factor Fantom FAUST FFP fish Fj?lnir FL Flavors Flex FlooP FLOW-MATIC FOCAL FOCUS FOIL FORMAC @Formula Forth Fortran – ISO/IEC 1539 Fortress FP Franz Lisp Futhark F-Script G Game Maker Language (Scripting language) GameMonkey Script GAMS GAP G-code GDScript Genie GDL GEORGE GLSL GNU E GNU Guile Go Go! GOAL G?del Golo GOM (Good Old Mad) Google Apps Script Gosu GOTRAN GPSS GraphTalk GRASS Grasshopper Groovy H Hack HAGGIS HAL/S Halide (programming language) Hamilton C shell Harbour Hartmann pipelines Haskell Haxe Hermes High Level Assembly HLSL Hollywood HolyC Hop Hopscotch Hope Hugo Hume HyperTalk I Io Icon IBM Basic assembly language IBM HAScript IBM Informix-4GL IBM RPG IDL Idris Inform J J J# J++ JADE JAL Janus (concurrent constraint programming language) Janus (time-reversible computing programming language) JASS Java JavaFX Script JavaScript(Scripting language) Jess (programming language) JCL JEAN Join Java JOSS Joule JOVIAL Joy JScript JScript .NET Julia Jython K K Kaleidoscope Karel KEE Kixtart Klerer-May System KIF Kojo Kotlin KRC KRL KRL (KUKA Robot Language) KRYPTON Korn shell (ksh) Kodu Kv L LabVIEW Ladder LANSA Lasso Lava LC-3 Legoscript LIL LilyPond Limbo Limnor LINC Lingo LINQ LIS LISA Language H Lisp – ISO/IEC 13816 Lite-C Lithe Little b LLL Logo Logtalk LotusScript LPC LSE LSL LiveCode LiveScript Lua Lucid Lustre LYaPAS Lynx M M2001 M4 M# Machine code MAD (Michigan Algorithm Decoder) MAD/I Magik Magma M?ni Maple MAPPER (now part of BIS) MARK-IV (now VISION:BUILDER) Mary MATLAB MASM Microsoft Assembly x86 MATH-MATIC Maude system Maxima (see also Macsyma) Max (Max Msp – Graphical Programming Environment) MaxScript internal language 3D Studio Max Maya (MEL) MDL Mercury Mesa MHEG-5 (Interactive TV programming language) Microcode MicroScript MIIS Milk (programming language) MIMIC Mirah Miranda MIVA Script ML Model 204 Modelica Modula Modula-2 Modula-3 Mohol MOO Mortran Mouse MPD MSL MUMPS MuPAD Mutan Mystic Programming Language (MPL) N NASM Napier88 Neko Nemerle NESL Net.Data NetLogo NetRexx NewLISP NEWP Newspeak NewtonScript Nial Nickle (NITIN) Nim Nix (Systems configuration language) NPL Not eXactly C (NXC) Not Quite C (NQC) NSIS Nu NWScript NXT-G O o:XML Oak Oberon OBJ2 Object Lisp ObjectLOGO Object REXX Object Pascal Objective-C Objective-J Obliq OCaml occam occam-? Octave OmniMark Opa Opal OpenCL OpenEdge ABL OPL OpenVera OPS5 OptimJ Orc ORCA/Modula-2 Oriel Orwell Oxygene Oz P P P4 P?? ParaSail (programming language) PARI/GP Pascal – ISO 7185 Pascal Script PCASTL PCF PEARL PeopleCode Perl PDL Pharo PHP Pico Picolisp Pict Pike PILOT Pipelines Pizza PL-11 PL/0 PL/B PL/C PL/I – ISO 6160 PL/M PL/P PL/SQL PL360 PLANC Plankalkül Planner PLEX PLEXIL Plus POP-11 POP-2 PostScript PortablE POV-Ray SDL Powerhouse PowerBuilder – 4GL GUI application generator from Sybase PowerShell PPL Processing Processing.js Prograph PROIV Prolog PROMAL Promela PROSE modeling language PROTEL ProvideX Pro*C Pure Pure Data PureScript Python Q Q (programming language from Kx Systems) Q# (Microsoft programming language) Qalb Quantum Computation Language QtScript QuakeC QPL .QL R R R++ Racket Raku RAPID Rapira Ratfiv Ratfor rc Reason REBOL Red Redcode REFAL REXX Rlab ROOP RPG RPL RSL RTL/2 Ruby Rust S S S2 S3 S-Lang S-PLUS SA-C SabreTalk SAIL SAS SASL Sather Sawzall Scala Scheme Scilab Scratch Script.NET Sed Seed7 Self SenseTalk SequenceL Serpent SETL SIMPOL SIGNAL SiMPLE SIMSCRIPT Simula Simulink Singularity SISAL SLIP SMALL Smalltalk SML Strongtalk Snap! SNOBOL (SPITBOL) Snowball SOL Solidity SOPHAEROS Source SPARK Speakeasy Speedcode SPIN SP/k SPS SQL SQR Squeak Squirrel SR S/SL Starlogo Strand Stata Stateflow Subtext SBL SuperCollider SuperTalk Swift (Apple programming language) Swift (parallel scripting language) SYMPL SystemVerilog T T TACL TACPOL TADS TAL Tcl Tea TECO TELCOMP TeX TEX TIE TMG, compiler-compiler Tom Toi Topspeed TPU Trac TTM T-SQL Transcript TTCN Turing TUTOR TXL TypeScript Tynker U Ubercode UCSD Pascal Umple Unicon Uniface UNITY Unix shell UnrealScript V Vala Verilog VHDL Vim script Viper Visual DataFlex Visual DialogScript Visual FoxPro Visual J++ Visual LISP Visual Objects Visual Prolog VSXu W WATFIV, WATFOR WebAssembly WebDNA Whiley Winbatch Wolfram Language Wyvern X X++ X10 xBase xBase++ XBL XC (targets XMOS architecture) xHarbour XL Xojo XOTcl Xod XPL XPL0 XQuery XSB XSharp XSLT Xtend Y YAML Yorick YQL Yoix YUI Z Z notation Zebra, ZPL, ZPL2 Zeno ZetaLisp ZOPL ZPL ",,,,
SE,"This article is about people who write computer software. For other uses, see Programmer (disambiguation). For someone who performs coding in the social sciences, see Coding (social sciences). For someone who performs medical coding, see Medical coding. ""Coder"" redirects here. For the settlement in Pennsylvania, see Coder, Pennsylvania. Two programmers working on an IBM 704 at NACA, 1954 A computer programmer, sometimes called a software developer, a programmer or more recently a coder (especially in more informal contexts), is a person who creates computer software. The term computer programmer can refer to a specialist in one area of computers, or to a generalist who writes code for many kinds of software. A programmer's most oft-used computer language (e.g., Assembly, COBOL, C, C++, C#, JavaScript, Lisp, Python) may be prefixed to the term programmer. Some who work with web programming languages also prefix their titles with web. A range of occupations that involve programming also often require a range of other, similar skills, for example: (software) developer, web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, game programmer, game developer and software analyst. The use of the term programmer as applied to these positions is sometimes considered an insulting simplification or even derogatory.[1][2][3][4][5] History See also: Computer program § History, Computer programming § History, and Programming language § History Ada Lovelace is considered by many to be the first computer programmer.[6] British countess and mathematician Ada Lovelace is often considered to be the first computer programmer, as she was the first to publish part of a program (specifically an algorithm) intended for implementation on Charles Babbage's analytical engine, in October 1842. The algorithm was used to calculate Bernoulli numbers.[7] Because Babbage's machine was never completed as a functioning standard in Lovelace's time, she unfortunately never had the opportunity to see the algorithm in action. The first person to execute a program on a functioning, modern, electronic computer was the renowned computer scientist Konrad Zuse, in 1941. Betty Jennings and Fran Bilas, part of the first ENIAC programming team The ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.[8][9] International Programmers' Day is celebrated annually on 7 January.[10] In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had already been an unofficial holiday before that in many countries. Software The word software was used as early as 1953, but did not regularly appear in print until the 1960s.[11] Before this time, computers were programmed either by customers or the few commercial computer manufacturers of the time, such as UNIVAC and IBM. The first company founded to specifically provide software products and services was the Computer Usage Company, in 1955.[12] The software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, governments, and businesses created a demand for software. Many of these programs were written in-house by full-time staff programmers; some were distributed freely between users of a particular machine for no charge. And others were developed on a commercial basis. Other firms, such as Computer Sciences Corporation (founded in 1959) also started to grow. The computer/hardware manufacturers soon started bundling operating systems, system software and programming environments with their machines.[citation needed] The industry expanded greatly with the rise of the personal computer (""PC"") in the mid-1970s, which brought computing to the average office worker. In the following years the PC also helped create a constantly-growing market for games, applications and utilities software.[13] In the early years of the 21st century, another successful business model has arisen for hosted software, called software-as-a-service, or SaaS. From the point of view of producers of some proprietary software, SaaS reduces the concerns about unauthorized copying, since it can only be accessed through the Web, and by definition, no client software is loaded onto the end user's PC. SaaS is typically run out of the cloud.[14] Nature of the work Some of this section is from the Occupational Outlook Handbook, 2006–07 Edition, which is in the public domain as a work of the United States Government. Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization. Programmers work in many settings, including corporate information technology (""IT"") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some[who?] authorities disagree on the grounds that only careers with legal licensing requirements count as a profession). Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer's supervision. A software engineer writing JavaScript Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ and Python are widely used for both scientific and business applications. Java, C#, JS and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as Java programmers, or by the type of function they perform or the environment in which they work: for example, database programmers, mainframe programmers, or Web developers. When making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps. Testing and debugging Programmers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called maintenance programming. Programmers may contribute to user guides and online help, or they may work with technical writers to do such work. Application versus system programming Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives. Qualifications and skills A software developer needs to have deep technical expertise with certain aspects of computing. Some positions will require a degree in a relevant field such as computer science, information technology, engineering, programming, or any other IT related post-graduate studies.[15] An ideal software developer is a self-motivated professional carrying a dynamic hands-on experience on key languages of programming such as C++, C#, PHP, Java, C, Javascript, Visual Basic, Python, and Smalltalk. According to developer Eric Sink, the differences between system design, software development, and programming are more apparent. Already in the current market place, there can be found a segregation between programmers and developers, in that one who implements is not the same as the one who designs the class structure or hierarchy. Even more, so that developers become software architects or systems architects, those who design the multi-leveled architecture or component interactions of a large software system.[16] Types of software Programmers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is, for example, the case in research laboratories.[citation needed] In some organizations, particularly small ones, people commonly known as programmer analysts are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end-users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.[citation needed] In addition, the rise of the Internet has made web development a huge part of the programming field. Currently, more software applications are web applications that can be used by anyone with a web browser.[citation needed] Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service. Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.[17] Globalization Globe icon. The examples and perspective in this article deal primarily with the United States and do not represent a worldwide view of the subject. You may improve this article, discuss the issue on the talk page, or create a new article, as appropriate. (December 2010) (Learn how and when to remove this template message) Market changes in the UK According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey.[18] The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.[19] Market changes in the US Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.[20] Large companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and avoid previously employer-paid training using industry-specific technologies not covered in most accredited degree programs.[21][22] Other reasons for employers claiming skill shortages is the result of their own cost-saving combining of several disparate skill sets previously held by several specialized programmers into fewer generalized multifaceted positions that are unlikely to have enough "" qualified"" candidates with the desired experience.[23] Enrollment in computer-related degrees in the US has dropped for years, especially for women[24], due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers.[25] This situation has resulted in confusion about whether the US economy is entering a ""post-information age"" and the nature of US comparative advantages. Most academic institutions have an Institutional research office that keeps past statistics of degrees conferred which shows several dips and rises in Computer Science degrees over the past 30 years. The overall trend shows a slightly overall decline in growth (especially when compared to other STEM degree growth) since certain peaks of 1986, 1992, 2002, and 2008 showing periods of flat growth or even declines.[26], In addition, the U.S. Bureau of Labor Statistics Occupational Outlook 2016-26 is -7% (a decline in their words) for Computer Programmers because Computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower.[27] ",,,,
ARC,"Computer architecture In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.[1] In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.[2] Subcategories The discipline of computer architecture has three main subcategories:[13] Instruction set architecture (ISA): defines the machine code that a processor reads and acts upon as well as the word size, memory address modes, processor registers, and data type. Microarchitecture: also known as ""computer organization"", this describes how a particular processor will implement the ISA.[14] The size of a computer's CPU cache for instance, is an issue that generally has nothing to do with the ISA. Systems design: includes all of the other hardware components within a computing system, such as data processing other than the CPU (e.g., direct memory access), virtualization, and multiprocessing There are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002[13] to count for 1% of all of computer architecture: Macroarchitecture: architectural layers more abstract than microarchitecture Assembly instruction set architecture: A smart assembler may convert an abstract assembly language common to a group of machines into slightly different machine language for different implementations. Programmer-visible macroarchitecture: higher-level language tools such as compilers may define a consistent interface or contract to programmers using them, abstracting differences between underlying ISA, UISA, and microarchitectures. For example, the C, C++, or Java standards define different programmer-visible macroarchitectures. Microcode: microcode is software that translates instructions to run on a chip. It acts like a wrapper around the hardware, presenting a preferred version of the hardware's instruction set interface. This instruction translation facilty gives chip designers flexible options: Eg 1. A new improved version of the chip can use microcode to present the exact same instruction set as the old chip version, so all software targetting that instruction set will run on the new chip without needing changes. Eg 2. Microcode can present a variety of instruction sets for the same underlying chip, allowing it to run a wider variety of software. UISA: User Instruction Set Architecture, refers to one of three subsets of the RISC CPU instructions provided by PowerPC RISC Processors. The UISA subset, are those RISC instructions of interest to application developers. The other two subsets are VEA (Virtual Environment Architecture) instructions used by virtualisation system developers, and OEA (Operating Environment Architecture) used by Operation System developers.[15] Pin architecture: The hardware functions that a microprocessor should provide to a hardware platform, e.g., the x86 pins A20M, FERR/IGNNE or FLUSH. Also, messages that the processor should emit so that external caches can be invalidated (emptied). Pin architecture functions are more flexible than ISA functions because external hardware can adapt to new encodings, or change from a pin to a message. The term ""architecture"" fits, because the functions must be provided for compatible systems, even if the detailed method changes. Roles Definition Computer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction).[16] However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways. The implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.[17] An instruction set architecture (ISA) is the interface between the computer's software and hardware and also can be viewed as the programmer's view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand. Besides instructions, the ISA defines items in the computer that are available to a program—e.g., data types, registers, addressing modes, and memory. Instructions locate these available items with register indexes (or names) and memory addressing modes. The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler. An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form. Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs. ISAs vary in quality and completeness. A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time). Memory organization defines how instructions interact with the memory, and how memory interacts with itself. During design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals. Computer organization Main article: Microarchitecture Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite detailed analysis of the computer's organization. For example, in a SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way. Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well. For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost. Implementation Once an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps: Logic implementation designs the circuits required at a logic-gate level. Circuit implementation does transistor-level designs of basic elements (e.g., gates, multiplexers, latches) as well as of some larger blocks (ALUs, caches etc.) that may be implemented at the logic-gate level, or even at the physical level if the design calls for it. Physical implementation draws physical circuits. The different circuit components are placed in a chip floorplan or on a board and the wires connecting them are created. Design validation tests the computer as a whole to see if it works in all situations and all timings. Once the design validation process starts, the design at the logic level are tested using logic emulators. However, this is usually too slow to run a realistic test. So, after making corrections based on the first test, prototypes are constructed using Field-Programmable Gate-Arrays (FPGAs). Most hobby projects stop at this stage. The final step is to test prototype integrated circuits, which may require several redesigns. For CPUs, the entire implementation process is organized differently and is often referred to as CPU design. Design goals The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors. The most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance. Performance Modern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach near 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.[citation needed] Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The ""instruction"" in the standard measurements is not a count of the ISA's machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture. Many people used to measure a computer's speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance. Other factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs. There are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time. Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data). Performance is affected by a very wide range of design choices — for example, pipelining a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a real-time environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable and limited time period after the brake pedal is sensed or else failure of the brake will occur. Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks. Power efficiency Main articles: Low-power electronics and Performance per watt Power efficiency is another important measurement in modern computers. A higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt). Modern circuits have less power required per transistor as the number of transistors per chip grows.[18] This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible.[19] In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency. Shifts in market demand Increases in clock frequency have grown more slowly over the past few years, compared to power reduction improvements. This has been driven by the end of Moore's Law and demand for longer battery life and reductions in size for mobile technology. This change in focus from higher clock rates to power consumption and miniaturization can be shown by the significant reductions in power consumption, as much as 50%, that were reported by Intel in their release of the Haswell microarchitecture; where they dropped their power consumption benchmark from 30-40 watts down to 10-20 watts.[20] Comparing this to the processing speed increase of 3 GHz to 4 GHz (2002 to 2006)[21] it can be seen that the focus in research and development are shifting away from clock frequency and moving towards consuming less power and taking up less space. ",,,,
ARC,"Architecture of Computer System Computer is an electronic machine that makes performing any task very easy. In computer, the CPU executes each instruction provided to it, in a series of steps, this series of steps is called Machine Cycle, and is repeated for each instruction. One machine cycle involves fetching of instruction, decoding the instruction, transferring the data, executing the instruction. Computer system has five basic units that help the computer to perform operations, which are given below: Input Unit Output Unit Storage Unit Arithmetic Logic Unit Control Unit Input Unit Input unit connects the external environment with internal computer system. It provides data and instructions to the computer system. Commonly used input devices are keyboard, mouse, magnetic tape etc. Input unit performs following tasks: Accept the data and instructions from the outside environment. Convert it into machine language. Supply the converted data to computer system. Output Unit It connects the internal system of a computer to the external environment. It provides the results of any computation, or instructions to the outside world. Some output devices are printers, monitor etc. Storage Unit This unit holds the data and instructions. It also stores the intermediate results before these are sent to the output devices. It also stores the data for later use. The storage unit of a computer system can be divided into two categories: Primary Storage: This memory is used to store the data which is being currently executed. It is used for temporary storage of data. The data is lost, when the computer is switched off. RAM is used as primary storage memory. Secondary Storage: The secondary memory is slower and cheaper than primary memory. It is used for permanent storage of data. Commonly used secondary memory devices are hard disk, CD etc. Arithmetic Logical Unit All the calculations are performed in ALU of the computer system. The ALU can perform basic operations such as addition, subtraction, division, multiplication etc. Whenever calculations are required, the control unit transfers the data from storage unit to ALU. When the operations are done, the result is transferred back to the storage unit. Control Unit It controls all other units of the computer. It controls the flow of data and instructions to and from the storage unit to ALU. Thus it is also known as central nervous system of the computer. CPU It is Central Processing Unit of the computer. The control unit and ALU are together known as CPU. CPU is the brain of computer system. It performs following tasks: It performs all operations. It takes all decisions. It controls all the units of computer. ",,,,
ARC,"Computer hardware Computer hardware includes the physical parts of a computer, such as the case,[1] central processing unit (CPU), monitor, mouse, keyboard, computer data storage, graphics card, sound card, speakers and motherboard.[2] By contrast, software is the set of instructions that can be stored and run by hardware. Hardware is so-termed because it is ""hard"" or rigid with respect to changes, whereas software is ""soft"" because it is easy to change. Hardware is typically directed by the software to execute any command or instruction. A combination of hardware and software forms a usable computing system, although other systems exist with only hardware. Von Neumann architecture Main articles: Von Neumann architecture and Stored-program computer Von Neumann architecture scheme The template for all modern computers is the Von Neumann architecture, detailed in a 1945 paper by Hungarian mathematician John von Neumann. This describes a design architecture for an electronic digital computer with subdivisions of a processing unit consisting of an arithmetic logic unit and processor registers, a control unit containing an instruction register and program counter, a memory to store both data and instructions, external mass storage, and input and output mechanisms.[3] The meaning of the term has evolved to mean a stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the Von Neumann bottleneck and often limits the performance of the system.[4] Types of computer systems Personal computer Basic hardware components of a modern personal computer, including a monitor, a motherboard, a CPU, a RAM, two expansion cards, a power supply, an optical disc drive, a hard disk drive, a keyboard and a mouse Inside a custom-built computer: power supply at the bottom has its own cooling fan The personal computer is one of the most common types of computer due to its versatility and relatively low price. Desktop personal computers have a monitor, a keyboard, a mouse, and a computer case. The computer case holds the motherboard, fixed or removable disk drives for data storage, the power supply, and may contain other peripheral devices such as modems or network interfaces. Some models of desktop computers integrated the monitor and keyboard into the same case as the processor and power supply. Separating the elements allows the user to arrange the components in a pleasing, comfortable array, at the cost of managing power and data cables between them. Laptops are designed for portability but operate similarly to desktop PCs.[1] They may use lower-power or reduced size components, with lower performance than a similarly priced desktop computer. [5] Laptops contain the keyboard, display, and processor in one case. The monitor in the folding upper cover of the case can be closed for transportation, to protect the screen and keyboard. Instead of a mouse, laptops may have a trackpad or pointing stick. Tablets are portable computer that uses a touch screen as the primary input device. Tablets generally weigh less and are smaller than laptops. Some tablets include fold-out keyboards, or offer connections to separate external keyboards. Some models of laptop computers have a detachable keyboard, which allows the system to be configured as a touch-screen tablet. They are sometimes called ""2-in-1 detachable laptops"" or ""tablet-laptop hybrids"".[6] Case Main article: Computer case The computer case encloses most of the components of the system. It provides mechanical support and protection for internal elements such as the motherboard, disk drives, and power supplies, and controls and directs the flow of cooling air over internal components. The case is also part of the system to control electromagnetic interference radiated by the computer and protects internal parts from electrostatic discharge. Large tower cases provide space for multiple disk drives or other peripherals and usually stand on the floor, while desktop cases provide less expansion room. All-in-one style designs include a video display built into the same case. Portable and laptop computers require cases that provide impact protection for the unit. Hobbyists may decorate the cases with colored lights, paint, or other features, in an activity called case modding. Power supply Main article: Power supply unit A power supply unit (PSU) converts alternating current (AC) electric power to low-voltage direct current (DC) power for the computer. Laptops can run on built-in rechargeable battery.[7] The PSU typically uses a switched-mode power supply (SMPS), with power MOSFETs (power metal–oxide–semiconductor field-effect transistors) used in the converters and regulator circuits of the SMPS.[8] Motherboard Main article: Motherboard The motherboard is the main component of a computer. It is a board with integrated circuitry that connects the other parts of the computer including the CPU, the RAM, the disk drives (CD, DVD, hard disk, or any others) as well as any peripherals connected via the ports or the expansion slots. The integrated circuit (IC) chips in a computer typically contain billions of tiny metal–oxide–semiconductor field-effect transistors (MOSFETs).[9] Components directly attached to or to part of the motherboard include: The CPU (central processing unit), which performs most of the calculations which enable a computer to function, and is referred to as the brain of the computer which get a hold of program instruction from random-access memory (RAM), interprets and processes it and then sends it back to computer result so that the relevant components can carry out the instructions. The CPU is a microprocessor, which is fabricated on a metal–oxide–semiconductor (MOS) integrated circuit (IC) chip. It is usually cooled by a heat sink and fan, or water-cooling system. Most newer CPU includes an on-die graphics processing unit (GPU). The clock speed of CPU governs how fast it executes instructions and is measured in GHz; typical values lie between 1 GHz and 5 GHz. Many modern computers have the option to overclock the CPU which enhances performance at the expense of greater thermal output and thus a need for improved cooling. The chipset, which includes the north bridge, mediates communication between the CPU and the other components of the system, including main memory; as well as south bridge, which is connected to the north bridge, and supports auxiliary interfaces and buses; and, finally, a Super I/O chip, connected through the south bridge, which supports the slowest and most legacy components like serial ports, hardware monitoring and fan control. Random-access memory (RAM), which stores the code and data that are being actively accessed by the CPU. For example, when a web browser is opened on the computer it takes up memory; this is stored in the RAM until the web browser is closed. It is typically a type of dynamic RAM (DRAM), such as synchronous DRAM (SDRAM), where MOS memory chips store data on memory cells consisting of MOSFETs and MOS capacitors. RAM usually comes on dual in-line memory modules (DIMMs) in the sizes of 2GB, 4GB, and 8GB, but can be much larger. Read-only memory (ROM), which stores the BIOS that runs when the computer is powered on or otherwise begins execution, a process known as Bootstrapping, or ""booting"" or ""booting up"". The ROM is typically a nonvolatile BIOS memory chip, which stores data on floating-gate MOSFET memory cells. The BIOS (Basic Input Output System) includes boot firmware and power management firmware. Newer motherboards use Unified Extensible Firmware Interface (UEFI) instead of BIOS. Buses that connect the CPU to various internal components and to expand cards for graphics and sound. The CMOS (complementary MOS) battery, which powers the CMOS memory for date and time in the BIOS chip. This battery is generally a watch battery. The video card (also known as the graphics card), which processes computer graphics. More powerful graphics cards are better suited to handle strenuous tasks, such as playing intensive video games or running computer graphics software. A video card contains a graphics processing unit (GPU) and video memory (typically a type of SDRAM), both fabricated on MOS integrated circuit (MOS IC) chips. Power MOSFETs make up the voltage regulator module (VRM), which controls how much voltage other hardware components receive.[8] Expansion cards Main article: Expansion card An expansion card in computing is a printed circuit board that can be inserted into an expansion slot of a computer motherboard or backplane to add functionality to a computer system via the expansion bus. Expansion cards can be used to obtain or expand on features not offered by the motherboard. Storage devices Main article: Computer data storage A storage device is any computing hardware and digital media that is used for storing, porting and extracting data files and objects. It can hold and store information both temporarily and permanently and can be internal or external to a computer, server or any similar computing device. Data storage is a core function and fundamental component of computers. Fixed media Data is stored by a computer using a variety of media. Hard disk drives (HDDs) are found in virtually all older computers, due to their high capacity and low cost, but solid-state drives (SSDs) are faster and more power efficient, although currently more expensive than hard drives in terms of dollar per gigabyte,[10] so are often found in personal computers built post-2007.[11] SSDs use flash memory, which stores data on MOS memory chips consisting of floating-gate MOSFET memory cells. Some systems may use a disk array controller for greater performance or reliability. Removable media To transfer data between computers, an external flash memory device (such as a memory card or USB flash drive) or optical disc (such as a CD-ROM, DVD-ROM or BD-ROM) may be used. Their usefulness depends on being readable by other systems; the majority of machines have an optical disk drive (ODD), and virtually all have at least one Universal Serial Bus (USB) port. Input and output peripherals Main article: Peripheral Input and output devices are typically housed externally to the main computer chassis. The following are either standard or very common to many computer systems. Input device Input devices allow the user to enter information into the system, or control its operation. Most personal computers have a mouse and keyboard, but laptop systems typically use a touchpad instead of a mouse. Other input devices include webcams, microphones, joysticks, and image scanners. Output device Output devices are designed around the senses of human beings. For example, monitors display text that can be read, speakers produce sound that can be heard.[12] Such devices also could include printers or a Braille embosser. Mainframe computer Main article: Mainframe Computer A mainframe computer is a much larger computer that typically fills a room and may cost many hundreds or thousands of times as much as a personal computer. They are designed to perform large numbers of calculations for governments and large enterprises. An IBM System z9 mainframe Departmental computing Main article: Minicomputer In the 1960s and 1970s, more and more departments started to use cheaper and dedicated systems for specific purposes like process control and laboratory automation. A minicomputer, or colloquially mini, is a class of smaller computers that was developed in the mid-1960s[13][14] and sold for much less than mainframe[15] and mid-size computers from IBM and its direct competitors. Supercomputer Main article: Supercomputer A supercomputer is superficially similar to a mainframe but is instead intended for extremely demanding computational tasks. As of June 2018, the fastest supercomputer on the TOP500 supercomputer list is the Summit, in the United States, with a LINPACK benchmark score of 122.3 PFLOPS Light, by around 29 PFLOPS. The term supercomputer does not refer to a specific technology. Rather it indicates the fastest computations available at any given time. In mid-2011, the fastest supercomputers boasted speeds exceeding one petaflop, or 1 quadrillion (10^15 or 1,000 trillion) floating-point operations per second. Supercomputers are fast but extremely costly, so they are generally used by large organizations to execute computationally demanding tasks involving large data sets. Supercomputers typically run military and scientific applications. Although costly, they are also being used for commercial applications where huge amounts of data must be analyzed. For example, large banks employ supercomputers to calculate the risks and returns of various investment strategies, and healthcare organizations use them to analyze giant databases of patient data to determine optimal treatments for various diseases and problems incurring to the country. ",,,,
ARC,"Digital electronics Digital electronics is a field of electronics involving the study of digital signals and the engineering of devices that use or produce them. This is in contrast to analog electronics and analog signals. Digital electronic circuits are usually made from large assemblies of logic gates, often packaged in integrated circuits. Complex devices may have simple electronic representations of Boolean logic functions.[1] Properties An advantage of digital circuits when compared to analog circuits is that signals represented digitally can be transmitted without degradation caused by noise.[39] For example, a continuous audio signal transmitted as a sequence of 1s and 0s, can be reconstructed without error, provided the noise picked up in transmission is not enough to prevent identification of the 1s and 0s. In a digital system, a more precise representation of a signal can be obtained by using more binary digits to represent it. While this requires more digital circuits to process the signals, each digit is handled by the same kind of hardware, resulting in an easily scalable system. In an analog system, additional resolution requires fundamental improvements in the linearity and noise characteristics of each step of the signal chain. With computer-controlled digital systems, new functions to be added through software revision and no hardware changes. Often this can be done outside of the factory by updating the product's software. So, the product's design errors can be corrected after the product is in a customer's hands. Information storage can be easier in digital systems than in analog ones. The noise immunity of digital systems permits data to be stored and retrieved without degradation. In an analog system, noise from aging and wear degrade the information stored. In a digital system, as long as the total noise is below a certain level, the information can be recovered perfectly. Even when more significant noise is present, the use of redundancy permits the recovery of the original data provided too many errors do not occur. In some cases, digital circuits use more energy than analog circuits to accomplish the same tasks, thus producing more heat which increases the complexity of the circuits such as the inclusion of heat sinks. In portable or battery-powered systems this can limit use of digital systems. For example, battery-powered cellular telephones often use a low-power analog front-end to amplify and tune in the radio signals from the base station. However, a base station has grid power and can use power-hungry, but very flexible software radios. Such base stations can be easily reprogrammed to process the signals used in new cellular standards. Many useful digital systems must translate from continuous analog signals to discrete digital signals. This causes quantization errors. Quantization error can be reduced if the system stores enough digital data to represent the signal to the desired degree of fidelity. The Nyquist–Shannon sampling theorem provides an important guideline as to how much digital data is needed to accurately portray a given analog signal. In some systems, if a single piece of digital data is lost or misinterpreted, the meaning of large blocks of related data can completely change. For example, a single-bit error in audio data stored directly as linear pulse code modulation causes, at worst, a single click. Instead, many people use audio compression to save storage space and download time, even though a single bit error may cause a larger disruption. Because of the cliff effect, it can be difficult for users to tell if a particular system is right on the edge of failure, or if it can tolerate much more noise before failing. Digital fragility can be reduced by designing a digital system for robustness. For example, a parity bit or other error management method can be inserted into the signal path. These schemes help the system detect errors, and then either correct the errors, or request retransmission of the data. Construction A binary clock, hand-wired on breadboards A digital circuit is typically constructed from small electronic circuits called logic gates that can be used to create combinational logic. Each logic gate is designed to perform a function of boolean logic when acting on logic signals. A logic gate is generally created from one or more electrically controlled switches, usually transistors but thermionic valves have seen historic use. The output of a logic gate can, in turn, control or feed into more logic gates. Another form of digital circuit is constructed from lookup tables, (many sold as ""programmable logic devices"", though other kinds of PLDs exist). Lookup tables can perform the same functions as machines based on logic gates, but can be easily reprogrammed without changing the wiring. This means that a designer can often repair design errors without changing the arrangement of wires. Therefore, in small volume products, programmable logic devices are often the preferred solution. They are usually designed by engineers using electronic design automation software. Integrated circuits consist of multiple transistors on one silicon chip, and are the least expensive way to make large number of interconnected logic gates. Integrated circuits are usually interconnected on a printed circuit board which is a board which holds electrical components, and connects them together with copper traces. Design Engineers use many methods to minimize logic redundancy in order to reduce the circuit complexity. Reduced complexity reduces component count and potential errors and therefore typically reduces cost. Logic redundancy can be removed by several well-known techniques, such as binary decision diagrams, Boolean algebra, Karnaugh maps, the Quine–McCluskey algorithm, and the heuristic computer method. These operations are typically performed within a computer-aided design system. Embedded systems with microcontrollers and programmable logic controllers are often used to implement digital logic for complex systems that don't require optimal performance. These systems are usually programmed by software engineers or by electricians, using ladder logic. Representation Representations are crucial to an engineer's design of digital circuits. To choose representations, engineers consider types of digital systems. The classical way to represent a digital circuit is with an equivalent set of logic gates. Each logic symbol is represented by a different shape. The actual set of shapes was introduced in 1984 under IEEE/ANSI standard 91-1984 and is now in common use by integrated circuit manufacturers.[40] Another way is to construct an equivalent system of electronic switches (usually transistors). This can be represented as a truth table. Most digital systems divide into combinational and sequential systems. A combinational system always presents the same output when given the same inputs. A sequential system is a combinational system with some of the outputs fed back as inputs. This makes the digital machine perform a sequence of operations. The simplest sequential system is probably a flip flop, a mechanism that represents a binary digit or ""bit"". Sequential systems are often designed as state machines. In this way, engineers can design a system's gross behavior, and even test it in a simulation, without considering all the details of the logic functions. Sequential systems divide into two further subcategories. ""Synchronous"" sequential systems change state all at once when a clock signal changes state. ""Asynchronous"" sequential systems propagate changes whenever inputs change. Synchronous sequential systems are made of well-characterized asynchronous circuits such as flip-flops, that change only when the clock changes, and which have carefully designed timing margins. For logic simulation, digital circuit representations have digital file formats that can be processed by computer programs. Synchronous systems A 4-bit ring counter using D-type flip flops is an example of synchronous logic. Each device is connected to the clock signal, and update together. Main article: synchronous logic The usual way to implement a synchronous sequential state machine is to divide it into a piece of combinational logic and a set of flip flops called a state register. The state register represents the state as a binary number. The combinational logic produces the binary representation for the next state. On each clock cycle, the state register captures the feedback generated from the previous state of the combinational logic and feeds it back as an unchanging input to the combinational part of the state machine. The clock rate is limited by the most time-consuming logic calculation in the combinational logic. Asynchronous systems Most digital logic is synchronous because it is easier to create and verify a synchronous design. However, asynchronous logic has the advantage of its speed not being constrained by an arbitrary clock; instead, it runs at the maximum speed of its logic gates.[a] Building an asynchronous system using faster parts makes the circuit faster. Nevertheless, most systems need to accept external unsynchronized signals into their synchronous logic circuits. This interface is inherently asynchronous and must be analyzed as such. Examples of widely used asynchronous circuits include synchronizer flip-flops, switch debouncers and arbiters. Asynchronous logic components can be hard to design because all possible states, in all possible timings must be considered. The usual method is to construct a table of the minimum and maximum time that each such state can exist and then adjust the circuit to minimize the number of such states. The designer must force the circuit to periodically wait for all of its parts to enter a compatible state (this is called ""self-resynchronization""). Without careful design, it is easy to accidentally produce asynchronous logic that is unstable, that is, real electronics will have unpredictable results because of the cumulative delays caused by small variations in the values of the electronic components. Register transfer systems Example of a simple circuit with a toggling output. The inverter forms the combinational logic in this circuit, and the register holds the state. Many digital systems are data flow machines. These are usually designed using synchronous register transfer logic, using hardware description languages such as VHDL or Verilog. In register transfer logic, binary numbers are stored in groups of flip flops called registers. The outputs of each register are a bundle of wires called a ""bus"" that carries that number to other calculations. A calculation is simply a piece of combinational logic. Each calculation also has an output bus, and these may be connected to the inputs of several registers. Sometimes a register will have a multiplexer on its input, so that it can store a number from any one of several buses. Alternatively, the outputs of several items may be connected to a bus through buffers that can turn off the output of all of the devices except one. A sequential state machine controls when each register accepts new data from its input. Asynchronous register-transfer systems (such as computers) have a general solution. In the 1980s, some researchers discovered that almost all synchronous register-transfer machines could be converted to asynchronous designs by using first-in-first-out synchronization logic. In this scheme, the digital machine is characterized as a set of data flows. In each step of the flow, an asynchronous ""synchronization circuit"" determines when the outputs of that step are valid, and presents a signal that says, ""grab the data"" to the stages that use that stage's inputs. It turns out that just a few relatively simple synchronization circuits are needed. Computer design Intel 80486DX2 microprocessor The most general-purpose register-transfer logic machine is a computer. This is basically an automatic binary abacus. The control unit of a computer is usually designed as a microprogram run by a microsequencer. A microprogram is much like a player-piano roll. Each table entry or ""word"" of the microprogram commands the state of every bit that controls the computer. The sequencer then counts, and the count addresses the memory or combinational logic machine that contains the microprogram. The bits from the microprogram control the arithmetic logic unit, memory and other parts of the computer, including the microsequencer itself. A ""specialized computer"" is usually a conventional computer with special-purpose control logic or microprogram. In this way, the complex task of designing the controls of a computer is reduced to a simpler task of programming a collection of much simpler logic machines. Almost all computers are synchronous. However, numerous true asynchronous computers have also been built. One example is the Aspida DLX core.[42] Another was offered by ARM Holdings. Speed advantages have not materialized, because modern computer designs already run at the speed of their slowest component, usually memory. These do use somewhat less power because a clock distribution network is not needed. An unexpected advantage is that asynchronous computers do not produce spectrally-pure radio noise, so they are used in some mobile-phone base-station controllers. They may be more secure in cryptographic applications because their electrical and radio emissions can be more difficult to decode.[43] Computer architecture Computer architecture is a specialized engineering activity that tries to arrange the registers, calculation logic, buses and other parts of the computer in the best way for some purpose. Computer architects have applied large amounts of ingenuity to computer design to reduce the cost and increase the speed and immunity to programming errors of computers. An increasingly common goal is to reduce the power used in a battery-powered computer system, such as a cell-phone. Many computer architects serve an extended apprenticeship as microprogrammers. Design issues in digital circuits This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2015) (Learn how and when to remove this template message) Digital circuits are made from analog components. The design must assure that the analog nature of the components doesn't dominate the desired digital behavior. Digital systems must manage noise and timing margins, parasitic inductances and capacitances, and filter power connections. Bad designs have intermittent problems such as ""glitches"", vanishingly fast pulses that may trigger some logic but not others, ""runt pulses"" that do not reach valid ""threshold"" voltages, or unexpected (""undecoded"") combinations of logic states. Additionally, where clocked digital systems interface to analog systems or systems that are driven from a different clock, the digital system can be subject to metastability where a change to the input violates the set-up time for a digital input latch. This situation will self-resolve, but will take a random time, and while it persists can result in invalid signals being propagated within the digital system for a short time. Since digital circuits are made from analog components, digital circuits calculate more slowly than low-precision analog circuits that use a similar amount of space and power. However, the digital circuit will calculate more repeatably, because of its high noise immunity. On the other hand, in the high-precision domain (for example, where 14 or more bits of precision are needed), analog circuits require much more power and area than digital equivalents. Automated design tools To save costly engineering effort, much of the effort of designing large logic machines has been automated. The computer programs are called ""electronic design automation tools"" or just ""EDA."" Simple truth table-style descriptions of logic are often optimized with EDA that automatically produces reduced systems of logic gates or smaller lookup tables that still produce the desired outputs. The most common example of this kind of software is the Espresso heuristic logic minimizer. Most practical algorithms for optimizing large logic systems use algebraic manipulations or binary decision diagrams, and there are promising experiments with genetic algorithms and annealing optimizations. To automate costly engineering processes, some EDA can take state tables that describe state machines and automatically produce a truth table or a function table for the combinational logic of a state machine. The state table is a piece of text that lists each state, together with the conditions controlling the transitions between them and the belonging output signals. It is common for the function tables of such computer-generated state-machines to be optimized with logic-minimization software such as Minilog. Often, real logic systems are designed as a series of sub-projects, which are combined using a ""tool flow."" The tool flow is usually a ""script,"" a simplified computer language that can invoke the software design tools in the right order. Tool flows for large logic systems such as microprocessors can be thousands of commands long, and combine the work of hundreds of engineers. Writing and debugging tool flows is an established engineering specialty in companies that produce digital designs. The tool flow usually terminates in a detailed computer file or set of files that describe how to physically construct the logic. Often it consists of instructions to draw the transistors and wires on an integrated circuit or a printed circuit board. Parts of tool flows are ""debugged"" by verifying the outputs of simulated logic against expected inputs. The test tools take computer files with sets of inputs and outputs, and highlight discrepancies between the simulated behavior and the expected behavior. Once the input data is believed correct, the design itself must still be verified for correctness. Some tool flows verify designs by first producing a design, and then scanning the design to produce compatible input data for the tool flow. If the scanned data matches the input data, then the tool flow has probably not introduced errors. The functional verification data are usually called ""test vectors"". The functional test vectors may be preserved and used in the factory to test that newly constructed logic works correctly. However, functional test patterns don't discover common fabrication faults. Production tests are often designed by software tools called ""test pattern generators"". These generate test vectors by examining the structure of the logic and systematically generating tests for particular faults. This way the fault coverage can closely approach 100%, provided the design is properly made testable (see next section). Once a design exists, and is verified and testable, it often needs to be processed to be manufacturable as well. Modern integrated circuits have features smaller than the wavelength of the light used to expose the photoresist. Manufacturability software adds interference patterns to the exposure masks to eliminate open-circuits, and enhance the masks' contrast. Design for testability There are several reasons for testing a logic circuit. When the circuit is first developed, it is necessary to verify that the design circuit meets the required functional and timing specifications. When multiple copies of a correctly designed circuit are being manufactured, it is essential to test each copy to ensure that the manufacturing process has not introduced any flaws.[44] A large logic machine (say, with more than a hundred logical variables) can have an astronomical number of possible states. Obviously, in the factory, testing every state is impractical if testing each state takes a microsecond, and there are more states than the number of microseconds since the universe began. This ridiculous-sounding case is typical. Large logic machines are almost always designed as assemblies of smaller logic machines. To save time, the smaller sub-machines are isolated by permanently installed ""design for test"" circuitry, and are tested independently. One common test scheme known as ""scan design"" moves test bits serially (one after another) from external test equipment through one or more serial shift registers known as ""scan chains"". Serial scans have only one or two wires to carry the data, and minimize the physical size and expense of the infrequently used test logic. After all the test data bits are in place, the design is reconfigured to be in ""normal mode"" and one or more clock pulses are applied, to test for faults (e.g. stuck-at low or stuck-at high) and capture the test result into flip-flops and/or latches in the scan shift register(s). Finally, the result of the test is shifted out to the block boundary and compared against the predicted ""good machine"" result. In a board-test environment, serial to parallel testing has been formalized with a standard called ""JTAG"" (named after the ""Joint Test Action Group"" that made it). Another common testing scheme provides a test mode that forces some part of the logic machine to enter a ""test cycle."" The test cycle usually exercises large independent parts of the machine. Trade-offs Several numbers determine the practicality of a system of digital logic: cost, reliability, fanout and speed. Engineers explored numerous electronic devices to get a favourable combination of these personalities. Cost The cost of a logic gate is crucial, primarily because very many gates are needed to build a computer or other advanced digital system and because the more gates can be used, the more able and/or respondent the machine can become. Since the bulk of a digital computer is simply an interconnected network of logic gates, the overall cost of building a computer correlates strongly with the price per logic gate. In the 1930s, the earliest digital logic systems were constructed from telephone relays because these were inexpensive and relatively reliable. After that, electrical engineers always used the cheapest available electronic switches that could still fulfill the requirements. The earliest integrated circuits were a happy accident. They were constructed not to save money, but to save weight, and permit the Apollo Guidance Computer to control an inertial guidance system for a spacecraft. The first integrated circuit logic gates cost nearly $50 (in 1960 dollars, when an engineer earned $10,000/year). Much to the surprise of many involved, by the time the circuits were mass-produced, they had become the least-expensive method of constructing digital logic. Improvements in this technology have driven all subsequent improvements in cost. With the rise of integrated circuits, reducing the absolute number of chips used represented another way to save costs. The goal of a designer is not just to make the simplest circuit, but to keep the component count down. Sometimes this results in more complicated designs with respect to the underlying digital logic but nevertheless reduces the number of components, board size, and even power consumption. A major motive for reducing component count on printed circuit boards is to reduce the manufacturing defect rate and increase reliability, as every soldered connection is a potentially bad one, so the defect and failure rates tend to increase along with the total number of component pins. For example, in some logic families, NAND gates are the simplest digital gate to build. All other logical operations can be implemented by NAND gates. If a circuit already required a single NAND gate, and a single chip normally carried four NAND gates, then the remaining gates could be used to implement other logical operations like logical and. This could eliminate the need for a separate chip containing those different types of gates. Reliability The ""reliability"" of a logic gate describes its mean time between failure (MTBF). Digital machines often have millions of logic gates. Also, most digital machines are ""optimized"" to reduce their cost. The result is that often, the failure of a single logic gate will cause a digital machine to stop working. It is possible to design machines to be more reliable by using redundant logic which will not malfunction as a result of the failure of any single gate (or even any two, three, or four gates), but this necessarily entails using more components, which raises the financial cost and also usually increases the weight of the machine and may increase the power it consumes. Digital machines first became useful when the MTBF for a switch got above a few hundred hours. Even so, many of these machines had complex, well-rehearsed repair procedures, and would be nonfunctional for hours because a tube burned-out, or a moth got stuck in a relay. Modern transistorized integrated circuit logic gates have MTBFs greater than 82 billion hours (8.2 · 1010 hours),[45] and need them because they have so many logic gates. Fanout Fanout describes how many logic inputs can be controlled by a single logic output without exceeding the electrical current ratings of the gate outputs.[46] The minimum practical fanout is about five. Modern electronic logic gates using CMOS transistors for switches have fanouts near fifty, and can sometimes go much higher. Speed The ""switching speed"" describes how many times per second an inverter (an electronic representation of a ""logical not"" function) can change from true to false and back. Faster logic can accomplish more operations in less time. Digital logic first became useful when switching speeds got above 50 Hz, because that was faster than a team of humans operating mechanical calculators. Modern electronic digital logic routinely switches at 5 GHz (5 · 109 Hz), and some laboratory systems switch at more than 1 THz (1 · 1012 Hz)[citation needed]. Logic families ",,,,
ARC,"Microarchitecture In computer engineering, microarchitecture, also called computer organization and sometimes abbreviated as µarch or uarch, is the way a given instruction set architecture (ISA) is implemented in a particular processor.[1] A given ISA may be implemented with different microarchitectures;[2][3] implementations may vary due to different goals of a given design or due to shifts in technology.[4] Computer architecture is the combination of microarchitecture and instruction set architecture. Relation to instruction set architecture A microarchitecture organized around a single bus The ISA is roughly the same as the programming model of a processor as seen by an assembly language programmer or compiler writer. The ISA includes the execution model, processor registers, address and data formats among other things. The microarchitecture includes the constituent parts of the processor and how these interconnect and interoperate to implement the ISA. The microarchitecture of a machine is usually represented as (more or less detailed) diagrams that describe the interconnections of the various microarchitectural elements of the machine, which may be anything from single gates and registers, to complete arithmetic logic units (ALUs) and even larger elements. These diagrams generally separate the datapath (where data is placed) and the control path (which can be said to steer the data).[5] The person designing a system usually draws the specific microarchitecture as a kind of data flow diagram. Like a block diagram, the microarchitecture diagram shows microarchitectural elements such as the arithmetic and logic unit and the register file as a single schematic symbol. Typically, the diagram connects those elements with arrows, thick lines and thin lines to distinguish between three-state buses (which require a three-state buffer for each device that drives the bus), unidirectional buses (always driven by a single source, such as the way the address bus on simpler computers is always driven by the memory address register), and individual control lines. Very simple computers have a single data bus organization – they have a single three-state bus. The diagram of more complex computers usually shows multiple three-state buses, which help the machine do more operations simultaneously. Each microarchitectural element is in turn represented by a schematic describing the interconnections of logic gates used to implement it. Each logic gate is in turn represented by a circuit diagram describing the connections of the transistors used to implement it in some particular logic family. Machines with different microarchitectures may have the same instruction set architecture, and thus be capable of executing the same programs. New microarchitectures and/or circuitry solutions, along with advances in semiconductor manufacturing, are what allows newer generations of processors to achieve higher performance while using the same ISA. In principle, a single microarchitecture could execute several different ISAs with only minor changes to the microcode. Aspects Intel 80286 microarchitecture The pipelined datapath is the most commonly used datapath design in microarchitecture today. This technique is used in most modern microprocessors, microcontrollers, and DSPs. The pipelined architecture allows multiple instructions to overlap in execution, much like an assembly line. The pipeline includes several different stages which are fundamental in microarchitecture designs.[5] Some of these stages include instruction fetch, instruction decode, execute, and write back. Some architectures include other stages such as memory access. The design of pipelines is one of the central microarchitectural tasks. Execution units are also essential to microarchitecture. Execution units include arithmetic logic units (ALU), floating point units (FPU), load/store units, branch prediction, and SIMD. These units perform the operations or calculations of the processor. The choice of the number of execution units, their latency and throughput is a central microarchitectural design task. The size, latency, throughput and connectivity of memories within the system are also microarchitectural decisions. System-level design decisions such as whether or not to include peripherals, such as memory controllers, can be considered part of the microarchitectural design process. This includes decisions on the performance-level and connectivity of these peripherals. Unlike architectural design, where achieving a specific performance level is the main goal, microarchitectural design pays closer attention to other constraints. Since microarchitecture design decisions directly affect what goes into a system, attention must be paid to issues such as chip area/cost, power consumption, logic complexity, ease of connectivity, manufacturability, ease of debugging, and testability. Microarchitectural concepts Instruction cycles Main article: instruction cycle To run programs, all single- or multi-chip CPUs: Read an instruction and decode it Find any associated data that is needed to process the instruction Process the instruction Write the results out The instruction cycle is repeated continuously until the power is turned off. Multicycle microarchitecture Historically, the earliest computers were multicycle designs. The smallest, least-expensive computers often still use this technique. Multicycle architectures often use the least total number of logic elements and reasonable amounts of power. They can be designed to have deterministic timing and high reliability. In particular, they have no pipeline to stall when taking conditional branches or interrupts. However, other microarchitectures often perform more instructions per unit time, using the same logic family. When discussing ""improved performance,"" an improvement is often relative to a multicycle design. In a multicycle computer, the computer does the four steps in sequence, over several cycles of the clock. Some designs can perform the sequence in two clock cycles by completing successive stages on alternate clock edges, possibly with longer operations occurring outside the main cycle. For example, stage one on the rising edge of the first cycle, stage two on the falling edge of the first cycle, etc. In the control logic, the combination of cycle counter, cycle state (high or low) and the bits of the instruction decode register determine exactly what each part of the computer should be doing. To design the control logic, one can create a table of bits describing the control signals to each part of the computer in each cycle of each instruction. Then, this logic table can be tested in a software simulation running test code. If the logic table is placed in a memory and used to actually run a real computer, it is called a microprogram. In some computer designs, the logic table is optimized into the form of combinational logic made from logic gates, usually using a computer program that optimizes logic. Early computers used ad-hoc logic design for control until Maurice Wilkes invented this tabular approach and called it microprogramming.[6] Increasing execution speed Complicating this simple-looking series of steps is the fact that the memory hierarchy, which includes caching, main memory and non-volatile storage like hard disks (where the program instructions and data reside), has always been slower than the processor itself. Step (2) often introduces a lengthy (in CPU terms) delay while the data arrives over the computer bus. A considerable amount of research has been put into designs that avoid these delays as much as possible. Over the years, a central goal was to execute more instructions in parallel, thus increasing the effective execution speed of a program. These efforts introduced complicated logic and circuit structures. Initially, these techniques could only be implemented on expensive mainframes or supercomputers due to the amount of circuitry needed for these techniques. As semiconductor manufacturing progressed, more and more of these techniques could be implemented on a single semiconductor chip. See Moore's law. Instruction set choice Instruction sets have shifted over the years, from originally very simple to sometimes very complex (in various respects). In recent years, load-store architectures, VLIW and EPIC types have been in fashion. Architectures that are dealing with data parallelism include SIMD and Vectors. Some labels used to denote classes of CPU architectures are not particularly descriptive, especially so the CISC label; many early designs retroactively denoted ""CISC"" are in fact significantly simpler than modern RISC processors (in several respects). However, the choice of instruction set architecture may greatly affect the complexity of implementing high-performance devices. The prominent strategy, used to develop the first RISC processors, was to simplify instructions to a minimum of individual semantic complexity combined with high encoding regularity and simplicity. Such uniform instructions were easily fetched, decoded and executed in a pipelined fashion and a simple strategy to reduce the number of logic levels in order to reach high operating frequencies; instruction cache-memories compensated for the higher operating frequency and inherently low code density while large register sets were used to factor out as much of the (slow) memory accesses as possible. Instruction pipelining Main article: Instruction pipelining One of the first, and most powerful, techniques to improve performance is the use of instruction pipelining. Early processor designs would carry out all of the steps above for one instruction before moving onto the next. Large portions of the circuitry were left idle at any one step; for instance, the instruction decoding circuitry would be idle during execution and so on. Pipelining improves performance by allowing a number of instructions to work their way through the processor at the same time. In the same basic example, the processor would start to decode (step 1) a new instruction while the last one was waiting for results. This would allow up to four instructions to be ""in flight"" at one time, making the processor look four times as fast. Although any one instruction takes just as long to complete (there are still four steps) the CPU as a whole ""retires"" instructions much faster. RISC makes pipelines smaller and much easier to construct by cleanly separating each stage of the instruction process and making them take the same amount of time—one cycle. The processor as a whole operates in an assembly line fashion, with instructions coming in one side and results out the other. Due to the reduced complexity of the classic RISC pipeline, the pipelined core and an instruction cache could be placed on the same size die that would otherwise fit the core alone on a CISC design. This was the real reason that RISC was faster. Early designs like the SPARC and MIPS often ran over 10 times as fast as Intel and Motorola CISC solutions at the same clock speed and price. Pipelines are by no means limited to RISC designs. By 1986 the top-of-the-line VAX implementation (VAX 8800) was a heavily pipelined design, slightly predating the first commercial MIPS and SPARC designs. Most modern CPUs (even embedded CPUs) are now pipelined, and microcoded CPUs with no pipelining are seen only in the most area-constrained embedded processors.[examples needed] Large CISC machines, from the VAX 8800 to the modern Pentium 4 and Athlon, are implemented with both microcode and pipelines. Improvements in pipelining and caching are the two major microarchitectural advances that have enabled processor performance to keep pace with the circuit technology on which they are based. Cache Main article: CPU cache It was not long before improvements in chip manufacturing allowed for even more circuitry to be placed on the die, and designers started looking for ways to use it. One of the most common was to add an ever-increasing amount of cache memory on-die. Cache is simply very fast memory. It can be accessed in a few cycles as opposed to many needed to ""talk"" to main memory. The CPU includes a cache controller which automates reading and writing from the cache. If the data is already in the cache it simply ""appears"", whereas if it is not the processor is ""stalled"" while the cache controller reads it in. RISC designs started adding cache in the mid-to-late 1980s, often only 4 KB in total. This number grew over time, and typical CPUs now have at least 512 KB, while more powerful CPUs come with 1 or 2 or even 4, 6, 8 or 12 MB, organized in multiple levels of a memory hierarchy. Generally speaking, more cache means more performance, due to reduced stalling. Caches and pipelines were a perfect match for each other. Previously, it didn't make much sense to build a pipeline that could run faster than the access latency of off-chip memory. Using on-chip cache memory instead, meant that a pipeline could run at the speed of the cache access latency, a much smaller length of time. This allowed the operating frequencies of processors to increase at a much faster rate than that of off-chip memory. Branch prediction Main article: Branch predictor One barrier to achieving higher performance through instruction-level parallelism stems from pipeline stalls and flushes due to branches. Normally, whether a conditional branch will be taken isn't known until late in the pipeline as conditional branches depend on results coming from a register. From the time that the processor's instruction decoder has figured out that it has encountered a conditional branch instruction to the time that the deciding register value can be read out, the pipeline needs to be stalled for several cycles, or if it's not and the branch is taken, the pipeline needs to be flushed. As clock speeds increase the depth of the pipeline increases with it, and some modern processors may have 20 stages or more. On average, every fifth instruction executed is a branch, so without any intervention, that's a high amount of stalling. Techniques such as branch prediction and speculative execution are used to lessen these branch penalties. Branch prediction is where the hardware makes educated guesses on whether a particular branch will be taken. In reality one side or the other of the branch will be called much more often than the other. Modern designs have rather complex statistical prediction systems, which watch the results of past branches to predict the future with greater accuracy. The guess allows the hardware to prefetch instructions without waiting for the register read. Speculative execution is a further enhancement in which the code along the predicted path is not just prefetched but also executed before it is known whether the branch should be taken or not. This can yield better performance when the guess is good, with the risk of a huge penalty when the guess is bad because instructions need to be undone. Superscalar Main article: Superscalar Even with all of the added complexity and gates needed to support the concepts outlined above, improvements in semiconductor manufacturing soon allowed even more logic gates to be used. In the outline above the processor processes parts of a single instruction at a time. Computer programs could be executed faster if multiple instructions were processed simultaneously. This is what superscalar processors achieve, by replicating functional units such as ALUs. The replication of functional units was only made possible when the die area of a single-issue processor no longer stretched the limits of what could be reliably manufactured. By the late 1980s, superscalar designs started to enter the market place. In modern designs it is common to find two load units, one store (many instructions have no results to store), two or more integer math units, two or more floating point units, and often a SIMD unit of some sort. The instruction issue logic grows in complexity by reading in a huge list of instructions from memory and handing them off to the different execution units that are idle at that point. The results are then collected and re-ordered at the end. Out-of-order execution Main article: Out-of-order execution The addition of caches reduces the frequency or duration of stalls due to waiting for data to be fetched from the memory hierarchy, but does not get rid of these stalls entirely. In early designs a cache miss would force the cache controller to stall the processor and wait. Of course there may be some other instruction in the program whose data is available in the cache at that point. Out-of-order execution allows that ready instruction to be processed while an older instruction waits on the cache, then re-orders the results to make it appear that everything happened in the programmed order. This technique is also used to avoid other operand dependency stalls, such as an instruction awaiting a result from a long latency floating-point operation or other multi-cycle operations. Register renaming Main article: Register renaming Register renaming refers to a technique used to avoid unnecessary serialized execution of program instructions because of the reuse of the same registers by those instructions. Suppose we have two groups of instruction that will use the same register. One set of instructions is executed first to leave the register to the other set, but if the other set is assigned to a different similar register, both sets of instructions can be executed in parallel (or) in series. Multiprocessing and multithreading Main articles: Multiprocessing and Multithreading (computer architecture) Computer architects have become stymied by the growing mismatch in CPU operating frequencies and DRAM access times. None of the techniques that exploited instruction-level parallelism (ILP) within one program could make up for the long stalls that occurred when data had to be fetched from main memory. Additionally, the large transistor counts and high operating frequencies needed for the more advanced ILP techniques required power dissipation levels that could no longer be cheaply cooled. For these reasons, newer generations of computers have started to exploit higher levels of parallelism that exist outside of a single program or program thread. This trend is sometimes known as throughput computing. This idea originated in the mainframe market where online transaction processing emphasized not just the execution speed of one transaction, but the capacity to deal with massive numbers of transactions. With transaction-based applications such as network routing and web-site serving greatly increasing in the last decade, the computer industry has re-emphasized capacity and throughput issues. One technique of how this parallelism is achieved is through multiprocessing systems, computer systems with multiple CPUs. Once reserved for high-end mainframes and supercomputers, small-scale (2–8) multiprocessors servers have become commonplace for the small business market. For large corporations, large scale (16–256) multiprocessors are common. Even personal computers with multiple CPUs have appeared since the 1990s. With further transistor size reductions made available with semiconductor technology advances, multi-core CPUs have appeared where multiple CPUs are implemented on the same silicon chip. Initially used in chips targeting embedded markets, where simpler and smaller CPUs would allow multiple instantiations to fit on one piece of silicon. By 2005, semiconductor technology allowed dual high-end desktop CPUs CMP chips to be manufactured in volume. Some designs, such as Sun Microsystems' UltraSPARC T1 have reverted to simpler (scalar, in-order) designs in order to fit more processors on one piece of silicon. Another technique that has become more popular recently is multithreading. In multithreading, when the processor has to fetch data from slow system memory, instead of stalling for the data to arrive, the processor switches to another program or program thread which is ready to execute. Though this does not speed up a particular program/thread, it increases the overall system throughput by reducing the time the CPU is idle. Conceptually, multithreading is equivalent to a context switch at the operating system level. The difference is that a multithreaded CPU can do a thread switch in one CPU cycle instead of the hundreds or thousands of CPU cycles a context switch normally requires. This is achieved by replicating the state hardware (such as the register file and program counter) for each active thread. A further enhancement is simultaneous multithreading. This technique allows superscalar CPUs to execute instructions from different programs/threads simultaneously in the same cycle. ",,,,
ARC,"Multiprocessing Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system.[1][2] The term also refers to the ability of a system to support more than one processor or the ability to allocate tasks between them. There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.). According to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs.[3][4] A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share ""some or all of the system’s memory and I/O facilities""; it also gave tightly coupled system as a synonymous term.[5] At the operating system level, multiprocessing is sometimes used to refer to the execution of multiple concurrent processes in a system, with each process running on a separate CPU or core, as opposed to a single process at any one instant.[6][7] When used with this definition, multiprocessing is sometimes contrasted with multitasking, which may use just a single processor but switch it in time slices between tasks (i.e. a time-sharing system). Multiprocessing however means true parallel execution of multiple processes using more than one processor.[7] Multiprocessing doesn't necessarily mean that a single process or task uses more than one processor simultaneously; the term parallel processing is generally used to denote that scenario.[6] Other authors prefer to refer to the operating system techniques as multiprogramming and reserve the term multiprocessing for the hardware aspect of having more than one processor.[2][8] The remainder of this article discusses multiprocessing only in this hardware sense. In Flynn's taxonomy, multiprocessors as defined above are MIMD machines.[9][10] As the term ""multiprocessor"" normally refers to tightly coupled systems in which all processors share memory, multiprocessors are not the entire class of MIMD machines, which also contains message passing multicomputer systems.[9] Pre-history Possibly the first expression of the idea of multiprocessing was written by Luigi Federico Menabrea in 1842, about Charles Babbage's analytical engine (as translated by Ada Lovelace): ""the machine can be brought into play so as to give several results at the same time, which will greatly abridge the whole amount of the processes.""[11] Key topics Processor symmetry In a multiprocessing system, all CPUs may be equal, or some may be reserved for special purposes. A combination of hardware and operating system software design considerations determine the symmetry (or lack thereof) in a given system. For example, hardware or software considerations may require that only one particular CPU respond to all hardware interrupts, whereas all other work in the system may be distributed equally among CPUs; or execution of kernel-mode code may be restricted to only one particular CPU, whereas user-mode code may be executed in any combination of processors. Multiprocessing systems are often easier to design if such restrictions are imposed, but they tend to be less efficient than systems in which all CPUs are utilized. Systems that treat all CPUs equally are called symmetric multiprocessing (SMP) systems. In systems where all CPUs are not equal, system resources may be divided in a number of ways, including asymmetric multiprocessing (ASMP), non-uniform memory access (NUMA) multiprocessing, and clustered multiprocessing. Master/slave multiprocessor system In a master/slave multiprocessor system, the master CPU is in control of the computer and the slave CPU(s) performs assigned tasks. The CPUs can be completely different in terms of speed and architecture. Some (or all) of the CPUs can have share common bus, each can also have a private bus (for private resources), or they may be isolated except for a common communications pathway. Likewise, the CPUs can share common RAM and/or have private RAM that the other processor(s) cannot access. The roles of master and slave can change from one CPU to another. An early example of a master/slave multiprocessor system is the Tandy/Radio Shack TRS-80 Model 16 desktop computer which came out in February 1982 and ran the multi-user/multi-tasking Xenix operating system, Microsoft's version of UNIX (called TRS-XENIX). The Model 16 has 3 microprocessors, an 8-bit Zilog Z80 CPU running at 4MHz, a 16-bit Motorola 68000 CPU running at 6MHz and an Intel 8021 in the keyboard. When the system was booted, the Z-80 was the master and the Xenix boot process initialized the slave 68000, and then transferred control to the 68000, whereupon the CPUs changed roles and the Z-80 became a slave processor that was responsible for all I/O operations including disk, communications, printer and network, as well as the keyboard and integrated monitor, while the operating system and applications ran on the 68000 CPU. The Z-80 could be used to do other tasks. The earlier TRS-80 Model II, which was released in 1979, could also be considered a multiprocessor system as it had both a Z-80 CPU and an Intel 8021[12] microprocessor in the keyboard. The 8021 made the Model II the first desktop computer system with a separate detachable lightweight keyboard connected with by a single thin flexible wire, and likely the first keyboard to use a dedicated microprocessor, both attributes that would later be copied years later by Apple and IBM. Instruction and data streams In multiprocessing, the processors can be used to execute a single sequence of instructions in multiple contexts (single-instruction, multiple-data or SIMD, often used in vector processing), multiple sequences of instructions in a single context (multiple-instruction, single-data or MISD, used for redundancy in fail-safe systems and sometimes applied to describe pipelined processors or hyper-threading), or multiple sequences of instructions in multiple contexts (multiple-instruction, multiple-data or MIMD). Processor coupling Tightly coupled multiprocessor system Tightly coupled multiprocessor systems contain multiple CPUs that are connected at the bus level. These CPUs may have access to a central shared memory (SMP or UMA), or may participate in a memory hierarchy with both local and shared memory (SM)(NUMA). The IBM p690 Regatta is an example of a high end SMP system. Intel Xeon processors dominated the multiprocessor market for business PCs and were the only major x86 option until the release of AMD's Opteron range of processors in 2004. Both ranges of processors had their own onboard cache but provided access to shared memory; the Xeon processors via a common pipe and the Opteron processors via independent pathways to the system RAM. Chip multiprocessors, also known as multi-core computing, involves more than one processor placed on a single chip and can be thought of the most extreme form of tightly coupled multiprocessing. Mainframe systems with multiple processors are often tightly coupled. Loosely coupled multiprocessor system Main article: shared nothing architecture Loosely coupled multiprocessor systems (often referred to as clusters) are based on multiple standalone single or dual processor commodity computers interconnected via a high speed communication system (Gigabit Ethernet is common). A Linux Beowulf cluster is an example of a loosely coupled system. Tightly coupled systems perform better and are physically smaller than loosely coupled systems, but have historically required greater initial investments and may depreciate rapidly; nodes in a loosely coupled system are usually inexpensive commodity computers and can be recycled as independent machines upon retirement from the cluster. Power consumption is also a consideration. Tightly coupled systems tend to be much more energy efficient than clusters. This is because considerable economy can be realized by designing components to work together from the beginning in tightly coupled systems, whereas loosely coupled systems use components that were not necessarily intended specifically for use in such systems. Loosely coupled systems have the ability to run different operating systems or OS versions on different systems. ",,,,
ARC,"Operating system An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources. For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer – from cellular phones and video game consoles to web servers and supercomputers. The dominant desktop operating system is Microsoft Windows with a market share of around 82.74%. macOS by Apple Inc. is in second place (13.23%), and the varieties of Linux are collectively in third place (1.57%).[3] In the mobile sector (including smartphones and tablets), Android's share is up to 70% in the year 2017.[4] According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.[5] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications. Types of operating systems Single-tasking and multi-tasking A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running in concurrency. This is achieved by time-sharing, where the available processor time is divided between multiple processes. These processes are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and co-operative types. In preemptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, such as Solaris and Linux—as well as non-Unix-like, such as AmigaOS—support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking; 32-bit versions of both Windows NT and Win9x used preemptive multi-tasking. Single- and multi-user Single-user operating systems have no facilities to distinguish users, but may allow multiple programs to run in tandem.[6] A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users. Distributed A distributed operating system manages a group of distinct, networked computers and makes them appear to be a single computer, as all computations are distributed (divided amongst the constituent computers).[7] Templated In the distributed and cloud computing context of an OS, templating refers to creating a single virtual machine image as a guest operating system, then saving it as a tool for multiple running virtual machines. The technique is used both in virtualization and cloud computing management, and is common in large server warehouses.[8] Embedded Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines with less autonomy (e.g. PDAs). They are very compact and extremely efficient by design, and are able to operate with a limited amount of resources. Windows CE and Minix 3 are some examples of embedded operating systems. Real-time A real-time operating system is an operating system that guarantees to process events or data by a specific moment in time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. Such an event-driven system switches between tasks based on their priorities or external events, whereas time-sharing operating systems switch tasks based on clock interrupts. Library A library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with the application and configuration code to construct a unikernel: a specialized, single address space, machine image that can be deployed to cloud or embedded environments. Examples Unix and Unix-like operating systems Main article: Unix Evolution of Unix systems Unix was originally written in assembly language.[13] Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History). The Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name ""UNIX"" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. ""UNIX-like"" is commonly used to refer to the large set of operating systems which resemble the original UNIX. Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas. Four operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris can run on multiple types of hardware, including x86 and Sparc servers, and PCs. Apple's macOS, a replacement for Apple's earlier (non-Unix) Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD. Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants. BSD and its descendants Main article: Berkeley Software Distribution The first server for the World Wide Web ran on NeXTSTEP, based on BSD. A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP. In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T. Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web. Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995). macOS Main article: macOS macOS (formerly ""Mac OS X"" and later ""OS X"") is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. macOS is the successor to the original classic Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997. The operating system was first released in 1999 as Mac OS X Server 1.0, followed in March 2001 by a client version (Mac OS X v10.0 ""Cheetah""). Since then, six more distinct ""client"" and ""server"" editions of macOS have been released, until the two were merged in OS X 10.7 ""Lion"". Prior to its merging with macOS, the server edition – macOS Server – was architecturally identical to its desktop counterpart and usually ran on Apple's line of Macintosh server hardware. macOS Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as ""OS X"" (dropping ""Mac"" from the name). The server tools are now offered as an application.[14] Linux Main articles: Linux and Linux kernel Ubuntu, desktop Linux distribution The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel. Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smart-watches. Although estimates suggest that Linux is used on only 1.82% of all ""desktop"" (or laptop) PCs,[15] it has been widely adopted for use in servers[16] and embedded systems[17] such as cell phones. Linux has superseded Unix on many platforms and is used on most supercomputers including the top 385.[18] Many of the same computers are also on Green500 (but in different order), and Linux runs on the top 10. Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android, Chrome OS, and Chromium OS. Microsoft Windows Main article: Microsoft Windows Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers.[15][19][20][21] The latest version is Windows 10. In 2011, Windows 7 overtook Windows XP as most common version in use.[22][23][24] Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS[25][26] and 16-bit Windows 3.x[27] drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and 32-bit ARM microprocessors.[28] In addition Itanium is still supported in older server version Windows Server 2008 R2. In the past, Windows NT supported additional architectures. Server editions of Windows are widely used. In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a server operating system. However, Windows' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.[29][30] ReactOS is a Windows-alternative operating system, which is being developed on the principles of Windows – without using any of Microsoft's code. Other There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; classic Mac OS, the non-Unix precursor to Apple's macOS; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications. OpenVMS, formerly from DEC, is still under active development by VMS Software Inc. Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research. Another example is the Oberon System designed at ETH Zürich by Niklaus Wirth, Jürg Gutknecht and a group of students at the former Computer Systems Institute in the 1980s. It was used mainly for research, teaching, and daily work in Wirth's group. Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9. Components The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component. Kernel Main article: Kernel (computing) A kernel connects the application software to the hardware of a computer. With the aid of the firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc. Program execution Main article: Process (computing) The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices. Interrupts Main article: Interrupt Interrupts are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment. The alternative – having the operating system ""watch"" the various sources of input for events (polling) that require action – can be found in older systems with very small stacks (50 or 60 bytes) but is unusual in modern systems with large stacks. Interrupt-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place. When an interrupt is received, the computer's hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call. In modern operating systems, interrupts are handled by the operating system's kernel. Interrupts may come from either the computer's hardware or the running program. When a hardware device triggers an interrupt, the operating system's kernel decides how to deal with this event, generally by running some processing code. The amount of code being run depends on the priority of the interrupt (for example: a person usually responds to a smoke detector alarm before answering the phone). The processing of hardware interrupts is a task that is usually delegated to software called a device driver, which may be part of the operating system's kernel, part of another program, or both. Device drivers may then relay information to a running program by various means. A program may also trigger an interrupt to the operating system. If a program wishes to access hardware, for example, it may interrupt the operating system's kernel, which causes control to be passed back to the kernel. The kernel then processes the request. If a program wishes additional resources (or wishes to shed resources) such as memory, it triggers an interrupt to get the kernel's attention. Modes Main articles: User mode and Supervisor mode Privilege rings for the x86 microprocessor architecture available in protected mode. Operating systems determine which processes run in each mode. Modern microprocessors (CPU or MPU) support multiple modes of operation. CPUs with this capability offer at least two modes: user mode and supervisor mode. In general terms, supervisor mode operation allows unrestricted access to all machine resources, including all MPU instructions. User mode operation sets limits on instruction use and typically disallows direct access to machine resources. CPUs might have other modes similar to user mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one. At power-on or reset, the system begins in supervisor mode. Once an operating system kernel has been loaded and started, the boundary between user mode and supervisor mode (also known as kernel mode) can be established. Supervisor mode is used by the kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is accessed, and communicating with devices such as disk drives and video display devices. User mode, in contrast, is used for almost everything else. Application programs, such as word processors and database managers, operate within user mode, and can only access machine resources by turning control over to the kernel, a process which causes a switch to supervisor mode. Typically, the transfer of control to the kernel is achieved by executing a software interrupt instruction, such as the Motorola 68000 TRAP instruction. The software interrupt causes the microprocessor to switch from user mode to supervisor mode and begin executing code that allows the kernel to take control. In user mode, programs usually have access to a restricted set of microprocessor instructions, and generally cannot execute any instructions that could potentially cause disruption to the system's operation. In supervisor mode, instruction execution restrictions are typically removed, allowing the kernel unrestricted access to all machine resources. The term ""user mode resource"" generally refers to one or more CPU registers, which contain information that the running program isn't allowed to alter. Attempts to alter these resources generally causes a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting, for example, by forcibly terminating (""killing"") the program). Memory management Main article: Memory management Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory. Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system. Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which doesn't exist in all computers. In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt which cause the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error. Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway. Virtual memory Main article: Virtual memory Further information: Page fault Many operating systems can ""trick"" programs into using memory scattered around the hard disk and RAM as if it is one continuous chunk of memory, called virtual memory. The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks. If a program tries to access memory that isn't in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault. When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet. In modern operating systems, memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand. ""Virtual memory"" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.[31] Multitasking This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message) Main articles: Computer multitasking and Process management (computing) Further information: Context switch, Preemptive multitasking, and Cooperative multitasking Multitasking refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute. An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch. An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop. Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well. The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.) On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. The AmigaOS is an exception, having preemptive multitasking from its very first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it didn't reach the home user market until Windows XP (since Windows NT was targeted at professionals). Disk access and file systems Main article: Virtual file system This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message) File systems allow users and programs to organize and sort files on a computer, often through the use of directories (or ""folders""). Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree. Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system. While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers. A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices. When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates. Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ext3 and ReiserFS in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software). Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on. It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system. Device drivers Main article: Device driver This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message) A device driver is a specific type of computer software developed to allow interaction with ha",,,,
ARC,"pically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs. The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view. Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do. Networking Main article: Computer network This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2018) (Learn how and when to remove this template message) Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface. Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's IP address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel. Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware. Security Main article: Computer security A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.[citation needed] The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between ""privileged"" and ""non-privileged"", systems commonly have a form of requester identity, such as a user name. To establish identity there may be a process of authentication. Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester identity is authorization; the particular services and resources accessible by the requester once logged into a system are tied to either the requester's user account or to the variously configured groups of users to which the requester belongs.[citation needed] In addition to the allow or disallow model of security, a system with a high level of security also offers auditing options. These would allow tracking of requests for access to resources (such as, ""who has been reading this file?""). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.[citation needed] External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the Trusted Computer System Evaluation Criteria (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information. Network services include offerings such as file sharing, print services, email, web sites, and file transfer protocols (FTP), most of which can have compromised security. At the front line of security are hardware devices known as firewalls or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port. An alternative strategy, and the only sandbox strategy available in systems that do not meet the Popek and Goldberg virtualization requirements, is where the operating system is not running user programs as native code, but instead either emulates a processor or provides a host for a p-code based system such as Java. Internal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing. User interface Real-time operating systems Main article: Real-time operating system A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems. An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System. Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase.[33] Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b. Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing. Operating system development as a hobby See also: Hobbyist operating system development Operating system development is one of the most complicated activities in which a computing hobbyist may engage.[citation needed] A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.[34] In some cases, hobby development is in support of a ""homebrew"" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests. Examples of a hobby operating system include Syllable and TempleOS. Diversity of operating systems and portability Application software is generally written for use on a specific operating system, and sometimes even for specific hardware.[citation needed] When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained. Unix was the first operating system not written in assembly language, making it very portable to systems different from its native PDP-11.[35] This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries. Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs. ",,,,
ARC,"Computer - Operating System The Operating System is a program with the following features ? An operating system is a program that acts as an interface between the software and the computer hardware. It is an integrated set of specialized programs used to manage overall resources and operations of the computer. It is a specialized software that controls and monitors the execution of all other programs that reside in the computer, including application programs and other system software. Operating System Objectives of Operating System The objectives of the operating system are ? To make the computer system convenient to use in an efficient manner. To hide the details of the hardware resources from the users. To provide users a convenient interface to use the computer system. To act as an intermediary between the hardware and its users, making it easier for the users to access and use other resources. To manage the resources of a computer system. To keep track of who is using which resource, granting resource requests, and mediating conflicting requests from different programs and users. To provide efficient and fair sharing of resources among users and programs. Characteristics of Operating System Here is a list of some of the most prominent characteristic features of Operating Systems ? Memory Management ? Keeps track of the primary memory, i.e. what part of it is in use by whom, what part is not in use, etc. and allocates the memory when a process or program requests it. Processor Management ? Allocates the processor (CPU) to a process and deallocates the processor when it is no longer required. Device Management ? Keeps track of all the devices. This is also called I/O controller that decides which process gets the device, when, and for how much time. File Management ? Allocates and de-allocates the resources and decides who gets the resources. Security ? Prevents unauthorized access to programs and data by means of passwords and other similar techniques. Job Accounting ? Keeps track of time and resources used by various jobs and/or users. Control Over System Performance ? Records delays between the request for a service and from the system. Interaction with the Operators ? Interaction may take place via the console of the computer in the form of instructions. The Operating System acknowledges the same, does the corresponding action, and informs the operation by a display screen. Error-detecting Aids ? Production of dumps, traces, error messages, and other debugging and error-detecting methods. Coordination Between Other Software and Users ? Coordination and assignment of compilers, interpreters, assemblers, and other software to the various users of the computer systems. ",,,,
NET,"Computer network A computer network is a group of computers that use a set of common communication protocols over digital interconnections for the purpose of sharing resources located on or provided by the network nodes. The interconnections between nodes are formed from a broad spectrum of telecommunication network technologies, based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies. The nodes of a computer network may be classified by many means as personal computers, servers, networking hardware, or general purpose hosts. They are identified by hostnames and network addresses. Hostnames serve as memorable labels for the nodes, rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol. Computer networks may be classified by many criteria, for example, the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent. Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications. Use A computer network extends interpersonal communications by electronic means with various technologies, such as email, instant messaging, online chat, voice and video telephone calls, and video conferencing. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer or use of a shared storage device. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. Distributed computing uses computing resources across a network to accomplish tasks. Network packet Most modern computer networks use protocols based on packet-mode transmission. A network packet is a formatted unit of data carried by a packet-switched network. The physical link technologies of packet network typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message is fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message. Packets consist of two types of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between. With packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link isn't overused. Often the route a packet needs to take through a network is not immediately available. In that case, the packet is queued and waits until a link is free. Network topology Common network topologies Network topology is the layout, pattern, or organizational hierarchy of the interconnection of network hosts, in contrast to their physical or geographic location. Typically, most diagrams describing networks are arranged by their topology. The network topology can affect throughput, but reliability is often more critical.[citation needed] With many technologies, such as bus networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the more expensive it is to install. Common layouts are: Bus network: all nodes are connected to a common medium along this medium. This was the layout used in the original Ethernet, called 10BASE5 and 10BASE2. This is still a common topology on the data link layer, although modern physical layer variants use point-to-point links instead. Star network: all nodes are connected to a special central node. This is the typical layout found in a Wireless LAN, where each wireless client connects to the central Wireless access point. Ring network: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The Fiber Distributed Data Interface (FDDI) made use of such a topology. Mesh network: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other. Fully connected network: each node is connected to every other node in the network. Tree network: nodes are arranged hierarchically. The physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring, but the physical topology is often a star, because all neighboring connections can be routed via a central physical location. Physical layout is not completely irrelevant, however, as common ducting and equipment locations can represent single points of failure due to issues like fires, power failures and flooding. Overlay network A sample overlay network An overlay network is a virtual network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet.[22] Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems, before any data network existed. The most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network.[22] Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network. Another example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys. Overlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees to achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP Multicast have not seen wide acceptance largely because they require modification of all routers in the network.[citation needed] On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers. The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination. For example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast). Academic research includes end system multicast,[23] resilient routing and quality of service studies, among others. Network links Further information: Data transmission The transmission media (often referred to in the literature as the physical medium) used to link devices to form a computer network include electrical cable, optical fiber, and free space. In the OSI model, the software to handle the media are defined at layers 1 and 2 — the physical layer and the data link layer. A widely adopted family that uses copper and fiber media in local area network (LAN) technology is collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Wireless LAN standards use radio waves, others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data. Wired technologies Bundle of glass threads with light emitting from the ends Fiber optic cables are used to transmit light from one computer/network node to another The following classes of wired technologies are used in computer networking. Coaxial cable is widely used for cable television systems, office buildings, and other work-sites for local area networks. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second.[citation needed] ITU-T G.hn technology uses existing home wiring (coaxial cable, phone lines and power lines) to create a high-speed local area network. Twisted pair cabling is used for wired Ethernet and other standards. It typically consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce crosstalk and electromagnetic induction. The transmission speed ranges from 2 Mbit/s to 10 Gbit/s. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios. World map with red and blue lines 2007 map showing submarine optical fiber telecommunication cables around the world. An optical fiber is a glass fiber. It carries pulses of light that represent data via lasers and optical amplifiers. Some advantages of optical fibers over metal wires are very low transmission loss and immunity to electrical interference. Using dense wave division multiplexing, optical fibers can simultaneously carry multiple streams of data on different wavelengths of light, which greatly increases the rate that data can be sent to up to trillions of bits per second. Optic fibers can be used for long runs of cable carrying very high data rates, and are used for undersea cables to interconnect continents. There are two basic types of fiber optics, single-mode optical fiber (SMF) and multi-mode optical fiber (MMF). Single-mode fiber has the advantage of being able to sustain a coherent signal for dozens or even a hundred kilometers. Multimode fiber is cheaper to terminate but is limited to a few hundred or even only a few dozens of meters, depending on the data rate and cable grade.[24] Wireless technologies Black laptop with router in the background Computers are very often connected to networks using wireless links Main article: Wireless network Network connections can be established wirelessly using radio or other electromagnetic means of communication. Terrestrial microwave – Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 40 miles (64 km) apart. Communications satellites – Satellites also communicate via microwave. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals. Cellular networks use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area is served by a low-power transceiver. Radio and spread spectrum technologies – Wireless LANs use a high-frequency radio technology similar to digital cellular. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wi-Fi. Free-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices. Exotic technologies There have been various attempts at transporting data over exotic media. IP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.[25] Extending the Internet to interplanetary dimensions via radio waves, the Interplanetary Internet.[26] Both cases have a large round-trip delay time, which gives slow two-way communication but doesn't prevent sending large amounts of information. Network nodes Main article: Node (networking) Apart from any physical transmission media, networks are built from additional basic system building blocks, such as network interface controllers (NICs), repeaters, hubs, bridges, switches, routers, modems, and firewalls. Any particular piece of equipment will frequently contain multiple building blocks and so may perform multiple functions. Network interfaces A network interface circuit with port for ATM An ATM network interface in the form of an accessory card. A lot of network interfaces are built-in. A network interface controller (NIC) is computer hardware that connects the computer to the network media and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry. In Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address—usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce. Repeaters and hubs A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of an obstruction so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart. Repeaters work on the physical layer of the OSI model but still require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters used in a network, e.g., the Ethernet 5-4-3 rule. An Ethernet repeater with multiple ports is known as an Ethernet hub. In addition to reconditioning and distributing network signals, a repeater hub assists with collision detection and fault isolation for the network. Hubs and repeaters in LANs have been largely obsoleted by modern network switches. Bridges A network bridge opeates at the data link layer (layer 2) of the OSI model and connects and filters traffic between two network segments to form a single network. This divides the network's collision domain but maintains a single broadcast domain. Network segmentation through bridging breaks down a large, congested network into an aggregation of smaller, more efficient networks. Switches A network switch is a device that forwards and filters OSI layer 2 datagrams (frames) between ports based on the destination MAC address in each frame.[27] A switch is distinct from a hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected. It can be thought of as a multi-port bridge.[28] It learns to associate physical ports to MAC addresses by examining the source addresses of received frames. If an unknown destination is targeted, the switch broadcasts to all ports but the source. Switches normally have numerous ports, facilitating a star topology for devices, and cascading additional switches. Routers A typical home or small office router showing the ADSL telephone line and Ethernet network cable connections A router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram (Internet protocol information from layer 3). The routing information is often processed in conjunction with the routing table (or forwarding table). A router uses its routing table to determine where to forward packets. A destination in a routing table can include a ""null"" interface, also known as the ""black hole"" interface because data can go into it, however, no further processing is done for said data, i.e. the packets are dropped. Modems Modems (MOdulator-DEModulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Modems are commonly used for telephone lines, using a digital subscriber line technology. Firewalls A firewall is a network device for controlling network security and access rules. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks. Communication protocols Protocols in relation to the Internet layering scheme. The TCP/IP model or Internet layering scheme and its relation to common protocols often layered on top of it. When a router is present, message flows go down through protocol layers, across to the router, up the stack inside the router and back down again and is sent on to the final destination where it climbs back up the stack Message flows (A-B) in the presence of a router (R), red flows are effective communication paths, black paths are across the actual network links. A communication protocol is a set of rules for exchanging information over a network. In a protocol stack (also see the OSI model), each protocol leverages the services of the protocol layer below it, until the lowest layer controls the hardware that sends information across the media. The use of protocol layering is today ubiquitous across the field of computer networking. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web. Communication protocols have various characteristics. They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing. There are many communication protocols, a few of which are described below. IEEE 802 IEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at levels 1 and 2 of the OSI model. For example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based Network Access Control protocol, which forms the basis for the authentication mechanisms used in VLANs (but it is also found in WLANs) – it is what the home user sees when the user has to enter a ""wireless access key"". Ethernet Ethernet, sometimes simply called LAN, is a family of protocols used in wired LANs, described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers. Wireless LAN Wireless LAN, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. It is standardized by IEEE 802.11 and shares many properties with wired Ethernet. Internet Protocol Suite The Internet Protocol Suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less as well as connection-oriented services over an inherently unreliable network traversed by datagram transmission at the Internet protocol (IP) level. At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability. The Internet Protocol Suite is the defining set of protocols for the Internet. Although many computers communicate via the Internet, it is actually a network of networks, as elaborated by Andrew Tannenbaum.[29] SONET/SDH Synchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support real-time, uncompressed, circuit-switched voice encoded in PCM (Pulse-Code Modulation) format. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames. Asynchronous Transfer Mode Asynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks. It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet Protocol Suite or Ethernet that use variable sized packets or frames. ATM has similarity with both circuit and packet switched networking. This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins. While the role of ATM is diminishing in favor of next-generation networks, it still plays a role in the last mile, which is the connection between an Internet service provider and the home user.[30] Cellular standards There are a number of different digital cellular standards, including: Global System for Mobile Communications (GSM), General Packet Radio Service (GPRS), cdmaOne, CDMA2000, Evolution-Data Optimized (EV-DO), Enhanced Data Rates for GSM Evolution (EDGE), Universal Mobile Telecommunications System (UMTS), Digital Enhanced Cordless Telecommunications (DECT), Digital AMPS (IS-136/TDMA), and Integrated Digital Enhanced Network (iDEN).[31] Geographic scale Computer network types by spatial scope Data Networks classification by spatial scope.png Nanoscale Near-field (NFC) Body (BAN) Personal (PAN) Near-me (NAN) Local (LAN) Home (HAN) Storage (SAN) Wireless (WLAN) Campus (CAN) Backbone Metropolitan (MAN) Municipal wireless (MWN) Wide (WAN) Cloud (IAN) Internet Interplanetary Internet vte Networks may be characterized by many properties or features, such as physical capacity, organizational purpose, user authorization, access rights, and others. Another distinct classification method is that of physical extent, or geographic scale. Nanoscale network A nanoscale communication network has key components implemented at the nanoscale including message carriers and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for classical communication.[32] Personal area network A personal area network (PAN) is a computer network used for communication among computer and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters.[33] A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN. Local area network A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Each computer or device on the network is a node. Wired LANs are most likely based on Ethernet technology. Newer standards such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.[34] The defining characteristics of a LAN, in contrast to a wide area network (WAN), include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to 100 Gbit/s, standardized by IEEE in 2010.[35] Currently, 400 Gbit/s Ethernet is being developed. A LAN can be connected to a WAN using a router. Home area network A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable TV or digital subscriber line (DSL) provider. Storage area network A storage area network (SAN) is a dedicated network that provides access to consolidated, block level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear like locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments. Campus area network A campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, copper plant, Cat5 cabling, etc.) are almost entirely owned by the campus tenant / owner (an enterprise, university, government, etc.). For example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls. Backbone network A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or subnetworks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area. For example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. When designing a network backbone, network performance and network congestion are critical factors to take into account. Normally, the backbone network's capacity is greater than that of the individual networks connected to it. Another example of a backbone network is the Internet backbone, which is a massive, global system of fiber-optic cable and optical networking that carry the bulk of data between wide area networks (WANs), metro, regional, national and transoceanic networks. Metropolitan area network A Metropolitan area network (MAN) is a large computer network that usually spans a city or a large campus. Wide area network A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and air waves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI reference model: the physical layer, the data link layer, and the network layer. Enterprise private network An enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. Virtual private network A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features. VPN may have best-effort performance, or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider. Generally, a VPN has a topology more complex than point-to-point. Global area network A global area network (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.[36] Organizational scope Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity. Intranet An intranet is a set of networks that are under the control of a single administrative entity. The intranet uses the IP protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. An intranet is also anything behind the router on a local area network. Extranet An extranet is a network that is also under the administrative control of a single organization, but supports a limited connection to a specific external network. For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers. These other entities are not necessarily trusted from a security standpoint. Network connection to an extranet is often, but not always, implemented via WAN technology. Internetwork An internetwork is the connection of multiple different types of computer networks to form a single computer network by layering on top of the different networking software and connecting them together using routers. Internet Partial map of the Internet, based on the January 15, 2005 data found on opte.org. Each line is drawn between two nodes, representing two IP addresses. The length of the lines are indicative of the delay between those two nodes. This graph represents less than 30% of the Class C networks reachable. The Internet is the largest example of an",,,,
NET,"k. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet utilizes copper communications and the optical networking backbone to enable the World Wide Web (WWW), the Internet of Things, video transfer and a broad range of information services. Participants in the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system (IP addresses) administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths. Darknet A darknet is an overlay network, typically running on the Internet, that is only accessible through specialized software. A darknet is an anonymizing network where connections are made only between trusted peers — sometimes called ""friends"" (F2F)[37] — using non-standard protocols and ports. Darknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.[38] Routing Routing calculates good paths through a network for information to take. For example, from node 1 to node 6 the best routes are likely to be 1-8-7-6 or 1-8-10-6, as this has the thickest routes. Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks. In packet-switched networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing. There are usually multiple routes that can be taken, and to choose between them, different elements can be considered to decide which routes get installed into the routing table, such as (sorted by priority): Prefix-Length: where longer subnet masks are preferred (independent if it is within a routing protocol or over different routing protocol) Metric: where a lower metric/cost is preferred (only valid within one and the same routing protocol) Administrative distance: where a lower distance is preferred (only valid between different routing protocols) Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths. Routing, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments. Network service Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate. The World Wide Web, E-mail,[39] printing and network file sharing are examples of well-known network services. Network services such as DNS (Domain Name System) give names for IP and MAC addresses (people remember names like “nm.lan” better than numbers like “210.121.67.18”),[40] and DHCP to ensure that the equipment on the network has a valid IP address.[41] Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service. Network performance Bandwidth Main article: Bandwidth (computing) Bandwidth in bit/s may refer to consumed bandwidth, corresponding to achieved throughput or goodput, i.e., the average rate of successful data transfer through a communication path. The throughput is affected by technologies such as bandwidth shaping, bandwidth management, bandwidth throttling, bandwidth cap, bandwidth allocation (for example bandwidth allocation protocol and dynamic bandwidth allocation), etc. A bit stream's bandwidth is proportional to the average consumed signal bandwidth in hertz (the average spectral bandwidth of the analog signal representing the bit stream) during a studied time interval. Network delay Main article: Network delay Any data sent across a network requires time to travel from source to destination. Depending on the application, the one-way delay or the round-trip time can have a significant impact on performance. Quality of service Depending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency. The following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM: Circuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads.[42] Other types of performance measures can include the level of noise and echo. ATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique and modem enhancements.[43][verification needed][full citation needed] There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modelled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.[44] Network congestion Network congestion occurs when a link or node is subjected to a greater data load than it is rated for, resulting in a deterioration of its quality of service. Typical effects include queueing delay, packet loss or the blocking of new connections. A consequence of these latter two is that incremental increases in offered load lead either to only a small increase in network throughput, or to a reduction in network throughput. Network protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion—even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse. Modern networks use congestion control, congestion avoidance and traffic control techniques to try to avoid congestion collapse. These include: exponential backoff in protocols such as 802.11's CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing priority schemes, so that some packets are transmitted with higher priority than others. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for some services. An example of this is 802.1p. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn standard, which provides high-speed (up to 1 Gbit/s) Local area networking over existing home wires (power lines, phone lines and coaxial cables). For the Internet, RFC 2914 addresses the subject of congestion control in detail. Network resilience Network resilience is ""the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.”[45] Security Main article: Computer security Computer networks are also used by security hackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial-of-service attack. Network security Network security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources.[46] Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority. Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies and individuals. Network surveillance Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency. Computer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity. Surveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high-speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.[47] However, many civil rights and privacy groups—such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union—have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T.[47][48] The hacktivist group Anonymous has hacked into government websites in protest of what it considers ""draconian surveillance"".[49][50] End to end encryption End-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet providers or application service providers, from discovering or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity. Examples of end-to-end encryption include HTTPS for web traffic, PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio. Typical server-based communications systems do not include end-to-end encryption. These systems can only guarantee protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. Some such systems, for example LavaBit and SecretInk, have even described themselves as offering ""end-to-end"" encryption when they do not. Some systems that normally offer end-to-end encryption have turned out to contain a back door that subverts negotiation of the encryption key between the communicating parties, for example Skype or Hushmail. The end-to-end encryption paradigm does not directly address risks at the communications endpoints themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the endpoints and the times and quantities of messages that are sent. SSL/TLS The introduction and rapid growth of e-commerce on the World Wide Web in the mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a client requests access to an SSL-secured server, the server sends a copy of the certificate to the client. The SSL client checks this certificate (all web browsers come with an exhaustive list of CA root certificates preloaded), and if the certificate checks out, the server is authenticated and the client negotiates a symmetric-key cipher for use in the session. The session is now in a very secure encrypted tunnel between the SSL server and the SSL client.[24] Views of networks Users and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running. A community of interest has less of a connection of being in a local area, and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies. Network administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using virtual LAN (VLAN) technology. Both users and administrators are aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees).[51] Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).[51] Unofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, that share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS). Over the Internet, there can be business-to-business (B2B), business-to-consumer (B2C) and consumer-to-consumer (C2C) communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism. Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure Virtual Private Network (VPN) technology. ",,,,
NET,"Basics of Computer Networking Open system: A system which is connected to the network and is ready for communication. Closed system: A system which is not connected to the network and can’t be communicated with. Computer Network: An interconnection of multiple devices, also known as hosts, that are connected using multiple paths for the purpose of sending/receiving data or media. Computer networks can also include multiple devices/mediums which help in the communication between two different devices; these are known as Network devices and include things such as routers, switches, hubs, and bridges. Computer-Networking-Diagram Network Topology: The layout arrangement of the different devices in a network. Common examples include: Bus, Star, Mesh, Ring, and Daisy chain. Network-Topology-Diagram OSI: OSI stands for Open Systems Interconnection. It is a reference model that specifies standards for communications protocols and also the functionalities of each layer. Protocol: A protocol is the set of rules or algorithms which define the way how two entities can communicate across the network and there exists different protocol defined at each layer of the OSI model. Few of such protocols are TCP, IP, UDP, ARP, DHCP, FTP and so on. UNIQUE IDENTIFIERS OF NETWORK Host name: Each device in the network is associated with a unique device name known as Hostname. Type “hostname” in the command prompt(Administrator Mode) and press ‘Enter’, this displays the hostname of your machine. IP Address (Internet Protocol address): Also known as the Logical Address, the IP Address is the network address of the system across the network. To identify each device in the world-wide-web, the Internet Assigned Numbers Authority (IANA) assigns an IPV4 (Version 4) address as a unique identifier to each device on the Internet. The length of an IPv4 address is 32-bits, hence, we have 232 IP addresses available. The length of an IPv6 address is 128-bits. Type “ipconfig” in the command prompt and press ‘Enter’, this gives us the IP address of the device. MAC Address (Media Access Control address): Also known as physical address, the MAC Address is the unique identifier of each host and is associated with its NIC (Network Interface Card). A MAC address is assigned to the NIC at the time of manufacturing. The length of the MAC address is : 12-nibble/ 6 bytes/ 48 bits Type “ipconfig/all” in the command prompt and press ‘Enter’, this gives us the MAC address. Port: A port can be referred to as a logical channel through which data can be sent/received to an application. Any host may have multiple applications running, and each of these applications is identified using the port number on which they are running. A port number is a 16-bit integer, hence, we have 216 ports available which are categorized as shown below: PORT TYPES",RANGE Well known Ports,0 – 1023 Registered Ports,1024 – 49151 Ephemeral Ports,"49152 – 65535 Number of ports: 65,536 Range: 0 – 65535 Type “netstat -a” in the command prompt and press ‘Enter’, this lists all the ports being used. Socket: The unique combination of IP address and Port number together are termed as Socket. Other related concepts DNS Server: DNS stands for Domain Name system. DNS is basically a server which translates web addresses or URLs (ex: www.google.com) into their corresponding IP addresses. We don’t have to remember all the IP addresses of each and every website. The command ‘nslookup’ gives you the IP address of the domain you are looking for. This also provides the information of our DNS Server. ARP: ARP stands for Address Resolution Protocol. It is used to convert an IP address to its corresponding physical address(i.e., MAC Address). ARP is used by the Data Link Layer to identify the MAC address of the Receiver’s machine. RARP: RARP stands for Reverse Address Resolution Protocol. As the name suggests, it provides the IP address of the device given a physical address as input. But RARP has become obsolete since the time DHCP has come into the picture. This article is contributed by Kundana Thiyari. If you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above. "
NET,"A computer network is a system in which multiple computers are connected to each other to share information and resources. Computer Networks Characteristics of a Computer Network Share resources from one computer to another. Create files and store them in one computer, access those files from the other computer(s) connected over the network. Connect a printer, scanner, or a fax machine to one computer within the network and let other computers of the network use the machines available over the network. Following is the list of hardware's required to set up a computer network. Network Cables Distributors Routers Internal Network Cards External Network Cards Network Cables Network cables are used to connect computers. The most commonly used cable is Category 5 cable RJ-45. Network Cables Distributors A computer can be connected to another one via a serial port but if we need to connect many computers to produce a network, this serial connection will not work. Network Distributors The solution is to use a central body to which other computers, printers, scanners, etc. can be connected and then this body will manage or distribute network traffic. Router A router is a type of device which acts as the central point among computers and other devices that are a part of the network. It is equipped with holes called ports. Computers and other devices are connected to a router using network cables. Now-a-days router comes in wireless modes using which computers can be connected without any physical cable. Network Router Network Card Network card is a necessary component of a computer without which a computer cannot be connected over a network. It is also known as the network adapter or Network Interface Card (NIC). Most branded computers have network card pre-installed. Network cards are of two types: Internal and External Network Cards. Internal Network Cards Motherboard has a slot for internal network card where it is to be inserted. Internal network cards are of two types in which the first type uses Peripheral Component Interconnect (PCI) connection, while the second type uses Industry Standard Architecture (ISA). Network cables are required to provide network access. Network Card External Network Cards External network cards are of two types: Wireless and USB based. Wireless network card needs to be inserted into the motherboard, however no network cable is required to connect to the network. External Network Card Universal Serial Bus (USB) USB card is easy to use and connects via USB port. Computers automatically detect USB card and can install the drivers required to support the USB network card automatically. ",,,,
NET,"Computer networks components comprise both physical parts as well as the software required for installing computer networks, both at organizations and at home. The hardware components are the server, client, peer, transmission medium, and connecting devices. The software components are operating system and protocols. The following figure shows a network along with its components ? Hardware Components Servers ?Servers are high-configuration computers that manage the resources of the network. The network operating system is typically installed in the server and so they give user accesses to the network resources. Servers can be of various kinds: file servers, database servers, print servers etc. Clients ? Clients are computers that request and receive service from the servers to access and use the network resources. Peers ? Peers are computers that provide as well as receive services from other peers in a workgroup network. Transmission Media ? Transmission media are the channels through which data is transferred from one device to another in a network. Transmission media may be guided media like coaxial cable, fibre optic cables etc; or maybe unguided media like microwaves, infra-red waves etc. Connecting Devices ? Connecting devices act as middleware between networks or computers, by binding the network media together. Some of the common connecting devices are: a. Routers b. Bridges c. Hubs d. Repeaters e. Gateways f. Switches Software Components Networking Operating System ? Network Operating Systems is typically installed in the server and facilitate workstations in a network to share files, database, applications, printers etc. Protocol Suite ? A protocol is a rule or guideline followed by each computer for data communication. Protocol suite is a set of related protocols that are laid down for computer networks. The two popular protocol suites are ? a. OSI Model ( Open System Interconnections) b. TCP / IP Model ",,,,
NET,"Basic computer network components Jump to navigationJump to search Computer networks share common devices, functions, and features including servers, clients, transmission media, shared data, shared printers and other hardware and software resources, network interface card(NIC), local operating system(LOS), and the network operating system (NOS). Servers - Servers are computers that hold shared files, programs, and the network operating system. Servers provide access to network resources to all the users of the network. There are many different kinds of servers, and one server can provide several functions. For example, there are file servers, print servers, mail servers, communication servers, database servers, fax servers and web servers, to name a few. Sometimes it is also called host computer, servers are powerful computer that store data or application and connect to resources that are shared by the user of a network. Clients - Clients are computers that access and use the network and shared network resources. Client computers are basically the customers(users) of the network, as they request and receive services from the servers. These days, it is typical for a client to be a personal computer that the users also use for their own non-network applications. Transmission Media - Transmission media are the facilities used to interconnect computers in a network, such as twisted-pair wire, coaxial cable, and optical fiber cable. Transmission media are sometimes called transmission medium channels, links or lines. Shared data - Shared data are data that file servers provide to clients such as data files, printer access programs and e-mail. Shared printers and other peripherals - Shared printers and peripherals are hardware resources provided to the users of the network by servers. Resources provided include data files, printers, software, or any other items used by clients on the network. Network Interface Card - Each computer in a network has a special expansion card called a network interface card (NIC). The NIC prepares(formats) and sends data, receives data, and controls data flow between the computer and the network. On the transmit side, the NIC passes frames of data on to the physical layer, which transmits the data to the physical link. On the receiver's side, the NIC processes bits received from the physical layer and processes the message based on its contents. Local Operating System - A local operating system allows personal computers to access files, print to a local printer, and have and use one or more disk and CD drives that are located on the computer. Examples are MS-DOS, Unix, Linux, Windows 2000, Windows 98, Windows XP etc. The network operating system is the software of the network. It serves a similar purpose that the OS serves in a stand-alone computer Network Operating System - The network operating system is a program that runs on computers and servers that allows the computers to communicate over the network. Hub - Hub is a device that splits a network connection into multiple computers. It is like a distribution center. When a computer requests information from a network or a specific computer, it sends the request to the hub through a cable. The hub will receive the request and transmit it to the entire network. Each computer in the network should then figure out whether the broadcast data is for them or not. Switch - Switch is a telecommunication device grouped as one of computer network components. Switch is like a Hub but built in with advanced features. It uses physical device addresses in each incoming messages so that it can deliver the message to the right destination or port. Unlike a hub, switch doesn't broadcast the received message to entire network, rather before sending it checks to which system or port should the message be sent. In other words, switch connects the source and destination directly which increases the speed of the network. Both switch and hub have common features: Multiple RJ-45 ports, power supply and connection lights. Router - When we talk about computer network components, the other device that used to connect a LAN with an internet connection is called Router. When you have two distinct networks (LANs) or want to share a single internet connection to multiple computers, we use a Router. In most cases, recent routers also include a switch which in other words can be used as a switch. You don’t need to buy both switch and router, particularly if you are installing small business and home networks. There are two types of Router: wired and wireless. The choice depends on your physical office/home setting, speed and cost. LAN Cable A local area Network cable is also known as data cable or Ethernet cable which is a wired cable used to connect a device to the internet or to other devices like computer, printers, etc. ",,,,
DB,"Database A database is an organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques. The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a ""database system"". Often the term ""database"" is also used to loosely refer to any of the DBMS, the database system or an application associated with the database. Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages. ",,,,
DB,"‘Recent Articles’ on DBMS ! Introduction Entity Relationship Model Relational Model Relational Algebra Functional Dependencies Normalisation Transactions and Concurrency Control Indexing, B and B+ trees File Organization Advanced Topics SQL Quick Links Introduction : DBMS Introduction | Set 1 DBMS Introduction | Set 2 (3-Tier Architecture) DBMS Architecture 2-level 3-level Need For DBMS Data Abstraction and Data Independence Database Objects Multimedia Database Interfaces Categories of End Users Use of DBMS in System Software Choice of DBMS | Economic factors Disadvantages of DBMS Entity Relationship Model : ER Model Enhanced ER Model Minimization of ER Diagram ER Model: Generalization, Specialization and Aggregation Recursive Relationships Impedance Mismatch Relational Model : Relational Model and CODD Rules Relational Model Keys in Relational Model (Candidate, Super, Primary, Alternate and Foreign) Number of possible Superkeys Anomalies in Relational Model Mapping from ER Model to Relational Model Strategies for Schema design Schema Integration Star Schema in Data Warehouse modeling Data Warehouse Modeling | Snowflake Schema Dimensional Data Modeling >> Quiz on ER and Relational Model Relational Algebra : Introduction Basic Operators Extended Operators Inner Join vs Outer Join Join operation Vs nested query DBMS | Tupple Relational Calculus Row oriented vs. column oriented data stores How to solve Relational Algebra Problems for GATE How to Solve Relational Algebra Problems for GATE Functional Dependencies : Functional Dependency and Attribute Closure Finding Attribute Closure and Candidate Keys using Functional Dependencies Armstrong’s Axioms in Functional Dependency Equivalence of Functional Dependencies Canonical Cover Normalisation : Introduction Normal Forms Minimum relations satisfying 1NF The Problem of redundancy in Database Dependency Preserving Decomposition Lossless Join Decomposition LossLess Join and Dependency Preserving Decomposition How to find the Highest Normal Form of a Relation Domain Key normal form Introduction of 4th and 5th Normal form Denormalization in Databases DBMS | Data Replication >> Quiz on Normal Forms Transactions and Concurrency Control : Introduction ACID Properties Concurrency Control -Introduction Implementation of Locking in DBMS Concurrency Control Protocols – Lock Based Protocol Concurrency Control Protocol | Graph Based Protocol Concurrency Control Protocol | Two Phase Locking (2-PL)-I Concurrency Control Protocol | Two Phase Locking (2-PL)-II Concurrency Control Protocol | Two Phase Locking (2-PL)-III Concurrency Control Protocol | Multiple Granularity Locking Concurrency Control Protocol | Thomas Write Rule Concurrency Control | Polygraph to check View Serializabilty DBMS | Log based recovery Timestamp Ordering Protocols Introduction to TimeStamp and Deadlock Prevention Schemes Dirty read in SQL Types of Schedules Conflict Serializability View Serializability How to test if two schedules are View Equal or not ? Recoverability of Schedules Precedence Graph for testing Conflict Serializabilty Transaction Isolation Levels in DBMS Database Recovery Techniques Starvation in DBMS Deadlock in DBMS DBMS | OLAP vs OLTP Types of OLAP Systems DBMS | Types of Recoverability of Schedules and easiest way to test schedule | Set 2 Web Information Retrieval | Vector Space Model Why recovery is needed? >> Quiz on Transactions and concurrency control Indexing, B and B+ trees : Indexing and its Types B-Tree | Set 1 (Introduction) B-Tree | Set 2 (Insert) B-Tree | Set 3 (Delete) B+ Tree (Introduction) Bitmap Indexing Inverted Index Difference between Inverted Index and Forward Index SQL queries on clustered and non-clustered Indexes >> Practice questions on B and B+ Trees >> Quizzes on Indexing, B and B+ Trees File Organization: File Organization – Set 1 File Organization – Set 2 (Hashing in DBMS) File Organization – Set 3 File Organization – Set 4 >> Quiz on File structures Advanced Topics : RAID Query Optimization How to store a password in database? Storage Area Networks Network attached storage Data Warehousing Data Warehouse Architecture Characteristics and Functions of Data warehouse Difficulties of Implementing Data Warehouses Data Mining Data Mining | KDD process Data Mining | Sources of Data that can be mined Data Marts ODBMS – Definition and overview Architecture of HBase Apache HBase Architecture and Working of Hive Apache Hive Difference between Hive and HBase Difference between RDBMS and HBase Challenges of database security Federated database management system issues Distributed Database System Functions of Distributed Database System Semantic Heterogeneity Advantages of Distributed database Comparison – Centralized, Decentralized and Distributed Systems Characteristics of Biological Data (Genome Data Management) Data Management issues in Mobile database Future Works in Geographic Information System Difference between Structured, Semi-structured and Unstructured data SQL Tutorial SQL | Tutorials Quiz on SQL DBMS practices questions : Database Management Systems | Set 1 Database Management Systems | Set 2 Database Management Systems | Set 3 Database Management Systems | Set 4 Database Management Systems | Set 5 Database Management Systems | Set 6 Database Management Systems | Set 7 Database Management Systems | Set 8 Database Management Systems | Set 9 Database Management Systems | Set 10 Database Management Systems | Set 11 Quick Links : Last Minutes Notes(LMNs) on DBMS Quizzes on DBMS ! ‘Practice Problems’ on DBMS ! DBMS interview questions | Set 1 DBMS interview questions | Set 2 ",,,,
DB,"What is Database ? Last Updated: 12-09-2019 The Database is an essential part of our life. As we encounter several activities that involve our interaction with database, for example in the bank, in the railway station, in school, in a grocery store, etc. These are the places where we need to a large amount of data at one place and fetching of this data should be easy. A database is a collection of data which is organized, which is also called as structured data. It can be accessed or stored at the computer system. It can be managed through Database management system (DBMS), which is a software which is used to manage data. Database refers to related data which is in a structured form. In Database, data is organized into tables which consist of rows and columns and it is indexed so data gets updated, expanded and deleted easily. Computer databases typically contain file records data like transactions money in one bank account to another bank account, sales and customer details, fee details of student and product details. There are different kinds of databases, ranging from the most prevalent approach, the relational database, to a distributed database, cloud database or NoSQL database. Relational Database: A relational database is made up of a set of tables with data that fits into a predefined category. Distributed Database: A distributed database is a database in which portions of the database are stored in multiple physical locations, and in which processing is dispersed or replicated among different points in a network. Cloud Database: A cloud database is a database that typically runs on a cloud computing platform. Database service provides access to the database. Database services make the underlying software-stack transparent to the user. These interactions are the example of Traditional database where data is of one type that is textual. In advancement of technology has led to new applications of database systems. New media technology has made it possible to store images, video clips. these important features are making multimedia databases. Nowadays, people are becoming smart before taking any decisions they analyze facts and figure related to it, which comes from these databases. As the database has made easy to manage previous information we are able to catch criminal and we are able to do deep researches. Attention reader! Don’t stop learning now. Get hold of all the important DSA concepts with the DSA Self Paced Course at a student-friendly price and become industry ready. Recommended Posts: How to pre populate database in Android using SQLite Database Difference between Centralized Database and Distributed Database Difference between Database Administrator (DBA) and Database Engineer Multimedia Database Difference between Blockchain and a Database Join algorithms in Database Concept of Time in database Difference between Schema and Database Significance of Database Design MySQL | Database Files Database Recovery Models Difference between Database and DBMS Problems due to database unavailability Introduction of Standby Database Statistical Database Security Different types of Database Users Create database in Cassandra Cassandra (NoSQL) Database SQL queries on FILM Database Advantages of Distributed database ",,,,
DB,"Database Objects in DBMS Last Updated: 20-08-2019 A database object is any defined object in a database that is used to store or reference data.Anything which we make from create command is known as Database Object.It can be used to hold and manipulate the data.Some of the examples of database objects are : view, sequence, indexes, etc. Table – Basic unit of storage; composed rows and columns View – Logically represents subsets of data from one or more tables Sequence – Generates primary key values Index – Improves the performance of some queries Synonym – Alternative name for an object Different database Objects : Table – This database object is used to create a table in database. Syntax : CREATE TABLE [schema.]table (column datatype [DEFAULT expr][, ...]); Example : CREATE TABLE dept (deptno NUMBER(2), dname VARCHAR2(14), loc VARCHAR2(13)); Output : DESCRIBE dept; table output View – This database object is used to create a view in database.A view is a logical table based on a table or another view. A view contains no data of its own but is like a window through which data from tables can be viewed or changed. The tables on which a view is based are called base tables. The view is stored as a SELECT statement in the data dictionary. Syntax : CREATE [OR REPLACE] [FORCE|NOFORCE] VIEW view [(alias[, alias]...)] AS subquery [WITH CHECK OPTION [CONSTRAINT constraint]] [WITH READ ONLY [CONSTRAINT constraint]]; Example : CREATE VIEW salvu50 AS SELECT employee_id ID_NUMBER, last_name NAME, salary*12 ANN_SALARY FROM employees WHERE department_id = 50; Output : SELECT * FROM salvu50; view output Sequence – This database object is used to create a sequence in database.A sequence is a user created database object that can be shared by multiple users to generate unique integers. A typical usage for sequences is to create a primary key value, which must be unique for each row.The sequence is generated and incremented (or decremented) by an internal Oracle routine. Syntax : CREATE SEQUENCE sequence [INCREMENT BY n] [START WITH n] [{MAXVALUE n | NOMAXVALUE}] [{MINVALUE n | NOMINVALUE}] [{CYCLE | NOCYCLE}] [{CACHE n | NOCACHE}]; Example : CREATE SEQUENCE dept_deptid_seq INCREMENT BY 10 START WITH 120 MAXVALUE 9999 NOCACHE NOCYCLE; Check if sequence is created by : SELECT sequence_name, min_value, max_value, increment_by, last_number FROM   user_sequences; Index – This database object is used to create a indexes in database.An Oracle server index is a schema object that can speed up the retrieval of rows by using a pointer.Indexes can be created explicitly or automatically. If you do not have an index on the column, then a full table scan occurs. An index provides direct and fast access to rows in a table. Its purpose is to reduce the necessity of disk I/O by using an indexed path to locate data quickly. The index is used and maintained automatically by the Oracle server. Once an index is created, no direct activity is required by the user.Indexes are logically and physically independent of the table they index. This means that they can be created or dropped at any time and have no effect on the base tables or other indexes. Syntax : CREATE INDEX index ON table (column[, column]...); Example : CREATE INDEX emp_last_name_idx ON  employees(last_name); Synonym – This database object is used to create a indexes in database.It simplify access to objects by creating a synonym(another name for an object). With synonyms, you can Ease referring to a table owned by another user and shorten lengthy object names.To refer to a table owned by another user, you need to prefix the table name with the name of the user who created it followed by a period. Creating a synonym eliminates the need to qualify the object name with the schema and provides you with an alternative name for a table, view, sequence,procedure, or other objects. This method can be especially useful with lengthy object names, such as views. In the syntax: PUBLIC : creates a synonym accessible to all users synonym : is the name of the synonym to be created object : identifies the object for which the synonym is created Syntax : CREATE [PUBLIC] SYNONYM synonym FOR  object; Example : CREATE SYNONYM d_sum FOR dept_sum_vu; ",,,,
DB,"ndexing in Databases | Set 1 Last Updated: 16-09-2019 Indexing is a way to optimize the performance of a database by minimizing the number of disk accesses required when a query is processed. It is a data structure technique which is used to quickly locate and access the data in a database. Indexes are created using a few database columns. The first column is the Search key that contains a copy of the primary key or candidate key of the table. These values are stored in sorted order so that the corresponding data can be accessed quickly. Note: The data may or may not be stored in sorted order. The second column is the Data Reference or Pointer which contains a set of pointers holding the address of the disk block where that particular key value can be found. The indexing has various attributes: Access Types: This refers to the type of access such as value based search, range access, etc. Access Time: It refers to the time needed to find particular data element or set of elements. Insertion Time: It refers to the time taken to find the appropriate space and insert a new data. Deletion Time: Time taken to find an item and delete it as well as update the index structure. Space Overhead: It refers to the additional space required by the index. In general, there are two types of file organization mechanism which are followed by the indexing methods to store the data: Sequential File Organization or Ordered Index File: In this, the indices are based on a sorted ordering of the values. These are generally fast and a more traditional type of storing mechanism. These Ordered or Sequential file organization might store the data in a dense or sparse format: Dense Index: For every search key value in the data file, there is an index record. This record contains the search key and also a reference to the first data record with that search key value. Sparse Index: The index record appears only for a few items in the data file. Each item points to a block as shown. To locate a record, we find the index record with the largest search key value less than or equal to the search key value we are looking for. We start at that record pointed to by the index record, and proceed along with the pointers in the file (that is, sequentially) until we find the desired record. Hash File organization: Indices are based on the values being distributed uniformly across a range of buckets. The buckets to which a value is assigned is determined by a function called a hash function. There are primarily three methods of indexing: Clustered Indexing Non-Clustered or Secondary Indexing Multilevel Indexing Clustered Indexing When more than two records are stored in the same file these types of storing known as cluster indexing. By using the cluster indexing we can reduce the cost of searching reason being multiple records related to the same thing are stored at one place and it also gives the frequent joing of more than two tables(records). Clustering index is defined on an ordered data file. The data file is ordered on a non-key field. In some cases, the index is created on non-primary key columns which may not be unique for each record. In such cases, in order to identify the records faster, we will group two or more columns together to get the unique values and create index out of them. This method is known as the clustering index. Basically, records with similar characteristics are grouped together and indexes are created for these groups. For example, students studying in each semester are grouped together. i.e. 1st Semester students, 2nd semester students, 3rd semester students etc are grouped. cluster_index Clustered index sorted according to first name (Search key) Primary Indexing: This is a type of Clustered Indexing wherein the data is sorted according to the search key and the primary key of the database table is used to create the index. It is a default format of indexing where it induces sequential file organization. As primary keys are unique and are stored in a sorted manner, the performance of the searching operation is quite efficient. Non-clustered or Secondary Indexing A non clustered index just tells us where the data lies, i.e. it gives us a list of virtual pointers or references to the location where the data is actually stored. Data is not physically stored in the order of the index. Instead, data is present in leaf nodes. For eg. the contents page of a book. Each entry gives us the page number or location of the information stored. The actual data here(information on each page of the book) is not organized but we have an ordered reference(contents page) to where the data points actually lie. We can have only dense ordering in the non-clustered index as sparse ordering is not possible because data is not physically organized accordingly. It requires more time as compared to the clustered index because some amount of extra work is done in order to extract the data by further following the pointer. In the case of a clustered index, data is directly present in front of the index. indexing3 Multilevel Indexing With the growth of the size of the database, indices also grow. As the index is stored in the main memory, a single-level index might become too large a size to store with multiple disk accesses. The multilevel indexing segregates the main block into various smaller blocks so that the same can stored in a single block. The outer blocks are divided into inner blocks which in turn are pointed to the data blocks. This can be easily stored in the main memory with fewer overheads. This article is contributed by Avneet Kaur. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above Attention reader! Don’t stop learning now. Get hold of all the important DSA concepts with the DSA Self Paced Course at a student-friendly price and become industry ready. ",,,,
DB,"The Review and Comparison of The Best Free and Licensed Database Management Software Systems: A database is a collection of information that is organized in tables and stored on a computer system. This information can be updated or modified as required. We can also say it’s like a room in an office which has files in it. If we don’t have a defined process we will not know how to get that data from the room. Similarly, a database management system (DBMS) is a software for creating and managing data in the databases. The DBMS provides users and programmers a defined process for data retrieval, management, updating, and creation. Database Management Software Primis Player Placeholder Database Management Software also keeps data guarded and safe. These tools help in reducing data redundancy and maintaining the efficiency of data. Some of them are open-source and some are commercial with specific features. Based on the usage and requirement we can choose a software tool that has needed features and the desired output. => Contact us to suggest a listing here. What You Will Learn: [show] List Of The Top Database Management Software Given below is the list of most popular database management systemsSolarWinds Database Performance Analyzer Oracle RDBMS IBM DB2 Altibase Microsoft SQL Server SAP Sybase ASE Teradata ADABAS MySQL FileMaker Microsoft Access Informix SQLite PostgresSQL AmazonRDS MongoDB Redis CouchDB Neo4j OrientDB Couchbase Toad phpMyAdmin SQL Developer Seqel PRO Robomongo DbVisualizer Hadoop HDFS Cloudera MariaDB Informix Dynamic Server 4D (4th Dimension) The Best Database Management Tools Here we go. The list includes some best free database management software. #1) SolarWinds Database Performance Analyzer SolarWinds_Logo SolarWinds Database Performance Analyzer is the database management software that can perform SQL query performance monitoring, analysis, and tuning. It supports cross-platform database performance tuning and optimization. Few features of SolarWinds are as follows: SolarWinds Database Performance Analyzer has the features of Machine Learning, Cross-Platform Database Support, Expert Tuning Advisors, Cloud Database Support, and Automation Management API, etc. Costs: The price for the software starts at $2107 and it offers a fully functional free trial for 14 days. => Download SolarWinds Database Performance Analyzer For Free #2) Oracle RDBMS Oracle RDBMS Oracle database is the most widely used object-relational database management software. The latest version of this tool is 12c where c means cloud computing. It supports multiple Windows, UNIX, and Linux versions. Few features of Oracle RDBMS are as follows: It is secured, occupies less space, supports large databases, and reduces CPU time to process data. Cost: It’s a commercial tool. Website: Oracle RDBMS #3) IBM DB2 IBM DB2 Latest release 11.1. Developed in the year 1983. The language used is Assembly Language, C, C++ for writing it. It supports multiple Windows, UNIX, and Linux versions. Few features of IBM DB2 are as follows: It is very easy to install and set up and data is easily accessible, we can save the huge amount of data almost up to pet bytes. Costs: It’s a commercial tool. Website: IBM DB2 #4) Altibase Altibase Altibase is an enterprise-grade, high performance, and relational open-source database. Altibase has over 650 enterprise clients including 8 Fortune Global 500 companies and has been deployed over 6,000 mission-critical use cases in various industries. Its main characteristics include: Altibase is a hybrid DBMS. A single database that delivers high-intensity data processing through an in-memory database portion and a large storage capacity through an on-disk database portion. Altibase is among a very small subset of relational DBMSs that currently provide scale-out technology, sharding, etc. Costs: Altibase is an open-source DBMS which includes its sharding => See More Details and Download Altibase Database #5) Microsoft SQL Server Microsoft SQL Server Developed in the year 1989. The latest updated version came in 2016. The language used is Assembly C, Linux, C++ for writing it. Works on Linux and Windows operating systems. Few features of MS SQL server include: Compatible with Oracle provides efficient management of workload and allows multiple users to use the same database. Costs: It’s a commercial tool. Website: Microsoft SQL server #6) SAP Sybase ASE SAP ASE stands for Adaptive Server Enterprise. Its latest version is 15.7. It was started in the middle of the eighties. Few features of ASE are: It can perform millions of transactions in a minute, using cloud computing even mobile devices can be synchronized with the database. Costs: It’s a commercial tool. Website: SAP Sybase ASE #7) Teradata Teradata Started in 1979 Works on Linux and Windows operating systems. Few features of Teradata are: Data import and export is easy, multiple processing is possible at the same time, data can be easily distributed, useful for very large databases. Costs: It’s a commercial tool. Website: Teradata #8) ADABAS ADABAS ADABAS stands for Adaptable Database System. Runs on Windows and Unix, Linux operating systems. Few features of this tool are: Data processing speed is fast, irrespective of the load, the output of any transaction is reliable, its architecture is quite flexible and keeps pace with the changing demands. Costs: It’s a commercial tool. Website: ADABAS #9) MySQL MySQL Latest version 8. The language used is C and C++. Works on Linux and Windows. Few features of this tool are: High-speed data processing, use of triggers increases productivity, with rollback and commit helps in data recovery if required. Costs: It’s a commercial tool. Website: MySQL #10) FileMaker FileMaker The latest stable release is 15.0.3. Works on Mac, Unix, Linux, Windows operating systems. Few features of Filemaker are: It can be connected across platforms like connections to SQL are possible, information sharing is easier because of the cloud. Costs: It’s a commercial tool. Website: Filemaker #11) Microsoft Access microsoft access Latest stable version 16.0.4229.1024. Works on Microsoft Windows. Few features of this tool are: It is an affordable database management system used mostly by e-commerce sites. Costs: It’s a commercial tool. Website: Microsoft Access #12) Informix Informix Latest stable release 12.10.xC7. Coded in assembly, C, C++. Few features of this tool are: Hardware uses less space, data is available at all times and does not need maintenance time. It is developed by IBM. Costs: It’s licensed tool and the cost of each license is affordable. Website: Informix #13) SQLite SQLite It is used as a database system for mobiles. It is coded in C language. It can work on Linux, Windows, and Mac operating systems. Few features of this tool are: It does not need much space hence, it can be used for storing small to medium size websites. It is fast and does not need to set up. Costs: It’s an open-source tool. Website: SQLite #14) PostgreSQL PostgreSQL It is an advanced database. The current version is 9.6.2. It can be used across Linux and Windows operating systems. Few features of this DBMS are: It is an object-relational database. The data remains secure. Data retrieval is faster. Data sharing through dashboards is faster. Costs: It’s an open-source tool. Website: PostgreSQL #15) Amazon RDS Amazon RDS It is also called the Amazon Relational Database Service. Few features of this system are: Setting up and operating is very easy and the database is very secure. The backing up of the database is an inbuilt feature. Recovery of data is also an inbuilt feature managed within. Costs: It’s a commercial tool. Website: Amazon RDS #16) MongoDB MongoDB Few features of MongoDB are: It can process a large amount of data simultaneously and uses internal memory so the data is easily accessible, the use of very complex joins is not supported, scaling is easily possible. Queries can be easily optimized for output. Costs: It’s an open-source tool Website: Mongo DB #17) Redis Redis The latest stable release is 3.2.8. It can work on Windows and Linux operating systems. It is coded in ANSI C language. Few features of Redis are: The database speed is very good, data types like hashes and strings are also supported and the performance of queries is high. Costs: It’s an open-source tool that is BDS licensed. Website: Redis #18) CouchDB CouchDB Latest stable release2.0.0. Written in Erlang Language. Works on Windows and Linux operating systems. Few features of  this tool are: Secure system network, efficient error handling, the output is reliable and fast. Costs: It’s an open-source tool. Website: Couch DB #19) Neo4j Neo4j The latest stable version is 3.1.0. It is coded in Java It can be used on Windows and Linux/Unix operating systems. Few features of this tool are: It has a large capacity server, this database stores data in the form of graphs. It is also called a graph database management system. Costs: It’s an open-source tool. Website: Neo4j #20) OrientDB OrientDB logo The latest stable version is 2.2.17. It is coded in Java language It can be used on Windows and Linux platforms. Few features of this DBMS are: It is a graphical database. It is widely used across big data market and in real-time web-based applications. Costs: It’s an open-source tool. Website: OrientDB #21) Couchbase Couchbase The latest stable version is 4.5 and is coded in C, C++/Eriang. It’s an open-source tool. It can work on Windows and Linux operating systems. Few features of this tool are: The latency and throughput are good for loads of medium size. Data Corruption proof system. Costs: It’s an open-source tool. Website: Couchbase #22) Toad Toad Few features of Toad DBMS are: Easy to use, faster to install, highly efficient output and data can be exported in many formats, lesser time required for its management, it can export a large amount of data in various formats. Costs: It’s a commercial tool. Website: Toad #23) phpMyAdmin phpMyAdmin logo The latest stable release is 4.6.6. It is coded in PHP, Javascript, and XHTML. It can work on Windows and Linux operating systems. Few features of this tool are: The interface is user-friendly, data can be exported in CSV, SQL, XML files and it can be imported from both CSV and SQL file formats. Costs: It’s an open-source tool. Website: phpMyAdmin #24) SQL Developer SQL Developer Logo The latest stable release is 4.1.5.21.78. It is coded in Java. It can work on Windows and Linux operating systems. Few features of this DBMS are: Lesser execution time needed for queries. Queries can be run and generated in many formats such as HTML, PDF, XML, and Excel. Costs: It’s an open-source tool. Website: SQL Developer #25) Sequel PRO Sequel PRO Few features of this tool are: Used for Mac databases. It is easy to use and works with My SQL databases. Connectivity is easy and flexible. Installation is easy and quick. It makes work smooth for web applications using it and the output is fast. Costs: It’s an open-source tool. Website: Sequel PRO #26) Robomongo Robomongo It can be used across Windows and Linux platforms. Free and open-source tool. Few features of Robomongo are: The tool is robust and can be used for a large quantity of load. Error handling is better, more stable as a tool and has many upcoming features. Costs: It’s an open-source tool. Website: Robomongo #27) DbVisualizer DbVisualizer Few features of this tool are: The user interface is friendly, set up and installation is easy. Has the facility to export data in CSV format. User can scroll down to see the output if it has a large number of rows retrieved. Costs: It’s a commercial tool. Website: DBVisualizer #28) Hadoop HDFS Hadoop HDFS logo Few features of Hadoop HDFS are: It provides large data storage and uses many machines for storing data, therefore, the data is easy to access. Data loss is prevented by storing data redundantly. Data authentication is also available. Parallel processing of data is possible. Costs: It’s a commercial tool. Website: Hadoop HDFS #29) Cloudera Cloudera Logo Few features of Cloudera are: High-speed data processing makes it an attractive option for large enterprises. Greater efficiency for a large amount of data provides a high level of security, this tool improves performance. Costs: It’s an open-source tool. Website: Cloudera #30) MariaDB MariaDB Works on Mac/Unix/Linux/Windows operating systems Few features of this tool are: It has higher uptime or availability and is highly scalable, has multicore support, uses multiple threads, supports Internet Protocol. It provides real-time database access. Costs: It’s an open-source tool. Website: MariaDB #31) Informix Dynamic Servers Informix Dynamic Servers Works on Mac/UnixLinuxx/Windows operating systems. Few features of this DBMS are: It is highly available and scalable, has multicore support, uses multiple threads, supports Internet Protocol. It provides parallel processing of data. Costs: It’s a commercial tool. Website: Informix Dynamic Server #32) 4D (4th Dimension) 4D(4th Dimension) Works on Windows and Mac operating systems. Few features of 4D are: It has the facility to import and export data. There is a script debugger, it supports XML format, it has drag and drop facility. Costs: It’s a commercial tool. Website: 4D (4th Dimension) Conclusion In a nutshell, we can say all the above-mentioned database management systems have their advantages and disadvantages, some might be useful whereas others might not be that suitable per your requirements. Today’s time is the time of data where an enormous amount of data has to be stored, updated, and created daily. The demand for Database Management Tools is growing exponentially and the competition is also high. With each tool trying to be better in terms of features compared to the others, you can select a DBMS per your requirement from the above list. ",,,,
ALG,"Data structure In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification.[1][2][3] More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data Usage Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.[5] Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval,[6] while compiler implementations usually use hash tables to look up identifiers.[7] Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.[8] Implementation Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).[9] Examples Main article: List of data structures There are numerous types of data structures, generally built upon simpler primitive data types:[10] An array is a number of elements in a specific order, typically all of the same type (depending on the language, individual elements may either all be forced to be the same type, or may be of almost any type). Elements are accessed using an integer index to specify which element is required. Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or resizable. A linked list (also just called list) is a linear collection of data elements of any type, called nodes, where each node has itself a value, and points to the next node in the linked list. The principal advantage of a linked list over an array is that values can always be efficiently inserted and removed without relocating the rest of the list. Certain other operations, such as random access to a certain element, are however slower on lists than on arrays. A record (also called tuple or struct) is an aggregate data structure. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members. A union is a data structure that specifies which of a number of permitted primitive types may be stored in its instances, e.g. float or long integer. Contrast with a record, which could be defined to contain a float and an integer; whereas in a union, there is only one value at a time. Enough space is allocated to contain the widest member datatype. A tagged union (also called variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type, for enhanced type safety. An object is a data structure that contains data fields, like a record does, as well as various methods which operate on the data contents. An object is an in-memory instance of a class from a taxonomy. In the context of object-oriented programming, records are known as plain old data structures to distinguish them from objects.[11] In addition, graphs and binary trees are other commonly used data structures. Language support Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.[12][13] Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework. Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose. Many known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.[1 ",,,,
ALG,"Algorithms Topics : Analysis of Algorithms Searching and Sorting Greedy Algorithms Dynamic Programming Pattern Searching Other String Algorithms Backtracking Divide and Conquer Geometric Algorithms Mathematical Algorithms Bit Algorithms Graph Algorithms Randomized Algorithms Branch and Bound Quizzes on Algorithms Misc Analysis of Algorithms: Asymptotic Analysis Worst, Average and Best Cases Asymptotic Notations Little o and little omega notations Lower and Upper Bound Theory Analysis of Loops Solving Recurrences Amortized Analysis What does ‘Space Complexity’ mean? Pseudo-polynomial Algorithms NP-Completeness Introduction Polynomial Time Approximation Scheme A Time Complexity Question Time Complexity of building a heap Time Complexity where loop variable is incremented by 1, 2, 3, 4 .. Time Complexity of Loop with Powers Performance of loops (A caching question) Recent Articles on Analysis of Algorithms Quiz on Analysis of Algorithms Quiz on Recurrences Searching and Sorting: Linear Search, Binary Search, Jump Search, Interpolation Search, Exponential Search, Ternary Search Selection Sort, Bubble Sort, Insertion Sort, Merge Sort, Heap Sort, QuickSort, Radix Sort, Counting Sort, Bucket Sort, ShellSort, Comb Sort, Pigeonhole Sort, Cycle Sort Interpolation search vs Binary search Stability in sorting algorithms When does the worst case of Quicksort occur? Lower bound for comparison based sorting algorithms Which sorting algorithm makes minimum number of memory writes? Find the Minimum length Unsorted Subarray, sorting which makes the complete array sorted Merge Sort for Linked Lists Sort a nearly sorted (or K sorted) array Iterative Quick Sort QuickSort on Singly Linked List QuickSort on Doubly Linked List Find k closest elements to a given value Sort n numbers in range from 0 to n^2 – 1 in linear time A Problem in Many Binary Search Implementations Search in an almost sorted array Sort an array in wave form Why is Binary Search preferred over Ternary Search? K’th Smallest/Largest Element in Unsorted Array K’th Smallest/Largest Element in Unsorted Array in Expected Linear Time K’th Smallest/Largest Element in Unsorted Array in Worst Case Linear Time Find the closest pair from two sorted arrays Find common elements in three sorted arrays Given a sorted array and a number x, find the pair in array whose sum is closest to x Count 1’s in a sorted binary array Binary Insertion Sort Insertion Sort for Singly Linked List Why Quick Sort preferred for Arrays and Merge Sort for Linked Lists? Merge Sort for Doubly Linked List Minimum adjacent swaps to move maximum and minimum to corners Recent Articles on Searching Recent Articles on Sorting Quiz on Searching Quiz on Sorting Coding Practice on Searching Coding Practice on Sorting Greedy Algorithms: Activity Selection Problem Kruskal’s Minimum Spanning Tree Algorithm Huffman Coding Efficient Huffman Coding for Sorted Input Prim’s Minimum Spanning Tree Algorithm Prim’s MST for Adjacency List Representation Dijkstra’s Shortest Path Algorithm Dijkstra’s Algorithm for Adjacency List Representation Job Sequencing Problem Quiz on Greedy Algorithms Greedy Algorithm to find Minimum number of Coins K Centers Problem Minimum Number of Platforms Required for a Railway/Bus Station Recent Articles on Greedy Algorithms Quiz on Greedy Algorithms Coding Practice on Greedy Algorithms Dynamic Programming: Overlapping Subproblems Property Optimal Substructure Property Longest Increasing Subsequence Longest Common Subsequence Edit Distance Min Cost Path Coin Change Matrix Chain Multiplication Binomial Coefficient 0-1 Knapsack Problem Egg Dropping Puzzle Longest Palindromic Subsequence Cutting a Rod Maximum Sum Increasing Subsequence Longest Bitonic Subsequence Floyd Warshall Algorithm Palindrome Partitioning Partition problem Word Wrap Problem Maximum Length Chain of Pairs Variations of LIS Box Stacking Problem Program for Fibonacci numbers Minimum number of jumps to reach end Maximum size square sub-matrix with all 1s Ugly Numbers Largest Sum Contiguous Subarray Longest Palindromic Substring Bellman–Ford Algorithm for Shortest Paths Optimal Binary Search Tree Largest Independent Set Problem Subset Sum Problem Maximum sum rectangle in a 2D matrix Count number of binary strings without consecutive 1?s Boolean Parenthesization Problem Count ways to reach the n’th stair Minimum Cost Polygon Triangulation Mobile Numeric Keypad Problem Count of n digit numbers whose sum of digits equals to given sum Minimum Initial Points to Reach Destination Total number of non-decreasing numbers with n digits Find length of the longest consecutive path from a given starting character Tiling Problem Minimum number of squares whose sum equals to given number n Find minimum number of coins that make a given value Collect maximum points in a grid using two traversals Shortest Common Supersequence Compute sum of digits in all numbers from 1 to n Count possible ways to construct buildings Maximum profit by buying and selling a share at most twice How to print maximum number of A’s using given four keys Find the minimum cost to reach destination using a train Vertex Cover Problem | Set 2 (Dynamic Programming Solution for Tree) Count number of ways to reach a given score in a game Weighted Job Scheduling Longest Even Length Substring such that Sum of First and Second Half is same Recent Articles on Dynamic Programming Quiz on Dynamic Programming Coding Practice on Dynamic Programing Pattern Searching: Naive Pattern Searching KMP Algorithm Rabin-Karp Algorithm A Naive Pattern Searching Question Finite Automata Efficient Construction of Finite Automata Boyer Moore Algorithm – Bad Character Heuristic Suffix Array Anagram Substring Search (Or Search for all permutations) Pattern Searching using a Trie of all Suffixes Aho-Corasick Algorithm for Pattern Searching kasai’s Algorithm for Construction of LCP array from Suffix Array Z algorithm (Linear time pattern searching Algorithm) Program to wish Women’s Day Recent Articles on Pattern Searching Other String Algorithms: Manacher’s Algorithm – Linear Time Longest Palindromic Substring – Part 1, Part 2, Part 3, Part 4 Longest Even Length Substring such that Sum of First and Second Half is same Print all possible strings that can be made by placing spaces Recent Articles on Strings Coding practice on Strings Backtracking: Print all permutations of a given string The Knight’s tour problem Rat in a Maze N Queen Problem Subset Sum m Coloring Problem Hamiltonian Cycle Sudoku Tug of War Solving Cryptarithmetic Puzzles Recent Articles on Backtracking Coding Practice on Backtracking Divide and Conquer: Introduction Write your own pow(x, n) to calculate x*n Median of two sorted arrays Count Inversions Closest Pair of Points Strassen’s Matrix Multiplication Quick Sort vs Merge Sort Recent Articles on Divide and Conquer Quiz on Divide and Conquer Coding practice on Divide and Conquer Geometric Algorithms: Closest Pair of Points | O(nlogn) Implementation How to check if two given line segments intersect? How to check if a given point lies inside or outside a polygon? Convex Hull | Set 1 (Jarvis’s Algorithm or Wrapping) Convex Hull | Set 2 (Graham Scan) Given n line segments, find if any two segments intersect Check whether a given point lies inside a triangle or not How to check if given four points form a square Recent Articles on Geometric Algorithms Coding Practice on Geometric Algorithms Mathematical Algorithms: Write an Efficient Method to Check if a Number is Multiple of 3 Efficient way to multiply with 7 Write a C program to print all permutations of a given string Lucky Numbers Write a program to add two numbers in base 14 Babylonian method for square root Multiply two integers without using multiplication, division and bitwise operators, and no loops Print all combinations of points that can compose a given number Write you own Power without using multiplication(*) and division(/) operators Program for Fibonacci numbers Average of a stream of numbers Count numbers that don’t contain 3 MagicSquare Sieve of Eratosthenes Number which has the maximum number of distinct prime factors in the range M to N Find day of the week for a given date DFA based division Generate integer from 1 to 7 with equal probability Given a number, find the next smallest palindrome Make a fair coin from a biased coin Check divisibility by 7 Find the largest multiple of 3 Lexicographic rank of a string Print all permutations in sorted (lexicographic) order Shuffle a given array Space and time efficient Binomial Coefficient Reservoir Sampling Pascal’s Triangle Select a random number from stream, with O(1) space Find the largest multiple of 2, 3 and 5 Efficient program to calculate e^x Measure one litre using two vessels and infinite water supply Efficient program to print all prime factors of a given number Print all possible combinations of r elements in a given array of size n Random number generator in arbitrary probability distribution fashion How to check if a given number is Fibonacci number? Russian Peasant Multiplication Count all possible groups of size 2 or 3 that have sum as multiple of 3 Tower of Hanoi Horner’s Method for Polynomial Evaluation Count trailing zeroes in factorial of a number Program for nth Catalan Number Generate one of 3 numbers according to given probabilities Find Excel column name from a given column number Find next greater number with same set of digits Count Possible Decodings of a given Digit Sequence Calculate the angle between hour hand and minute hand Count number of binary strings without consecutive 1?s Find the smallest number whose digits multiply to a given number n Draw a circle without floating point arithmetic How to check if an instance of 8 puzzle is solvable? Birthday Paradox Multiply two polynomials Count Distinct Non-Negative Integer Pairs (x, y) that Satisfy the Inequality x*x + y*y < n Count ways to reach the n’th stair Replace all ‘0’ with ‘5’ in an input Integer Program to add two polynomials Print first k digits of 1/n where n is a positive integer Given a number as a string, find the number of contiguous subsequences which recursively add up to 9 Program for Bisection Method Program for Method Of False Position Program for Newton Raphson Method Recent Articles on Mathematical Algorithms Coding Practice on Mathematical Algorithms Bit Algorithms: Find the element that appears once Detect opposite signs Set bits in all numbers from 1 to n Swap bits Add two numbers Smallest of three A Boolean Array Puzzle Set bits in an (big) array Next higher number with same number of set bits Optimization Technique (Modulus) Add 1 to a number Multiply with 3.5 Turn off the rightmost set bit Check for Power of 4 Absolute value (abs) without branching Modulus division by a power-of-2-number Minimum or Maximum of two integers Rotate bits Find the two non-repeating elements in an array Number Occurring Odd Number of Times Check for Integer Overflow Little and Big Endian Reverse Bits of a Number Count set bits in an integer Number of bits to be flipped to convert A to B Next Power of 2 Check if a Number is Multiple of 3 Find parity Multiply with 7 Find whether a no is power of two Position of rightmost set bit Binary representation of a given number Swap all odd and even bits Find position of the only set bit Karatsuba algorithm for fast multiplication How to swap two numbers without using a temporary variable? Check if a number is multiple of 9 using bitwise operators Swap two nibbles in a byte How to turn off a particular bit in a number? Check if binary representation of a number is palindrome Recent Articles on Bit Algorithms Quiz on Bit Algorithms Coding Practice on Bit Algorithms Graph Algorithms: Introduction, DFS and BFS: Graph and its representations Breadth First Traversal for a Graph Depth First Traversal for a Graph Applications of Depth First Search Detect Cycle in a Directed Graph Detect Cycle in a an Undirected Graph Detect cycle in an undirected graph Longest Path in a Directed Acyclic Graph Topological Sorting Check whether a given graph is Bipartite or not Snake and Ladder Problem Biconnected Components Check if a given graph is tree or not Minimum Spanning Tree: Prim’s Minimum Spanning Tree (MST)) Applications of Minimum Spanning Tree Problem Prim’s MST for Adjacency List Representation Kruskal’s Minimum Spanning Tree Algorithm Boruvka’s algorithm for Minimum Spanning Tree Shortest Paths: Dijkstra’s shortest path algorithm Dijkstra’s Algorithm for Adjacency List Representation Bellman–Ford Algorithm Floyd Warshall Algorithm Johnson’s algorithm for All-pairs shortest paths Shortest Path in Directed Acyclic Graph Some interesting shortest path questions Shortest path with exactly k edges in a directed and weighted graph Connectivity: Find if there is a path between two vertices in a directed graph Connectivity in a directed graph Articulation Points (or Cut Vertices) in a Graph Biconnected graph Bridges in a graph Eulerian path and circuit Fleury’s Algorithm for printing Eulerian Path or Circuit Strongly Connected Components Transitive closure of a graph Find the number of islands Count all possible walks from a source to a destination with exactly k edges Euler Circuit in a Directed Graph Biconnected Components Tarjan’s Algorithm to find Strongly Connected Components Hard Problems: Graph Coloring (Introduction and Applications) Greedy Algorithm for Graph Coloring Travelling Salesman Problem (Naive and Dynamic Programming) Travelling Salesman Problem (Approximate using MST) Hamiltonian Cycle Vertex Cover Problem (Introduction and Approximate Algorithm) K Centers Problem (Greedy Approximate Algorithm) Maximum Flow: Ford-Fulkerson Algorithm for Maximum Flow Problem Find maximum number of edge disjoint paths between two vertices Find minimum s-t cut in a flow network Maximum Bipartite Matching Channel Assignment Problem Misc: Find if the strings can be chained to form a circle Given a sorted dictionary of an alien language, find order of characters Karger’s algorithm for Minimum Cut Karger’s algorithm for Minimum Cut | Set 2 (Analysis and Applications) Hopcroft–Karp Algorithm for Maximum Matching | Set 1 (Introduction) Hopcroft–Karp Algorithm for Maximum Matching | Set 2 (Implementation) Length of shortest chain to reach a target word Find same contacts in a list of contacts All Algorithms on Graph Quiz on Graph Quiz on Graph Traversals Quiz on Graph Shortest Paths Quiz on Graph Minimum Spanning Tree Coding Practice on Graph Randomized Algorithms: Linearity of Expectation Expected Number of Trials until Success Randomized Algorithms | Set 0 (Mathematical Background) Randomized Algorithms | Set 1 (Introduction and Analysis) Randomized Algorithms | Set 2 (Classification and Applications) Randomized Algorithms | Set 3 (1/2 Approximate Median) Karger’s algorithm for Minimum Cut K’th Smallest/Largest Element in Unsorted Array | Set 2 (Expected Linear Time) Reservoir Sampling Shuffle a given array Select a Random Node from a Singly Linked List Recent Articles on Randomized Algorithms Branch and Bound: Branch and Bound | Set 1 (Introduction with 0/1 Knapsack) Branch and Bound | Set 2 (Implementation of 0/1 Knapsack) Branch and Bound | Set 3 (8 puzzle Problem) Branch And Bound | Set 4 (Job Assignment Problem) Branch and Bound | Set 5 (N Queen Problem) Branch And Bound | Set 6 (Traveling Salesman Problem) Recent Articles on Branch and Bound Quizzes on Algorithms: Analysis of Algorithms Sorting Divide and Conquer Greedy Algorithms Dynamic Programming Backtracking Misc NP Complete Searching Analysis of Algorithms (Recurrences) Recursion Bit Algorithms Graph Traversals Graph Shortest Paths Graph Minimum Spanning Tree Misc: Commonly Asked Algorithm Interview Questions | Set 1 Given a matrix of ‘O’ and ‘X’, find the largest subsquare surrounded by ‘X’ Nuts & Bolts Problem (Lock & Key problem) Flood fill Algorithm – how to implement fill() in paint? Given n appointments, find all conflicting appointments Check a given sentence for a given set of simple grammer rules Find Index of 0 to be replaced with 1 to get longest continuous sequence of 1s in a binary array How to check if two given sets are disjoint? Minimum Number of Platforms Required for a Railway/Bus Station Length of the largest subarray with contiguous elements | Set 1 Length of the largest subarray with contiguous elements | Set 2 Print all increasing sequences of length k from first n natural numbers Given two strings, find if first string is a subsequence of second Snake and Ladder Problem Write a function that returns 2 for input 1 and returns 1 for 2 Connect n ropes with minimum cost Find the number of valid parentheses expressions of given length Longest Monotonically Increasing Subsequence Size (N log N): Simple implementation Generate all binary permutations such that there are more 1’s than 0’s at every point in all permutations Lexicographically minimum string rotation Construct an array from its pair-sum array Program to evaluate simple expressions Check if characters of a given string can be rearranged to form a palindrome Print all pairs of anagrams in a given array of strings Please see Data Structures and Advanced Data Structures for Graph, Binary Tree, BST and Linked List based algorithms. We will be adding more categories and posts to this page soon. You can create a new Algorithm topic and discuss it with other geeks using our portal PRACTICE. See recently added problems on Algorithms on PRACTICE. ",,,,
ALG,"Algorithm In mathematics and computer science, an algorithm (/??l??r???m/ (About this soundlisten)) is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation.[1][2] Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. As an effective method, an algorithm can be expressed within a finite amount of space and time,[3] and in a well-defined formal language[4] for calculating a function.[5] Starting from an initial state and initial input (perhaps empty),[6] the instructions describe a computation that, when executed, proceeds through a finite[7] number of well-defined successive states, eventually producing ""output""[8] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[9] The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.[10] Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers,[11] and the Euclidean algorithm for finding the greatest common divisor of two numbers.[12] Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.[13] The word algorithm itself is derived from the 9th-century mathematician Mu?ammad ibn M?s? al-Khw?rizm?, Latinized Algoritmi.[14] A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability""[15] or ""effective method"".[16] Those formalizations included the G?del–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939 Etymology The word 'algorithm' has its roots in latinizing the name of mathematician Muhammad ibn Musa al-Khwarizmi to algorismus.[17][18] Al-Khw?rizm? (Arabic: ÇáÎæÇÑÒã?ý, c. 780–850) was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad,[11] whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan.[19][20] About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century. The manuscript starts with the phrase Dixit Algorizmi ('Thus spake Al-Khwarizmi'), where ""Algorizmi"" was the translator's Latinization of Al-Khwarizmi's name.[21] Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra.[22] In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the ""decimal number system"".[23] In the 15th century, under the influence of the Greek word ??????? (arithmos), 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.[24] In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that ""algorithm"" took on the meaning that it has in modern English.[25] Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with: Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris. which translates to: Algorism is the art by which at present we use those Indian figures, which number two times five. The poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.[26] Informal definition For a detailed presentation of the various points of view on the definition of ""algorithm"", see Algorithm characterizations. An informal definition could be ""a set of rules that precisely defines a sequence of operations"",[27][need quotation to verify] which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure[28] or cook-book recipe.[29] In general, a program is only an algorithm if it stops eventually[30] - even though infinite loops may sometimes prove desirable. A prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section. Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word ""algorithm"" in the following quotation: No human being can write fast enough, or long enough, or small enough† ( †""smaller and smaller without limit … you'd be trying to write on molecules, on atoms, on electrons"") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.[31] An ""enumerably infinite set"" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that ""creates"" output integers from an arbitrary ""input"" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary ""input variables"" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example): Precise instructions (in language understood by ""the computer"")[32] for a fast, efficient, ""good""[33] process that specifies the ""moves"" of ""the computer"" (machine or human, equipped with the necessary internally contained information and capabilities)[34] to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = … and ""effectively""[35] produce, in a ""reasonable"" time,[36] output-integer y at a specified place and in a specified format. The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term. Formalization Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000): Minsky: ""But we will also maintain, with Turing … that any procedure which could ""naturally"" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments … in its favor are hard to refute"".[37] Gurevich: “… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine"".[38] Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem. Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures. For some of these computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable). Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting ""from the top"" and going ""down to the bottom""—an idea that is described more formally by flow of control. So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, ""mechanical"" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of ""memory"" as a scratchpad. An example of such an assignment can be found below. For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming. Expressing algorithms Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms. There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called ""sets of quadruples"" (see Turing machine for more). Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:[39] 1 High-level description “…prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head."" 2 Implementation description “…prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function."" 3 Formal description Most detailed, ""lowest level"", gives the Turing machine's ""state table"". For an example of the simple algorithm ""Add m+n"" described in all three levels, see Algorithm#Examples. Design It has been suggested that this article be split into a new article titled Algorithm design. (Discuss) (March 2020) See also: Algorithm § By design paradigm Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[40] with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design lies in the creation of algorithm that has an efficient run-time, also known as its Big O. Typical steps in the development of algorithms: Problem definition Development of a model Specification of the algorithm Designing an algorithm Checking the correctness of the algorithm Analysis of algorithm Implementation of algorithm Program testing Documentation preparation Implementation Logical NAND algorithm implemented electronically in 7400 chip Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device. Computer algorithms Flowchart examples of the canonical B?hm-Jacopini structures: the SEQUENCE (rectangles descending the page), the WHILE-DO and the IF-THEN-ELSE. The three structures are made of the primitive conditional GOTO ({{{1}}}) (a diamond), the unconditional GOTO (rectangle), various assignment operators (rectangle), and HALT (rectangle). Nesting of these structures inside assignment-blocks result in complex diagrams (cf Tausworthe 1977:100, 114). In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended ""target"" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology. ""Elegant"" (compact) programs, ""good"" (fast) programs : The notion of ""simplicity and elegance"" appears informally in Knuth and precisely in Chaitin: Knuth: "" … we want good algorithms in some loosely defined aesthetic sense. One criterion … is the length of time taken to perform the algorithm …. Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc""[41] Chaitin: "" … a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does""[42] Chaitin prefaces his definition with: ""I'll show you can't prove that a program is 'elegant'""—such a proof would solve the Halting problem (ibid). Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that ""It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms"".[43] Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below. Computers (and computors), models of computation: A computer (or human ""computor""[44]) is a restricted type of machine, a ""discrete deterministic mechanical device""[45] that blindly follows its instructions.[46] Melzak's and Lambek's primitive models[47] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[48] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[49] Minsky describes a more congenial variation of Lambek's ""abacus"" model in his ""Very Simple Bases for Computability"".[50] Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[51] operations: ZERO (e.g. the contents of location replaced by 0: L ? 0), SUCCESSOR (e.g. L ? L+1), and DECREMENT (e.g. L ? L ? 1).[52] Rarely must a programmer write ""code"" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction "" Z ? 0 ""; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional. Simulation of an algorithm: computer (computor) language: Knuth advises the reader that ""the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example"".[53] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[54] This means that the programmer must know a ""language"" that is effective relative to the target computing agent (computer/computor). But what model should be used for the simulation? Van Emde Boas observes ""even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters"".[55] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a ""modulus"" instruction available rather than just subtraction (or worse: just Minsky's ""decrement""). Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while ""undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in ""spaghetti code"", a programmer can write structured programs using only these instructions; on the other hand ""it is also possible, and not too hard, to write badly structured programs in a structured language"".[56] Tausworthe augments the three B?hm-Jacopini canonical structures:[57] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[58] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[59] Canonical flowchart symbols[60]: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The B?hm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can ""nest"" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram. ",,,,
ALG,"Data Structures are the programmatic way of storing data so that data can be used efficiently. Almost every enterprise application uses various types of data structures in one or the other way. This tutorial will give you a great understanding on Data Structures needed to understand the complexity of enterprise level applications and need of algorithms, and data structures. Why to Learn Data Structure and Algorithms? As applications are getting complex and data rich, there are three common problems that applications face now-a-days. Data Search ? Consider an inventory of 1 million(106) items of a store. If the application is to search an item, it has to search an item in 1 million(106) items every time slowing down the search. As data grows, search will become slower. Processor speed ? Processor speed although being very high, falls limited if the data grows to billion records. Multiple requests ? As thousands of users can search data simultaneously on a web server, even the fast server fails while searching the data. To solve the above-mentioned problems, data structures come to rescue. Data can be organized in a data structure in such a way that all items may not be required to be searched, and the required data can be searched almost instantly. Applications of Data Structure and Algorithms Algorithm is a step-by-step procedure, which defines a set of instructions to be executed in a certain order to get the desired output. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language. From the data structure point of view, following are some important categories of algorithms ? Search ? Algorithm to search an item in a data structure. Sort ? Algorithm to sort items in a certain order. Insert ? Algorithm to insert item in a data structure. Update ? Algorithm to update an existing item in a data structure. Delete ? Algorithm to delete an existing item from a data structure. The following computer problems can be solved using Data Structures ? Fibonacci number series Knapsack problem Tower of Hanoi All pair shortest path by Floyd-Warshall Shortest path by Dijkstra Project scheduling Audience This tutorial is designed for Computer Science graduates as well as Software Professionals who are willing to learn data structures and algorithm programming in simple and easy steps. After completing this tutorial you will be at intermediate level of expertise from where you can take yourself to higher level of expertise. Prerequisites Before proceeding with this tutorial, you should have a basic understanding of C programming language, text editor, and execution of programs, etc. ",,,,
ALG,"Data Structure is a systematic way to organize data in order to use it efficiently. Following terms are the foundation terms of a data structure. Interface ? Each data structure has an interface. Interface represents the set of operations that a data structure supports. An interface only provides the list of supported operations, type of parameters they can accept and return type of these operations. Implementation ? Implementation provides the internal representation of a data structure. Implementation also provides the definition of the algorithms used in the operations of the data structure. Characteristics of a Data Structure Correctness ? Data structure implementation should implement its interface correctly. Time Complexity ? Running time or the execution time of operations of data structure must be as small as possible. Space Complexity ? Memory usage of a data structure operation should be as little as possible. Need for Data Structure As applications are getting complex and data rich, there are three common problems that applications face now-a-days. Data Search ? Consider an inventory of 1 million(106) items of a store. If the application is to search an item, it has to search an item in 1 million(106) items every time slowing down the search. As data grows, search will become slower. Processor speed ? Processor speed although being very high, falls limited if the data grows to billion records. Multiple requests ? As thousands of users can search data simultaneously on a web server, even the fast server fails while searching the data. To solve the above-mentioned problems, data structures come to rescue. Data can be organized in a data structure in such a way that all items may not be required to be searched, and the required data can be searched almost instantly. Execution Time Cases There are three cases which are usually used to compare various data structure's execution time in a relative manner. Worst Case ? This is the scenario where a particular data structure operation takes maximum time it can take. If an operation's worst case time is ƒ(n) then this operation will not take more than ƒ(n) time where ƒ(n) represents function of n. Average Case ? This is the scenario depicting the average execution time of an operation of a data structure. If an operation takes ƒ(n) time in execution, then m operations will take mƒ(n) time. Best Case ? This is the scenario depicting the least possible execution time of an operation of a data structure. If an operation takes ƒ(n) time in execution, then the actual operation may take time as the random number which would be maximum as ƒ(n). Basic Terminology Data ? Data are values or set of values. Data Item ? Data item refers to single unit of values. Group Items ? Data items that are divided into sub items are called as Group Items. Elementary Items ? Data items that cannot be divided are called as Elementary Items. Attribute and Entity ? An entity is that which contains certain attributes or properties, which may be assigned values. Entity Set ? Entities of similar attributes form an entity set. Field ? Field is a single elementary unit of information representing an attribute of an entity. Record ? Record is a collection of field values of a given entity. File ? File is a collection of records of the entities in a given entity set. ",,,,
ALG,"Algorithm is a step-by-step procedure, which defines a set of instructions to be executed in a certain order to get the desired output. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language. From the data structure point of view, following are some important categories of algorithms ? Search ? Algorithm to search an item in a data structure. Sort ? Algorithm to sort items in a certain order. Insert ? Algorithm to insert item in a data structure. Update ? Algorithm to update an existing item in a data structure. Delete ? Algorithm to delete an existing item from a data structure. Characteristics of an Algorithm Not all procedures can be called an algorithm. An algorithm should have the following characteristics ? Unambiguous ? Algorithm should be clear and unambiguous. Each of its steps (or phases), and their inputs/outputs should be clear and must lead to only one meaning. Input ? An algorithm should have 0 or more well-defined inputs. Output ? An algorithm should have 1 or more well-defined outputs, and should match the desired output. Finiteness ? Algorithms must terminate after a finite number of steps. Feasibility ? Should be feasible with the available resources. Independent ? An algorithm should have step-by-step directions, which should be independent of any programming code. How to Write an Algorithm? There are no well-defined standards for writing algorithms. Rather, it is problem and resource dependent. Algorithms are never written to support a particular programming code. As we know that all programming languages share basic code constructs like loops (do, for, while), flow-control (if-else), etc. These common constructs can be used to write an algorithm. We write algorithms in a step-by-step manner, but it is not always the case. Algorithm writing is a process and is executed after the problem domain is well-defined. That is, we should know the problem domain, for which we are designing a solution. Algorithm Analysis Efficiency of an algorithm can be analyzed at two different stages, before implementation and after implementation. They are the following ? A Priori Analysis ? This is a theoretical analysis of an algorithm. Efficiency of an algorithm is measured by assuming that all other factors, for example, processor speed, are constant and have no effect on the implementation. A Posterior Analysis ? This is an empirical analysis of an algorithm. The selected algorithm is implemented using programming language. This is then executed on target computer machine. In this analysis, actual statistics like running time and space required, are collected. We shall learn about a priori algorithm analysis. Algorithm analysis deals with the execution or running time of various operations involved. The running time of an operation can be defined as the number of computer instructions executed per operation. Algorithm Complexity Suppose X is an algorithm and n is the size of input data, the time and space used by the algorithm X are the two main factors, which decide the efficiency of X. Time Factor ? Time is measured by counting the number of key operations such as comparisons in the sorting algorithm. Space Factor ? Space is measured by counting the maximum memory space required by the algorithm. The complexity of an algorithm f(n) gives the running time and/or the storage space required by the algorithm in terms of n as the size of input data. Space Complexity Space complexity of an algorithm represents the amount of memory space required by the algorithm in its life cycle. The space required by an algorithm is equal to the sum of the following two components ? A fixed part that is a space required to store certain data and variables, that are independent of the size of the problem. For example, simple variables and constants used, program size, etc. A variable part is a space required by variables, whose size depends on the size of the problem. For example, dynamic memory allocation, recursion stack space, etc. Time Complexity Time complexity of an algorithm represents the amount of time required by the algorithm to run to completion. Time requirements can be defined as a numerical function T(n), where T(n) can be measured as the number of steps, provided each step consumes constant time. For example, addition of two n-bit integers takes n steps. Consequently, the total computational time is T(n) = c ? n, where c is the time taken for the addition of two bits. Here, we observe that T(n) grows linearly as the input size increases. ",,,,
ALG,"Data Definition Data Definition defines a particular data with the following characteristics. Atomic ? Definition should define a single concept. Traceable ? Definition should be able to be mapped to some data element. Accurate ? Definition should be unambiguous. Clear and Concise ? Definition should be understandable. Data Object Data Object represents an object having a data. Data Type Data type is a way to classify various types of data such as integer, string, etc. which determines the values that can be used with the corresponding type of data, the type of operations that can be performed on the corresponding type of data. There are two data types ? Built-in Data Type Derived Data Type Built-in Data Type Those data types for which a language has built-in support are known as Built-in Data types. For example, most of the languages provide the following built-in data types. Integers Boolean (true, false) Floating (Decimal numbers) Character and Strings Derived Data Type Those data types which are implementation independent as they can be implemented in one or the other way are known as derived data types. These data types are normally built by the combination of primary or built-in data types and associated operations on them. For example ? List Array Stack Queue Basic Operations The data in the data structures are processed by certain operations. The particular data structure chosen largely depends on the frequency of the operation that needs to be performed on the data structure. Traversing Searching Insertion Deletion Sorting Merging ",,,,
ALG,"A data structure is a particular way of organizing data in a computer so that it can be used effectively. For example, we can store a list of items having the same data-type using the array data structure. Click to enlarge Array Data Structure This page contains detailed tutorials on different data structures (DS) with topic-wise problems. Topics: Array Linked List Stack Queue Binary Tree Binary Search Tree Heap Hashing Graph Matrix Misc Advanced Data Structure Overview: Overview of Data Structures | Set 1 (Linear Data Structures) Overview of Data Structures | Set 2 (Binary Tree, BST, Heap and Hash) Overview of Data Structures | Set 3 (Graph, Trie, Segment Tree and Suffix Tree) Abstract Data Types Linked List: Singly Linked List: Introduction to Linked List Linked List vs Array Linked List Insertion Linked List Deletion (Deleting a given key) Linked List Deletion (Deleting a key at given position) A Programmer’s approach of looking at Array vs. Linked List Find Length of a Linked List (Iterative and Recursive) How to write C functions that modify head pointer of a Linked List? Swap nodes in a linked list without swapping data Reverse a linked list Merge two sorted linked lists Merge Sort for Linked Lists Reverse a Linked List in groups of given size Detect and Remove Loop in a Linked List Add two numbers represented by linked lists | Set 1 Rotate a Linked List Generic Linked List in C Circular Linked List: Circular Linked List Introduction and Applications, Circular Singly Linked List Insertion< Circular Linked List Traversal Split a Circular Linked List into two halves Sorted insert for circular linked list Doubly Linked List: Doubly Linked List Introduction and Insertion Delete a node in a Doubly Linked List Reverse a Doubly Linked List The Great Tree-List Recursion Problem. QuickSort on Doubly Linked List Merge Sort for Doubly Linked List All Articles of Linked List Quiz on Linked List Coding Practice on Linked List Recent Articles on Linked List Stack: Introduction to Stack Infix to Postfix Conversion using Stack Evaluation of Postfix Expression Reverse a String using Stack Implement two stacks in an array Check for balanced parentheses in an expression Next Greater Element Reverse a stack using recursion Sort a stack using recursion The Stock Span Problem Design and Implement Special Stack Data Structure Implement Stack using Queues Design a stack with operations on middle element How to efficiently implement k stacks in a single array? Sort a stack using recursion Quiz on Stack All Articles on Stack Coding Practice on Stack Recent Articles on Stack Queue: Queue Introduction and Array Implementation Linked List Implementation of Queue Applications of Queue Data Structure Priority Queue Introduction Deque (Introduction and Applications) Implementation of Deque using circular array Implement Queue using Stacks Find the first circular tour that visits all petrol pumps Maximum of all subarrays of size k An Interesting Method to Generate Binary Numbers from 1 to n How to efficiently implement k Queues in a single array? Quiz on Queue All Articles on Queue Coding Practice on Queue Recent Articles on Queue Binary Tree: Binary Tree Introduction Binary Tree Properties Types of Binary Tree Handshaking Lemma and Interesting Tree Properties Enumeration of Binary Tree Applications of tree data structure Tree Traversals BFS vs DFS for Binary Tree Level Order Tree Traversal Diameter of a Binary Tree Inorder Tree Traversal without Recursion Inorder Tree Traversal without recursion and without stack! Threaded Binary Tree Maximum Depth or Height of a Tree If you are given two traversal sequences, can you construct the binary tree? Clone a Binary Tree with Random Pointers Construct Tree from given Inorder and Preorder traversals Maximum width of a binary tree Print nodes at k distance from root Print Ancestors of a given node in Binary Tree Check if a binary tree is subtree of another binary tree Connect nodes at same level Quiz on Binary Tree Quiz on Binary Tree Traversals All articles on Binary Tree Coding Practice on Binary Tree Recent Articles on Tree Binary Search Tree: Search and Insert in BST Deletion from BST Minimum value in a Binary Search Tree Inorder predecessor and successor for a given key in BST Check if a binary tree is BST or not Lowest Common Ancestor in a Binary Search Tree. Inorder Successor in Binary Search Tree Find k-th smallest element in BST (Order Statistics in BST) Merge two BSTs with limited extra space Two nodes of a BST are swapped, correct the BST Floor and Ceil from a BST In-place conversion of Sorted DLL to Balanced BST Find a pair with given sum in a Balanced BST Total number of possible Binary Search Trees with n keys Merge Two Balanced Binary Search Trees Binary Tree to Binary Search Tree Conversion Quiz on Binary Search Trees Quiz on Balanced Binary Search Trees All Articles on Binary Search Tree Coding Practice on Binary Search Tree Recent Articles on BST Heap: Binary Heap Why is Binary Heap Preferred over BST for Priority Queue? Binomial Heap Fibonacci Heap Heap Sort K’th Largest Element in an array Sort an almost sorted array/ Tournament Tree (Winner Tree) and Binary Heap All Articles on Heap Quiz on Heap Coding Practice on Heap Recent Articles on Heap Hashing: Hashing Introduction Separate Chaining for Collision Handling Open Addressing for Collision Handling Print a Binary Tree in Vertical Order Find whether an array is subset of another array Union and Intersection of two Linked Lists Find a pair with given sum Check if a given array contains duplicate elements within k distance from each other Find Itinerary from a given list of tickets Find number of Employees Under every Employee Quiz on Hashing All Articles on Hashing Coding Practice on Hashing Recent Articles on Hashing Graph: Introduction, DFS and BFS: Graph and its representations Breadth First Traversal for a Graph Depth First Traversal for a Graph Applications of Depth First Search Applications of Breadth First Traversal Detect Cycle in a Directed Graph Detect Cycle in a an Undirected Graph Detect cycle in an undirected graph Longest Path in a Directed Acyclic Graph Topological Sorting Check whether a given graph is Bipartite or not Snake and Ladder Problem Minimize Cash Flow among a given set of friends who have borrowed money from each other Boggle (Find all possible words in a board of characters) Assign directions to edges so that the directed graph remains acyclic All Articles on Graph Data Structure Quiz on Graph Quiz on Graph Traversals Quiz on Graph Shortest Paths Quiz on Graph Minimum Spanning Tree Coding Practice on Graph Recent Articles on Graph Advanced Data Structure: Advanced Lists: Memory efficient doubly linked list XOR Linked List – A Memory Efficient Doubly Linked List | Set 1 XOR Linked List – A Memory Efficient Doubly Linked List | Set 2 Skip List | Set 1 (Introduction) Self Organizing List | Set 1 (Introduction) Unrolled Linked List | Set 1 (Introduction) Segment Tree: Segment Tree | Set 1 (Sum of given range) Segment Tree | Set 2 (Range Minimum Query) Lazy Propagation in Segment Tree Persistent Segment Tree | Set 1 (Introduction) All articles on Segment Tree Trie: Trie | (Insert and Search) Trie | (Delete) Longest prefix matching – A Trie based solution in Java Print unique rows in a given boolean matrix How to Implement Reverse DNS Look Up Cache? How to Implement Forward DNS Look Up Cache? All Articles on Trie Binary Indexed Tree: Binary Indexed Tree Two Dimensional Binary Indexed Tree or Fenwick Tree Binary Indexed Tree : Range Updates and Point Queries Binary Indexed Tree : Range Update and Range Queries All Articles on Binary Indexed Tree Suffix Array and Suffix Tree: Suffix Array Introduction Suffix Array nLogn Algorithm kasai’s Algorithm for Construction of LCP array from Suffix Array Suffix Tree Introduction Ukkonen’s Suffix Tree Construction – Part 1 Ukkonen’s Suffix Tree Construction – Part 2 Ukkonen’s Suffix Tree Construction – Part 3 Ukkonen’s Suffix Tree Construction – Part 4, Ukkonen’s Suffix Tree Construction – Part 5 Ukkonen’s Suffix Tree Construction – Part 6 Generalized Suffix Tree Build Linear Time Suffix Array using Suffix Tree Substring Check Searching All Patterns Longest Repeated Substring, Longest Common Substring, Longest Palindromic Substring All Articles on Suffix Tree AVL Tree: AVL Tree | Set 1 (Insertion) AVL Tree | Set 2 (Deletion) AVL with duplicate keys Splay Tree: Splay Tree | Set 1 (Search) Splay Tree | Set 2 (Insert) B Tree: B-Tree | Set 1 (Introduction) B-Tree | Set 2 (Insert) B-Tree | Set 3 (Delete) Red-Black Tree: Red-Black Tree Introduction Red Black Tree Insertion. Red-Black Tree Deletion Program for Red Black Tree Insertion All Articles on Self-Balancing BSTs K Dimensional Tree: KD Tree (Search and Insert) K D Tree (Find Minimum) K D Tree (Delete) Others: Treap (A Randomized Binary Search Tree) Ternary Search Tree Interval Tree Implement LRU Cache Sort numbers stored on different machines Find the k most frequent words from a file Given a sequence of words, print all anagrams together Tournament Tree (Winner Tree) and Binary Heap Decision Trees – Fake (Counterfeit) Coin Puzzle (12 Coin Puzzle) Spaghetti Stack Data Structure for Dictionary and Spell Checker? Cartesian Tree Cartesian Tree Sorting Sparse Set Centroid Decomposition of Tree Gomory-Hu Tree Recent Articles on Advanced Data Structures. Array: Search, insert and delete in an unsorted array Search, insert and delete in a sorted array Write a program to reverse an array Leaders in an array Given an array A[] and a number x, check for pair in A[] with sum as x Majority Element Find the Number Occurring Odd Number of Times Largest Sum Contiguous Subarray Find the Missing Number Search an element in a sorted and pivoted array Merge an array of size n into another array of size m+n Median of two sorted arrays Program for array rotation Reversal algorithm for array rotation Block swap algorithm for array rotation Maximum sum such that no two elements are adjacent Sort elements by frequency | Set 1 Count Inversions in an array All Articles on Array Coding Practice on Array Quiz on Array Coding Practice on Array Recent Articles on Array Matrix: Search in a row wise and column wise sorted matrix Print a given matrix in spiral form A Boolean Matrix Question Print unique rows in a given boolean matrix Maximum size square sub-matrix with all 1s Print unique rows in a given boolean matrix Inplace M x N size matrix transpose | Updated Dynamic Programming | Set 27 (Maximum sum rectangle in a 2D matrix) Strassen’s Matrix Multiplication Create a matrix with alternating rectangles of O and X Print all elements in sorted order from row and column wise sorted matrix Given an n x n square matrix, find sum of all sub-squares of size k x k Count number of islands where every island is row-wise and column-wise separated Find a common element in all rows of a given row-wise sorted matrix All Articles on Matrix Coding Practice on Matrix Recent Articles on Matrix. Misc: Commonly Asked Data Structure Interview Questions | Set 1 A data structure for n elements and O(1) operations Expression Tree You can create a new DS topic and discuss it with other geeks using our portal PRACTICE. See recently added problems on Data Structures on PRACTICE. ",,,,
ALG,"A data structure is a particular way of organizing data in a computer so that it can be used effectively. For example, we can store a list of items having the same data-type using the array data structure. Click to enlarge Array Data Structure This page contains detailed tutorials on different data structures (DS) with topic-wise problems. Topics: Array Linked List Stack Queue Binary Tree Binary Search Tree Heap Hashing Graph Matrix Misc Advanced Data Structure Overview: Overview of Data Structures | Set 1 (Linear Data Structures) Overview of Data Structures | Set 2 (Binary Tree, BST, Heap and Hash) Overview of Data Structures | Set 3 (Graph, Trie, Segment Tree and Suffix Tree) Abstract Data Types Linked List: Singly Linked List: Introduction to Linked List Linked List vs Array Linked List Insertion Linked List Deletion (Deleting a given key) Linked List Deletion (Deleting a key at given position) A Programmer’s approach of looking at Array vs. Linked List Find Length of a Linked List (Iterative and Recursive) How to write C functions that modify head pointer of a Linked List? Swap nodes in a linked list without swapping data Reverse a linked list Merge two sorted linked lists Merge Sort for Linked Lists Reverse a Linked List in groups of given size Detect and Remove Loop in a Linked List Add two numbers represented by linked lists | Set 1 Rotate a Linked List Generic Linked List in C Circular Linked List: Circular Linked List Introduction and Applications, Circular Singly Linked List Insertion< Circular Linked List Traversal Split a Circular Linked List into two halves Sorted insert for circular linked list Doubly Linked List: Doubly Linked List Introduction and Insertion Delete a node in a Doubly Linked List Reverse a Doubly Linked List The Great Tree-List Recursion Problem. QuickSort on Doubly Linked List Merge Sort for Doubly Linked List All Articles of Linked List Quiz on Linked List Coding Practice on Linked List Recent Articles on Linked List Stack: Introduction to Stack Infix to Postfix Conversion using Stack Evaluation of Postfix Expression Reverse a String using Stack Implement two stacks in an array Check for balanced parentheses in an expression Next Greater Element Reverse a stack using recursion Sort a stack using recursion The Stock Span Problem Design and Implement Special Stack Data Structure Implement Stack using Queues Design a stack with operations on middle element How to efficiently implement k stacks in a single array? Sort a stack using recursion Quiz on Stack All Articles on Stack Coding Practice on Stack Recent Articles on Stack Queue: Queue Introduction and Array Implementation Linked List Implementation of Queue Applications of Queue Data Structure Priority Queue Introduction Deque (Introduction and Applications) Implementation of Deque using circular array Implement Queue using Stacks Find the first circular tour that visits all petrol pumps Maximum of all subarrays of size k An Interesting Method to Generate Binary Numbers from 1 to n How to efficiently implement k Queues in a single array? Quiz on Queue All Articles on Queue Coding Practice on Queue Recent Articles on Queue Binary Tree: Binary Tree Introduction Binary Tree Properties Types of Binary Tree Handshaking Lemma and Interesting Tree Properties Enumeration of Binary Tree Applications of tree data structure Tree Traversals BFS vs DFS for Binary Tree Level Order Tree Traversal Diameter of a Binary Tree Inorder Tree Traversal without Recursion Inorder Tree Traversal without recursion and without stack! Threaded Binary Tree Maximum Depth or Height of a Tree If you are given two traversal sequences, can you construct the binary tree? Clone a Binary Tree with Random Pointers Construct Tree from given Inorder and Preorder traversals Maximum width of a binary tree Print nodes at k distance from root Print Ancestors of a given node in Binary Tree Check if a binary tree is subtree of another binary tree Connect nodes at same level Quiz on Binary Tree Quiz on Binary Tree Traversals All articles on Binary Tree Coding Practice on Binary Tree Recent Articles on Tree Binary Search Tree: Search and Insert in BST Deletion from BST Minimum value in a Binary Search Tree Inorder predecessor and successor for a given key in BST Check if a binary tree is BST or not Lowest Common Ancestor in a Binary Search Tree. Inorder Successor in Binary Search Tree Find k-th smallest element in BST (Order Statistics in BST) Merge two BSTs with limited extra space Two nodes of a BST are swapped, correct the BST Floor and Ceil from a BST In-place conversion of Sorted DLL to Balanced BST Find a pair with given sum in a Balanced BST Total number of possible Binary Search Trees with n keys Merge Two Balanced Binary Search Trees Binary Tree to Binary Search Tree Conversion Quiz on Binary Search Trees Quiz on Balanced Binary Search Trees All Articles on Binary Search Tree Coding Practice on Binary Search Tree Recent Articles on BST Heap: Binary Heap Why is Binary Heap Preferred over BST for Priority Queue? Binomial Heap Fibonacci Heap Heap Sort K’th Largest Element in an array Sort an almost sorted array/ Tournament Tree (Winner Tree) and Binary Heap All Articles on Heap Quiz on Heap Coding Practice on Heap Recent Articles on Heap Hashing: Hashing Introduction Separate Chaining for Collision Handling Open Addressing for Collision Handling Print a Binary Tree in Vertical Order Find whether an array is subset of another array Union and Intersection of two Linked Lists Find a pair with given sum Check if a given array contains duplicate elements within k distance from each other Find Itinerary from a given list of tickets Find number of Employees Under every Employee Quiz on Hashing All Articles on Hashing Coding Practice on Hashing Recent Articles on Hashing Graph: Introduction, DFS and BFS: Graph and its representations Breadth First Traversal for a Graph Depth First Traversal for a Graph Applications of Depth First Search Applications of Breadth First Traversal Detect Cycle in a Directed Graph Detect Cycle in a an Undirected Graph Detect cycle in an undirected graph Longest Path in a Directed Acyclic Graph Topological Sorting Check whether a given graph is Bipartite or not Snake and Ladder Problem Minimize Cash Flow among a given set of friends who have borrowed money from each other Boggle (Find all possible words in a board of characters) Assign directions to edges so that the directed graph remains acyclic All Articles on Graph Data Structure Quiz on Graph Quiz on Graph Traversals Quiz on Graph Shortest Paths Quiz on Graph Minimum Spanning Tree Coding Practice on Graph Recent Articles on Graph Advanced Data Structure: Advanced Lists: Memory efficient doubly linked list XOR Linked List – A Memory Efficient Doubly Linked List | Set 1 XOR Linked List – A Memory Efficient Doubly Linked List | Set 2 Skip List | Set 1 (Introduction) Self Organizing List | Set 1 (Introduction) Unrolled Linked List | Set 1 (Introduction) Segment Tree: Segment Tree | Set 1 (Sum of given range) Segment Tree | Set 2 (Range Minimum Query) Lazy Propagation in Segment Tree Persistent Segment Tree | Set 1 (Introduction) All articles on Segment Tree Trie: Trie | (Insert and Search) Trie | (Delete) Longest prefix matching – A Trie based solution in Java Print unique rows in a given boolean matrix How to Implement Reverse DNS Look Up Cache? How to Implement Forward DNS Look Up Cache? All Articles on Trie Binary Indexed Tree: Binary Indexed Tree Two Dimensional Binary Indexed Tree or Fenwick Tree Binary Indexed Tree : Range Updates and Point Queries Binary Indexed Tree : Range Update and Range Queries All Articles on Binary Indexed Tree Suffix Array and Suffix Tree: Suffix Array Introduction Suffix Array nLogn Algorithm kasai’s Algorithm for Construction of LCP array from Suffix Array Suffix Tree Introduction Ukkonen’s Suffix Tree Construction – Part 1 Ukkonen’s Suffix Tree Construction – Part 2 Ukkonen’s Suffix Tree Construction – Part 3 Ukkonen’s Suffix Tree Construction – Part 4, Ukkonen’s Suffix Tree Construction – Part 5 Ukkonen’s Suffix Tree Construction – Part 6 Generalized Suffix Tree Build Linear Time Suffix Array using Suffix Tree Substring Check Searching All Patterns Longest Repeated Substring, Longest Common Substring, Longest Palindromic Substring All Articles on Suffix Tree AVL Tree: AVL Tree | Set 1 (Insertion) AVL Tree | Set 2 (Deletion) AVL with duplicate keys Splay Tree: Splay Tree | Set 1 (Search) Splay Tree | Set 2 (Insert) B Tree: B-Tree | Set 1 (Introduction) B-Tree | Set 2 (Insert) B-Tree | Set 3 (Delete) Red-Black Tree: Red-Black Tree Introduction Red Black Tree Insertion. Red-Black Tree Deletion Program for Red Black Tree Insertion All Articles on Self-Balancing BSTs K Dimensional Tree: KD Tree (Search and Insert) K D Tree (Find Minimum) K D Tree (Delete) Others: Treap (A Randomized Binary Search Tree) Ternary Search Tree Interval Tree Implement LRU Cache Sort numbers stored on different machines Find the k most frequent words from a file Given a sequence of words, print all anagrams together Tournament Tree (Winner Tree) and Binary Heap Decision Trees – Fake (Counterfeit) Coin Puzzle (12 Coin Puzzle) Spaghetti Stack Data Structure for Dictionary and Spell Checker? Cartesian Tree Cartesian Tree Sorting Sparse Set Centroid Decomposition of Tree Gomory-Hu Tree Recent Articles on Advanced Data Structures. Array: Search, insert and delete in an unsorted array Search, insert and delete in a sorted array Write a program to reverse an array Leaders in an array Given an array A[] and a number x, check for pair in A[] with sum as x Majority Element Find the Number Occurring Odd Number of Times Largest Sum Contiguous Subarray Find the Missing Number Search an element in a sorted and pivoted array Merge an array of size n into another array of size m+n Median of two sorted arrays Program for array rotation Reversal algorithm for array rotation Block swap algorithm for array rotation Maximum sum such that no two elements are adjacent Sort elements by frequency | Set 1 Count Inversions in an array All Articles on Array Coding Practice on Array Quiz on Array Coding Practice on Array Recent Articles on Array Matrix: Search in a row wise and column wise sorted matrix Print a given matrix in spiral form A Boolean Matrix Question Print unique rows in a given boolean matrix Maximum size square sub-matrix with all 1s Print unique rows in a given boolean matrix Inplace M x N size matrix transpose | Updated Dynamic Programming | Set 27 (Maximum sum rectangle in a 2D matrix) Strassen’s Matrix Multiplication Create a matrix with alternating rectangles of O and X Print all elements in sorted order from row and column wise sorted matrix Given an n x n square matrix, find sum of all sub-squares of size k x k Count number of islands where every island is row-wise and column-wise separated Find a common element in all rows of a given row-wise sorted matrix All Articles on Matrix Coding Practice on Matrix Recent Articles on Matrix. Misc: Commonly Asked Data Structure Interview Questions | Set 1 A data structure for n elements and O(1) operations Expression Tree You can create a new DS topic and discuss it with other geeks using our portal PRACTICE. See recently added problems on Data Structures on PRACTICE. ",,,,
